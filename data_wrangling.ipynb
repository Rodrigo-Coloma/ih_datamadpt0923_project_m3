{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate, train_test_split\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBRegressor, plot_importance\n",
    "import sqlite3 as lite\n",
    "#from umap import UMAP\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = lite.connect('./data/src/diamonds_train.db/diamonds_train.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = con.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 889,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.read_sql('''SELECT *\n",
    "FROM diamonds_transactional dt JOIN diamonds_dimensions ddi \n",
    "\t\tON dt.index_id = ddi.index_id \n",
    "\tJOIN diamonds_properties dp ON dp.index_id = dt.index_id \n",
    "\tJOIN diamonds_cut dc ON dc.cut_id = dp.cut_id \n",
    "\tJOIN diamonds_color dc2 ON dc2.color_id = dp.color_id \n",
    "\tJOIN diamonds_clarity dc3 ON dc3.clarity_id = dp.clarity_id \n",
    "\tJOIN diamonds_city dc4 ON dc4.city_id = dt.city_id;''', con )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 890,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_id</th>\n",
       "      <th>price</th>\n",
       "      <th>city_id</th>\n",
       "      <th>carat</th>\n",
       "      <th>index_id</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>...</th>\n",
       "      <th>color_id</th>\n",
       "      <th>clarity_id</th>\n",
       "      <th>cut_id</th>\n",
       "      <th>cut</th>\n",
       "      <th>color_id</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity_id</th>\n",
       "      <th>clarity</th>\n",
       "      <th>city_id</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91...</td>\n",
       "      <td>4268</td>\n",
       "      <td>6c425048aa7badd9d84615bd8620ca1864efd81cfdb69d...</td>\n",
       "      <td>1.21</td>\n",
       "      <td>5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91...</td>\n",
       "      <td>62.4</td>\n",
       "      <td>58.0</td>\n",
       "      <td>6.83</td>\n",
       "      <td>6.79</td>\n",
       "      <td>4.25</td>\n",
       "      <td>...</td>\n",
       "      <td>6da43b944e494e885e69af021f93c6d9331c78aa228084...</td>\n",
       "      <td>f0b2a1d0db08cc64f85d74f1d15c2191e0e49039f4d8f2...</td>\n",
       "      <td>de88c121a82a06352bf1aaceba20578356408a334ba046...</td>\n",
       "      <td>Premium</td>\n",
       "      <td>6da43b944e494e885e69af021f93c6d9331c78aa228084...</td>\n",
       "      <td>J</td>\n",
       "      <td>f0b2a1d0db08cc64f85d74f1d15c2191e0e49039f4d8f2...</td>\n",
       "      <td>VS2</td>\n",
       "      <td>6c425048aa7badd9d84615bd8620ca1864efd81cfdb69d...</td>\n",
       "      <td>Dubai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d...</td>\n",
       "      <td>505</td>\n",
       "      <td>89c7286890f7347ab235234e74d406596a127ae3679042...</td>\n",
       "      <td>0.32</td>\n",
       "      <td>6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d...</td>\n",
       "      <td>63.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>4.35</td>\n",
       "      <td>4.38</td>\n",
       "      <td>2.75</td>\n",
       "      <td>...</td>\n",
       "      <td>44bd7ae60f478fae1061e11a7739f4b94d1daf917982d3...</td>\n",
       "      <td>f0b2a1d0db08cc64f85d74f1d15c2191e0e49039f4d8f2...</td>\n",
       "      <td>388655e25e91872329272fc10128ef5354b3b19a05d7e8...</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>44bd7ae60f478fae1061e11a7739f4b94d1daf917982d3...</td>\n",
       "      <td>H</td>\n",
       "      <td>f0b2a1d0db08cc64f85d74f1d15c2191e0e49039f4d8f2...</td>\n",
       "      <td>VS2</td>\n",
       "      <td>89c7286890f7347ab235234e74d406596a127ae3679042...</td>\n",
       "      <td>Kimberly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f...</td>\n",
       "      <td>2686</td>\n",
       "      <td>2bd25cd960aba8b706e2b67f2bb38b750ee5384b0e9883...</td>\n",
       "      <td>0.71</td>\n",
       "      <td>d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f...</td>\n",
       "      <td>65.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>5.62</td>\n",
       "      <td>5.53</td>\n",
       "      <td>3.65</td>\n",
       "      <td>...</td>\n",
       "      <td>333e0a1e27815d0ceee55c473fe3dc93d56c63e3bee2b3...</td>\n",
       "      <td>ef736c1f91cd1900c3d9cde925b1bf4f013adc0211a9ee...</td>\n",
       "      <td>f7b19afcde965ea4942b878d266f89f8ba9a5a833e60f7...</td>\n",
       "      <td>Fair</td>\n",
       "      <td>333e0a1e27815d0ceee55c473fe3dc93d56c63e3bee2b3...</td>\n",
       "      <td>G</td>\n",
       "      <td>ef736c1f91cd1900c3d9cde925b1bf4f013adc0211a9ee...</td>\n",
       "      <td>VS1</td>\n",
       "      <td>2bd25cd960aba8b706e2b67f2bb38b750ee5384b0e9883...</td>\n",
       "      <td>Las Vegas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4e07408562bedb8b60ce05c1decfe3ad16b72230967de0...</td>\n",
       "      <td>738</td>\n",
       "      <td>89c7286890f7347ab235234e74d406596a127ae3679042...</td>\n",
       "      <td>0.41</td>\n",
       "      <td>4e07408562bedb8b60ce05c1decfe3ad16b72230967de0...</td>\n",
       "      <td>63.8</td>\n",
       "      <td>56.0</td>\n",
       "      <td>4.68</td>\n",
       "      <td>4.72</td>\n",
       "      <td>3.00</td>\n",
       "      <td>...</td>\n",
       "      <td>3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0e...</td>\n",
       "      <td>bd4f4a250da88f87729febc739ae97f439a14f9d38f0e3...</td>\n",
       "      <td>c939327ca16dcf97ca32521d8b834bf1de16573d21deda...</td>\n",
       "      <td>Good</td>\n",
       "      <td>3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0e...</td>\n",
       "      <td>D</td>\n",
       "      <td>bd4f4a250da88f87729febc739ae97f439a14f9d38f0e3...</td>\n",
       "      <td>SI1</td>\n",
       "      <td>89c7286890f7347ab235234e74d406596a127ae3679042...</td>\n",
       "      <td>Kimberly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4b227777d4dd1fc61c6f884f48641d02b4d121d3fd328c...</td>\n",
       "      <td>4882</td>\n",
       "      <td>6c425048aa7badd9d84615bd8620ca1864efd81cfdb69d...</td>\n",
       "      <td>1.02</td>\n",
       "      <td>4b227777d4dd1fc61c6f884f48641d02b4d121d3fd328c...</td>\n",
       "      <td>60.5</td>\n",
       "      <td>59.0</td>\n",
       "      <td>6.55</td>\n",
       "      <td>6.51</td>\n",
       "      <td>3.95</td>\n",
       "      <td>...</td>\n",
       "      <td>333e0a1e27815d0ceee55c473fe3dc93d56c63e3bee2b3...</td>\n",
       "      <td>bd4f4a250da88f87729febc739ae97f439a14f9d38f0e3...</td>\n",
       "      <td>4e3cfaa334cbafb57a399a98fad8d3812ece460018f457...</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>333e0a1e27815d0ceee55c473fe3dc93d56c63e3bee2b3...</td>\n",
       "      <td>G</td>\n",
       "      <td>bd4f4a250da88f87729febc739ae97f439a14f9d38f0e3...</td>\n",
       "      <td>SI1</td>\n",
       "      <td>6c425048aa7badd9d84615bd8620ca1864efd81cfdb69d...</td>\n",
       "      <td>Dubai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40450</th>\n",
       "      <td>f0bc79169405ebeb24e308055156b946ffd819db9b4f75...</td>\n",
       "      <td>10070</td>\n",
       "      <td>ca3aa21a5b70c3e88cc6336682c8c7da928a0c66a5ead4...</td>\n",
       "      <td>1.34</td>\n",
       "      <td>f0bc79169405ebeb24e308055156b946ffd819db9b4f75...</td>\n",
       "      <td>62.7</td>\n",
       "      <td>57.0</td>\n",
       "      <td>7.10</td>\n",
       "      <td>7.04</td>\n",
       "      <td>4.43</td>\n",
       "      <td>...</td>\n",
       "      <td>333e0a1e27815d0ceee55c473fe3dc93d56c63e3bee2b3...</td>\n",
       "      <td>ef736c1f91cd1900c3d9cde925b1bf4f013adc0211a9ee...</td>\n",
       "      <td>4e3cfaa334cbafb57a399a98fad8d3812ece460018f457...</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>333e0a1e27815d0ceee55c473fe3dc93d56c63e3bee2b3...</td>\n",
       "      <td>G</td>\n",
       "      <td>ef736c1f91cd1900c3d9cde925b1bf4f013adc0211a9ee...</td>\n",
       "      <td>VS1</td>\n",
       "      <td>ca3aa21a5b70c3e88cc6336682c8c7da928a0c66a5ead4...</td>\n",
       "      <td>Antwerp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40451</th>\n",
       "      <td>339916a23bf22b052b54cb2a9b36ee8418c1c68b46acad...</td>\n",
       "      <td>12615</td>\n",
       "      <td>e9c722cbefc2f055ae60b4e2cbe73a2d99537eab0c37f3...</td>\n",
       "      <td>2.02</td>\n",
       "      <td>339916a23bf22b052b54cb2a9b36ee8418c1c68b46acad...</td>\n",
       "      <td>57.1</td>\n",
       "      <td>60.0</td>\n",
       "      <td>8.31</td>\n",
       "      <td>8.25</td>\n",
       "      <td>4.73</td>\n",
       "      <td>...</td>\n",
       "      <td>f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d378...</td>\n",
       "      <td>03c358cbd92e83278fd523f58dc6a9b4b198d00728af65...</td>\n",
       "      <td>c939327ca16dcf97ca32521d8b834bf1de16573d21deda...</td>\n",
       "      <td>Good</td>\n",
       "      <td>f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d378...</td>\n",
       "      <td>F</td>\n",
       "      <td>03c358cbd92e83278fd523f58dc6a9b4b198d00728af65...</td>\n",
       "      <td>SI2</td>\n",
       "      <td>e9c722cbefc2f055ae60b4e2cbe73a2d99537eab0c37f3...</td>\n",
       "      <td>Madrid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40452</th>\n",
       "      <td>46957922b99954654c1deb8d854c3f069bf118b2ce9415...</td>\n",
       "      <td>5457</td>\n",
       "      <td>89c7286890f7347ab235234e74d406596a127ae3679042...</td>\n",
       "      <td>1.01</td>\n",
       "      <td>46957922b99954654c1deb8d854c3f069bf118b2ce9415...</td>\n",
       "      <td>62.7</td>\n",
       "      <td>56.0</td>\n",
       "      <td>6.37</td>\n",
       "      <td>6.42</td>\n",
       "      <td>4.01</td>\n",
       "      <td>...</td>\n",
       "      <td>44bd7ae60f478fae1061e11a7739f4b94d1daf917982d3...</td>\n",
       "      <td>bd4f4a250da88f87729febc739ae97f439a14f9d38f0e3...</td>\n",
       "      <td>4e3cfaa334cbafb57a399a98fad8d3812ece460018f457...</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>44bd7ae60f478fae1061e11a7739f4b94d1daf917982d3...</td>\n",
       "      <td>H</td>\n",
       "      <td>bd4f4a250da88f87729febc739ae97f439a14f9d38f0e3...</td>\n",
       "      <td>SI1</td>\n",
       "      <td>89c7286890f7347ab235234e74d406596a127ae3679042...</td>\n",
       "      <td>Kimberly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40453</th>\n",
       "      <td>9d733392d362d5c6f1d9b9659b601c7d4b5a1c1c8df579...</td>\n",
       "      <td>456</td>\n",
       "      <td>89c7286890f7347ab235234e74d406596a127ae3679042...</td>\n",
       "      <td>0.33</td>\n",
       "      <td>9d733392d362d5c6f1d9b9659b601c7d4b5a1c1c8df579...</td>\n",
       "      <td>61.9</td>\n",
       "      <td>54.3</td>\n",
       "      <td>4.45</td>\n",
       "      <td>4.47</td>\n",
       "      <td>2.76</td>\n",
       "      <td>...</td>\n",
       "      <td>6da43b944e494e885e69af021f93c6d9331c78aa228084...</td>\n",
       "      <td>ef736c1f91cd1900c3d9cde925b1bf4f013adc0211a9ee...</td>\n",
       "      <td>4e3cfaa334cbafb57a399a98fad8d3812ece460018f457...</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>6da43b944e494e885e69af021f93c6d9331c78aa228084...</td>\n",
       "      <td>J</td>\n",
       "      <td>ef736c1f91cd1900c3d9cde925b1bf4f013adc0211a9ee...</td>\n",
       "      <td>VS1</td>\n",
       "      <td>89c7286890f7347ab235234e74d406596a127ae3679042...</td>\n",
       "      <td>Kimberly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40454</th>\n",
       "      <td>a02744a70faa594d240b067f21fcc23a8d17cd1098a9fb...</td>\n",
       "      <td>6232</td>\n",
       "      <td>ecc0e7dc084f141b29479058967d0bc07dee25d9690a98...</td>\n",
       "      <td>1.24</td>\n",
       "      <td>a02744a70faa594d240b067f21fcc23a8d17cd1098a9fb...</td>\n",
       "      <td>62.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>6.83</td>\n",
       "      <td>6.88</td>\n",
       "      <td>4.25</td>\n",
       "      <td>...</td>\n",
       "      <td>a83dd0ccbffe39d071cc317ddf6e97f5c6b1c87af91919...</td>\n",
       "      <td>bd4f4a250da88f87729febc739ae97f439a14f9d38f0e3...</td>\n",
       "      <td>4e3cfaa334cbafb57a399a98fad8d3812ece460018f457...</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>a83dd0ccbffe39d071cc317ddf6e97f5c6b1c87af91919...</td>\n",
       "      <td>I</td>\n",
       "      <td>bd4f4a250da88f87729febc739ae97f439a14f9d38f0e3...</td>\n",
       "      <td>SI1</td>\n",
       "      <td>ecc0e7dc084f141b29479058967d0bc07dee25d9690a98...</td>\n",
       "      <td>London</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40455 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                index_id  price  \\\n",
       "0      5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91...   4268   \n",
       "1      6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d...    505   \n",
       "2      d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f...   2686   \n",
       "3      4e07408562bedb8b60ce05c1decfe3ad16b72230967de0...    738   \n",
       "4      4b227777d4dd1fc61c6f884f48641d02b4d121d3fd328c...   4882   \n",
       "...                                                  ...    ...   \n",
       "40450  f0bc79169405ebeb24e308055156b946ffd819db9b4f75...  10070   \n",
       "40451  339916a23bf22b052b54cb2a9b36ee8418c1c68b46acad...  12615   \n",
       "40452  46957922b99954654c1deb8d854c3f069bf118b2ce9415...   5457   \n",
       "40453  9d733392d362d5c6f1d9b9659b601c7d4b5a1c1c8df579...    456   \n",
       "40454  a02744a70faa594d240b067f21fcc23a8d17cd1098a9fb...   6232   \n",
       "\n",
       "                                                 city_id  carat  \\\n",
       "0      6c425048aa7badd9d84615bd8620ca1864efd81cfdb69d...   1.21   \n",
       "1      89c7286890f7347ab235234e74d406596a127ae3679042...   0.32   \n",
       "2      2bd25cd960aba8b706e2b67f2bb38b750ee5384b0e9883...   0.71   \n",
       "3      89c7286890f7347ab235234e74d406596a127ae3679042...   0.41   \n",
       "4      6c425048aa7badd9d84615bd8620ca1864efd81cfdb69d...   1.02   \n",
       "...                                                  ...    ...   \n",
       "40450  ca3aa21a5b70c3e88cc6336682c8c7da928a0c66a5ead4...   1.34   \n",
       "40451  e9c722cbefc2f055ae60b4e2cbe73a2d99537eab0c37f3...   2.02   \n",
       "40452  89c7286890f7347ab235234e74d406596a127ae3679042...   1.01   \n",
       "40453  89c7286890f7347ab235234e74d406596a127ae3679042...   0.33   \n",
       "40454  ecc0e7dc084f141b29479058967d0bc07dee25d9690a98...   1.24   \n",
       "\n",
       "                                                index_id  depth  table     x  \\\n",
       "0      5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91...   62.4   58.0  6.83   \n",
       "1      6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d...   63.0   57.0  4.35   \n",
       "2      d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f...   65.5   55.0  5.62   \n",
       "3      4e07408562bedb8b60ce05c1decfe3ad16b72230967de0...   63.8   56.0  4.68   \n",
       "4      4b227777d4dd1fc61c6f884f48641d02b4d121d3fd328c...   60.5   59.0  6.55   \n",
       "...                                                  ...    ...    ...   ...   \n",
       "40450  f0bc79169405ebeb24e308055156b946ffd819db9b4f75...   62.7   57.0  7.10   \n",
       "40451  339916a23bf22b052b54cb2a9b36ee8418c1c68b46acad...   57.1   60.0  8.31   \n",
       "40452  46957922b99954654c1deb8d854c3f069bf118b2ce9415...   62.7   56.0  6.37   \n",
       "40453  9d733392d362d5c6f1d9b9659b601c7d4b5a1c1c8df579...   61.9   54.3  4.45   \n",
       "40454  a02744a70faa594d240b067f21fcc23a8d17cd1098a9fb...   62.0   58.0  6.83   \n",
       "\n",
       "          y     z  ...                                           color_id  \\\n",
       "0      6.79  4.25  ...  6da43b944e494e885e69af021f93c6d9331c78aa228084...   \n",
       "1      4.38  2.75  ...  44bd7ae60f478fae1061e11a7739f4b94d1daf917982d3...   \n",
       "2      5.53  3.65  ...  333e0a1e27815d0ceee55c473fe3dc93d56c63e3bee2b3...   \n",
       "3      4.72  3.00  ...  3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0e...   \n",
       "4      6.51  3.95  ...  333e0a1e27815d0ceee55c473fe3dc93d56c63e3bee2b3...   \n",
       "...     ...   ...  ...                                                ...   \n",
       "40450  7.04  4.43  ...  333e0a1e27815d0ceee55c473fe3dc93d56c63e3bee2b3...   \n",
       "40451  8.25  4.73  ...  f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d378...   \n",
       "40452  6.42  4.01  ...  44bd7ae60f478fae1061e11a7739f4b94d1daf917982d3...   \n",
       "40453  4.47  2.76  ...  6da43b944e494e885e69af021f93c6d9331c78aa228084...   \n",
       "40454  6.88  4.25  ...  a83dd0ccbffe39d071cc317ddf6e97f5c6b1c87af91919...   \n",
       "\n",
       "                                              clarity_id  \\\n",
       "0      f0b2a1d0db08cc64f85d74f1d15c2191e0e49039f4d8f2...   \n",
       "1      f0b2a1d0db08cc64f85d74f1d15c2191e0e49039f4d8f2...   \n",
       "2      ef736c1f91cd1900c3d9cde925b1bf4f013adc0211a9ee...   \n",
       "3      bd4f4a250da88f87729febc739ae97f439a14f9d38f0e3...   \n",
       "4      bd4f4a250da88f87729febc739ae97f439a14f9d38f0e3...   \n",
       "...                                                  ...   \n",
       "40450  ef736c1f91cd1900c3d9cde925b1bf4f013adc0211a9ee...   \n",
       "40451  03c358cbd92e83278fd523f58dc6a9b4b198d00728af65...   \n",
       "40452  bd4f4a250da88f87729febc739ae97f439a14f9d38f0e3...   \n",
       "40453  ef736c1f91cd1900c3d9cde925b1bf4f013adc0211a9ee...   \n",
       "40454  bd4f4a250da88f87729febc739ae97f439a14f9d38f0e3...   \n",
       "\n",
       "                                                  cut_id        cut  \\\n",
       "0      de88c121a82a06352bf1aaceba20578356408a334ba046...    Premium   \n",
       "1      388655e25e91872329272fc10128ef5354b3b19a05d7e8...  Very Good   \n",
       "2      f7b19afcde965ea4942b878d266f89f8ba9a5a833e60f7...       Fair   \n",
       "3      c939327ca16dcf97ca32521d8b834bf1de16573d21deda...       Good   \n",
       "4      4e3cfaa334cbafb57a399a98fad8d3812ece460018f457...      Ideal   \n",
       "...                                                  ...        ...   \n",
       "40450  4e3cfaa334cbafb57a399a98fad8d3812ece460018f457...      Ideal   \n",
       "40451  c939327ca16dcf97ca32521d8b834bf1de16573d21deda...       Good   \n",
       "40452  4e3cfaa334cbafb57a399a98fad8d3812ece460018f457...      Ideal   \n",
       "40453  4e3cfaa334cbafb57a399a98fad8d3812ece460018f457...      Ideal   \n",
       "40454  4e3cfaa334cbafb57a399a98fad8d3812ece460018f457...      Ideal   \n",
       "\n",
       "                                                color_id color  \\\n",
       "0      6da43b944e494e885e69af021f93c6d9331c78aa228084...     J   \n",
       "1      44bd7ae60f478fae1061e11a7739f4b94d1daf917982d3...     H   \n",
       "2      333e0a1e27815d0ceee55c473fe3dc93d56c63e3bee2b3...     G   \n",
       "3      3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0e...     D   \n",
       "4      333e0a1e27815d0ceee55c473fe3dc93d56c63e3bee2b3...     G   \n",
       "...                                                  ...   ...   \n",
       "40450  333e0a1e27815d0ceee55c473fe3dc93d56c63e3bee2b3...     G   \n",
       "40451  f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d378...     F   \n",
       "40452  44bd7ae60f478fae1061e11a7739f4b94d1daf917982d3...     H   \n",
       "40453  6da43b944e494e885e69af021f93c6d9331c78aa228084...     J   \n",
       "40454  a83dd0ccbffe39d071cc317ddf6e97f5c6b1c87af91919...     I   \n",
       "\n",
       "                                              clarity_id clarity  \\\n",
       "0      f0b2a1d0db08cc64f85d74f1d15c2191e0e49039f4d8f2...     VS2   \n",
       "1      f0b2a1d0db08cc64f85d74f1d15c2191e0e49039f4d8f2...     VS2   \n",
       "2      ef736c1f91cd1900c3d9cde925b1bf4f013adc0211a9ee...     VS1   \n",
       "3      bd4f4a250da88f87729febc739ae97f439a14f9d38f0e3...     SI1   \n",
       "4      bd4f4a250da88f87729febc739ae97f439a14f9d38f0e3...     SI1   \n",
       "...                                                  ...     ...   \n",
       "40450  ef736c1f91cd1900c3d9cde925b1bf4f013adc0211a9ee...     VS1   \n",
       "40451  03c358cbd92e83278fd523f58dc6a9b4b198d00728af65...     SI2   \n",
       "40452  bd4f4a250da88f87729febc739ae97f439a14f9d38f0e3...     SI1   \n",
       "40453  ef736c1f91cd1900c3d9cde925b1bf4f013adc0211a9ee...     VS1   \n",
       "40454  bd4f4a250da88f87729febc739ae97f439a14f9d38f0e3...     SI1   \n",
       "\n",
       "                                                 city_id       city  \n",
       "0      6c425048aa7badd9d84615bd8620ca1864efd81cfdb69d...      Dubai  \n",
       "1      89c7286890f7347ab235234e74d406596a127ae3679042...   Kimberly  \n",
       "2      2bd25cd960aba8b706e2b67f2bb38b750ee5384b0e9883...  Las Vegas  \n",
       "3      89c7286890f7347ab235234e74d406596a127ae3679042...   Kimberly  \n",
       "4      6c425048aa7badd9d84615bd8620ca1864efd81cfdb69d...      Dubai  \n",
       "...                                                  ...        ...  \n",
       "40450  ca3aa21a5b70c3e88cc6336682c8c7da928a0c66a5ead4...    Antwerp  \n",
       "40451  e9c722cbefc2f055ae60b4e2cbe73a2d99537eab0c37f3...     Madrid  \n",
       "40452  89c7286890f7347ab235234e74d406596a127ae3679042...   Kimberly  \n",
       "40453  89c7286890f7347ab235234e74d406596a127ae3679042...   Kimberly  \n",
       "40454  ecc0e7dc084f141b29479058967d0bc07dee25d9690a98...     London  \n",
       "\n",
       "[40455 rows x 22 columns]"
      ]
     },
     "execution_count": 890,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 891,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>carat</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3928.444469</td>\n",
       "      <td>0.797706</td>\n",
       "      <td>61.752841</td>\n",
       "      <td>57.446133</td>\n",
       "      <td>5.729392</td>\n",
       "      <td>5.732819</td>\n",
       "      <td>3.537154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3992.416147</td>\n",
       "      <td>0.475544</td>\n",
       "      <td>1.431725</td>\n",
       "      <td>2.233535</td>\n",
       "      <td>1.124453</td>\n",
       "      <td>1.146650</td>\n",
       "      <td>0.697062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>326.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>945.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>4.710000</td>\n",
       "      <td>4.720000</td>\n",
       "      <td>2.910000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2397.000000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>61.800000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>5.690000</td>\n",
       "      <td>5.710000</td>\n",
       "      <td>3.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5331.000000</td>\n",
       "      <td>1.040000</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>6.540000</td>\n",
       "      <td>6.540000</td>\n",
       "      <td>4.035000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>18823.000000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>10.230000</td>\n",
       "      <td>58.900000</td>\n",
       "      <td>8.060000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              price         carat         depth         table             x  \\\n",
       "count  40455.000000  40455.000000  40455.000000  40455.000000  40455.000000   \n",
       "mean    3928.444469      0.797706     61.752841     57.446133      5.729392   \n",
       "std     3992.416147      0.475544      1.431725      2.233535      1.124453   \n",
       "min      326.000000      0.200000     43.000000     43.000000      0.000000   \n",
       "25%      945.000000      0.400000     61.000000     56.000000      4.710000   \n",
       "50%     2397.000000      0.700000     61.800000     57.000000      5.690000   \n",
       "75%     5331.000000      1.040000     62.500000     59.000000      6.540000   \n",
       "max    18823.000000      4.500000     79.000000     95.000000     10.230000   \n",
       "\n",
       "                  y             z  \n",
       "count  40455.000000  40455.000000  \n",
       "mean       5.732819      3.537154  \n",
       "std        1.146650      0.697062  \n",
       "min        0.000000      0.000000  \n",
       "25%        4.720000      2.910000  \n",
       "50%        5.710000      3.520000  \n",
       "75%        6.540000      4.035000  \n",
       "max       58.900000      8.060000  "
      ]
     },
     "execution_count": 891,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 892,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = raw_df[ ['price', 'carat',  'depth', 'table','x', 'y', 'z','cut', 'color', 'clarity', 'city']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 893,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>carat</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4268</td>\n",
       "      <td>1.21</td>\n",
       "      <td>62.4</td>\n",
       "      <td>58.0</td>\n",
       "      <td>6.83</td>\n",
       "      <td>6.79</td>\n",
       "      <td>4.25</td>\n",
       "      <td>Premium</td>\n",
       "      <td>J</td>\n",
       "      <td>VS2</td>\n",
       "      <td>Dubai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>505</td>\n",
       "      <td>0.32</td>\n",
       "      <td>63.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>4.35</td>\n",
       "      <td>4.38</td>\n",
       "      <td>2.75</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>H</td>\n",
       "      <td>VS2</td>\n",
       "      <td>Kimberly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2686</td>\n",
       "      <td>0.71</td>\n",
       "      <td>65.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>5.62</td>\n",
       "      <td>5.53</td>\n",
       "      <td>3.65</td>\n",
       "      <td>Fair</td>\n",
       "      <td>G</td>\n",
       "      <td>VS1</td>\n",
       "      <td>Las Vegas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>738</td>\n",
       "      <td>0.41</td>\n",
       "      <td>63.8</td>\n",
       "      <td>56.0</td>\n",
       "      <td>4.68</td>\n",
       "      <td>4.72</td>\n",
       "      <td>3.00</td>\n",
       "      <td>Good</td>\n",
       "      <td>D</td>\n",
       "      <td>SI1</td>\n",
       "      <td>Kimberly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4882</td>\n",
       "      <td>1.02</td>\n",
       "      <td>60.5</td>\n",
       "      <td>59.0</td>\n",
       "      <td>6.55</td>\n",
       "      <td>6.51</td>\n",
       "      <td>3.95</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>G</td>\n",
       "      <td>SI1</td>\n",
       "      <td>Dubai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40450</th>\n",
       "      <td>10070</td>\n",
       "      <td>1.34</td>\n",
       "      <td>62.7</td>\n",
       "      <td>57.0</td>\n",
       "      <td>7.10</td>\n",
       "      <td>7.04</td>\n",
       "      <td>4.43</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>G</td>\n",
       "      <td>VS1</td>\n",
       "      <td>Antwerp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40451</th>\n",
       "      <td>12615</td>\n",
       "      <td>2.02</td>\n",
       "      <td>57.1</td>\n",
       "      <td>60.0</td>\n",
       "      <td>8.31</td>\n",
       "      <td>8.25</td>\n",
       "      <td>4.73</td>\n",
       "      <td>Good</td>\n",
       "      <td>F</td>\n",
       "      <td>SI2</td>\n",
       "      <td>Madrid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40452</th>\n",
       "      <td>5457</td>\n",
       "      <td>1.01</td>\n",
       "      <td>62.7</td>\n",
       "      <td>56.0</td>\n",
       "      <td>6.37</td>\n",
       "      <td>6.42</td>\n",
       "      <td>4.01</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>H</td>\n",
       "      <td>SI1</td>\n",
       "      <td>Kimberly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40453</th>\n",
       "      <td>456</td>\n",
       "      <td>0.33</td>\n",
       "      <td>61.9</td>\n",
       "      <td>54.3</td>\n",
       "      <td>4.45</td>\n",
       "      <td>4.47</td>\n",
       "      <td>2.76</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>J</td>\n",
       "      <td>VS1</td>\n",
       "      <td>Kimberly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40454</th>\n",
       "      <td>6232</td>\n",
       "      <td>1.24</td>\n",
       "      <td>62.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>6.83</td>\n",
       "      <td>6.88</td>\n",
       "      <td>4.25</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>I</td>\n",
       "      <td>SI1</td>\n",
       "      <td>London</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40455 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       price  carat  depth  table     x     y     z        cut color clarity  \\\n",
       "0       4268   1.21   62.4   58.0  6.83  6.79  4.25    Premium     J     VS2   \n",
       "1        505   0.32   63.0   57.0  4.35  4.38  2.75  Very Good     H     VS2   \n",
       "2       2686   0.71   65.5   55.0  5.62  5.53  3.65       Fair     G     VS1   \n",
       "3        738   0.41   63.8   56.0  4.68  4.72  3.00       Good     D     SI1   \n",
       "4       4882   1.02   60.5   59.0  6.55  6.51  3.95      Ideal     G     SI1   \n",
       "...      ...    ...    ...    ...   ...   ...   ...        ...   ...     ...   \n",
       "40450  10070   1.34   62.7   57.0  7.10  7.04  4.43      Ideal     G     VS1   \n",
       "40451  12615   2.02   57.1   60.0  8.31  8.25  4.73       Good     F     SI2   \n",
       "40452   5457   1.01   62.7   56.0  6.37  6.42  4.01      Ideal     H     SI1   \n",
       "40453    456   0.33   61.9   54.3  4.45  4.47  2.76      Ideal     J     VS1   \n",
       "40454   6232   1.24   62.0   58.0  6.83  6.88  4.25      Ideal     I     SI1   \n",
       "\n",
       "            city  \n",
       "0          Dubai  \n",
       "1       Kimberly  \n",
       "2      Las Vegas  \n",
       "3       Kimberly  \n",
       "4          Dubai  \n",
       "...          ...  \n",
       "40450    Antwerp  \n",
       "40451     Madrid  \n",
       "40452   Kimberly  \n",
       "40453   Kimberly  \n",
       "40454     London  \n",
       "\n",
       "[40455 rows x 11 columns]"
      ]
     },
     "execution_count": 893,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.to_csv('./data/clean/clean.csv')\n",
    "clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 894,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_labeling(df, cols):\n",
    "   dic = {}\n",
    "   for col in cols:\n",
    "      group_by = df[[col,'price']].groupby(col).median().sort_values('price')\n",
    "      for key in group_by.index:\n",
    "         dic[key] = sorted(list(group_by['price'])).index(group_by.loc[key,'price']) + 1\n",
    "      print(dic)\n",
    "      for key,value in dic.items():\n",
    "         df.loc[df[col] == key,col] = value\n",
    "      df[col] = df[col].astype(float)\n",
    "   return dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_labeling_test(df, cols, dic):\n",
    "   for col in cols:\n",
    "      for key,value in dic.items():\n",
    "         df.loc[df[col] == key,col] = value\n",
    "      df[col] = df[col].astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic2 = {'Ideal': 5,\n",
    " 'Very Good': 3,\n",
    " 'Good': 2,\n",
    " 'Premium': 4,\n",
    " 'Fair': 1,\n",
    " 'E': 6,\n",
    " 'D': 7,\n",
    " 'G': 4,\n",
    " 'F': 5,\n",
    " 'H': 3,\n",
    " 'I': 2,\n",
    " 'J': 1,\n",
    " 'IF': 8,\n",
    " 'VVS1': 7,\n",
    " 'VVS2': 6,\n",
    " 'VS1': 5,\n",
    " 'VS2': 4,\n",
    " 'SI1': 3,\n",
    " 'I1': 1,\n",
    " 'SI2': 2,\n",
    " 'Paris': 1.0,\n",
    " 'Luxembourg': 1.0017263703064307,\n",
    " 'Tel Aviv': 1.0051791109192922,\n",
    " 'Zurich': 1.0310746655157532,\n",
    " 'London': 1.0332326283987916,\n",
    " 'Antwerp': 1.0358221838584376,\n",
    " 'Madrid': 1.0360379801467414,\n",
    " 'Las Vegas': 1.0388433318946915,\n",
    " 'Surat': 1.0401381096245144,\n",
    " 'New York City': 1.048769961156668,\n",
    " 'Amsterdam': 1.0548122572291756,\n",
    " 'Kimberly': 1.0561070349589987,\n",
    " 'Dubai': 1.0699179974104445}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 897,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_580/1822649115.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = df[col].astype(float)\n",
      "/tmp/ipykernel_580/1822649115.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = df[col].astype(float)\n",
      "/tmp/ipykernel_580/1822649115.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = df[col].astype(float)\n",
      "/tmp/ipykernel_580/1822649115.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = df[col].astype(float)\n"
     ]
    }
   ],
   "source": [
    "#dic = data_labeling(clean_df, ['cut', 'color', 'clarity','city'])\n",
    "data_labeling_test(clean_df, ['cut', 'color', 'clarity','city'],dic2)\n",
    "#clean_df = pd.concat([clean_df, pd.get_dummies(clean_df[['cut', 'color', 'clarity','city']], drop_first=True)], axis = 1).drop(['cut', 'color', 'clarity','city'],axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>carat</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3928.444469</td>\n",
       "      <td>0.797706</td>\n",
       "      <td>61.752841</td>\n",
       "      <td>57.446133</td>\n",
       "      <td>5.729392</td>\n",
       "      <td>5.732819</td>\n",
       "      <td>3.537154</td>\n",
       "      <td>3.904783</td>\n",
       "      <td>4.400766</td>\n",
       "      <td>4.049388</td>\n",
       "      <td>1.036433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3992.416147</td>\n",
       "      <td>0.475544</td>\n",
       "      <td>1.431725</td>\n",
       "      <td>2.233535</td>\n",
       "      <td>1.124453</td>\n",
       "      <td>1.146650</td>\n",
       "      <td>0.697062</td>\n",
       "      <td>1.117876</td>\n",
       "      <td>1.701260</td>\n",
       "      <td>1.648181</td>\n",
       "      <td>0.019357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>326.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>945.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>4.710000</td>\n",
       "      <td>4.720000</td>\n",
       "      <td>2.910000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.031075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2397.000000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>61.800000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>5.690000</td>\n",
       "      <td>5.710000</td>\n",
       "      <td>3.520000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.038843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5331.000000</td>\n",
       "      <td>1.040000</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>6.540000</td>\n",
       "      <td>6.540000</td>\n",
       "      <td>4.035000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.048770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>18823.000000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>10.230000</td>\n",
       "      <td>58.900000</td>\n",
       "      <td>8.060000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.069918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              price         carat         depth         table             x  \\\n",
       "count  40455.000000  40455.000000  40455.000000  40455.000000  40455.000000   \n",
       "mean    3928.444469      0.797706     61.752841     57.446133      5.729392   \n",
       "std     3992.416147      0.475544      1.431725      2.233535      1.124453   \n",
       "min      326.000000      0.200000     43.000000     43.000000      0.000000   \n",
       "25%      945.000000      0.400000     61.000000     56.000000      4.710000   \n",
       "50%     2397.000000      0.700000     61.800000     57.000000      5.690000   \n",
       "75%     5331.000000      1.040000     62.500000     59.000000      6.540000   \n",
       "max    18823.000000      4.500000     79.000000     95.000000     10.230000   \n",
       "\n",
       "                  y             z           cut         color       clarity  \\\n",
       "count  40455.000000  40455.000000  40455.000000  40455.000000  40455.000000   \n",
       "mean       5.732819      3.537154      3.904783      4.400766      4.049388   \n",
       "std        1.146650      0.697062      1.117876      1.701260      1.648181   \n",
       "min        0.000000      0.000000      1.000000      1.000000      1.000000   \n",
       "25%        4.720000      2.910000      3.000000      3.000000      3.000000   \n",
       "50%        5.710000      3.520000      4.000000      4.000000      4.000000   \n",
       "75%        6.540000      4.035000      5.000000      6.000000      5.000000   \n",
       "max       58.900000      8.060000      5.000000      7.000000      8.000000   \n",
       "\n",
       "               city  \n",
       "count  40455.000000  \n",
       "mean       1.036433  \n",
       "std        0.019357  \n",
       "min        1.000000  \n",
       "25%        1.031075  \n",
       "50%        1.038843  \n",
       "75%        1.048770  \n",
       "max        1.069918  "
      ]
     },
     "execution_count": 898,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 899,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_outlyers(df,deviations):\n",
    "    df2 = df.copy()\n",
    "    for col in df.columns:\n",
    "        min = df[col].mean() - deviations * df[col].std()\n",
    "        max = df[col].mean() + deviations * df[col].std()\n",
    "        df2.loc[df[col] < min,col] = min    \n",
    "        df2.loc[df[col] > max,col] = max\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 900,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_580/4003587078.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_df[outlier_columns] = sub_outlyers(clean_df[outlier_columns],3.7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>carat</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3928.444469</td>\n",
       "      <td>0.797105</td>\n",
       "      <td>61.750399</td>\n",
       "      <td>57.440964</td>\n",
       "      <td>5.729562</td>\n",
       "      <td>5.731749</td>\n",
       "      <td>3.537448</td>\n",
       "      <td>3.904783</td>\n",
       "      <td>4.400766</td>\n",
       "      <td>4.049388</td>\n",
       "      <td>1.036433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3992.416147</td>\n",
       "      <td>0.472775</td>\n",
       "      <td>1.390932</td>\n",
       "      <td>2.202605</td>\n",
       "      <td>1.123510</td>\n",
       "      <td>1.115275</td>\n",
       "      <td>0.695006</td>\n",
       "      <td>1.117876</td>\n",
       "      <td>1.701260</td>\n",
       "      <td>1.648181</td>\n",
       "      <td>0.019357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>326.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>56.455457</td>\n",
       "      <td>49.182052</td>\n",
       "      <td>1.568917</td>\n",
       "      <td>1.490214</td>\n",
       "      <td>0.958026</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>945.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>4.710000</td>\n",
       "      <td>4.720000</td>\n",
       "      <td>2.910000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.031075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2397.000000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>61.800000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>5.690000</td>\n",
       "      <td>5.710000</td>\n",
       "      <td>3.520000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.038843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5331.000000</td>\n",
       "      <td>1.040000</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>6.540000</td>\n",
       "      <td>6.540000</td>\n",
       "      <td>4.035000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.048770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>18823.000000</td>\n",
       "      <td>2.557219</td>\n",
       "      <td>67.050225</td>\n",
       "      <td>65.710213</td>\n",
       "      <td>9.889867</td>\n",
       "      <td>9.975423</td>\n",
       "      <td>6.116282</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.069918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              price         carat         depth         table             x  \\\n",
       "count  40455.000000  40455.000000  40455.000000  40455.000000  40455.000000   \n",
       "mean    3928.444469      0.797105     61.750399     57.440964      5.729562   \n",
       "std     3992.416147      0.472775      1.390932      2.202605      1.123510   \n",
       "min      326.000000      0.200000     56.455457     49.182052      1.568917   \n",
       "25%      945.000000      0.400000     61.000000     56.000000      4.710000   \n",
       "50%     2397.000000      0.700000     61.800000     57.000000      5.690000   \n",
       "75%     5331.000000      1.040000     62.500000     59.000000      6.540000   \n",
       "max    18823.000000      2.557219     67.050225     65.710213      9.889867   \n",
       "\n",
       "                  y             z           cut         color       clarity  \\\n",
       "count  40455.000000  40455.000000  40455.000000  40455.000000  40455.000000   \n",
       "mean       5.731749      3.537448      3.904783      4.400766      4.049388   \n",
       "std        1.115275      0.695006      1.117876      1.701260      1.648181   \n",
       "min        1.490214      0.958026      1.000000      1.000000      1.000000   \n",
       "25%        4.720000      2.910000      3.000000      3.000000      3.000000   \n",
       "50%        5.710000      3.520000      4.000000      4.000000      4.000000   \n",
       "75%        6.540000      4.035000      5.000000      6.000000      5.000000   \n",
       "max        9.975423      6.116282      5.000000      7.000000      8.000000   \n",
       "\n",
       "               city  \n",
       "count  40455.000000  \n",
       "mean       1.036433  \n",
       "std        0.019357  \n",
       "min        1.000000  \n",
       "25%        1.031075  \n",
       "50%        1.038843  \n",
       "75%        1.048770  \n",
       "max        1.069918  "
      ]
     },
     "execution_count": 900,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outlier_columns = ['depth','table', 'x', 'y', 'z','carat']\n",
    "clean_df[outlier_columns] = sub_outlyers(clean_df[outlier_columns],3.7)\n",
    "clean_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 901,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_xy(x,y):\n",
    "    try:\n",
    "        return x/y\n",
    "    except:\n",
    "        return 1\n",
    "    \n",
    "def feature_ad(carat,x,y,z,):\n",
    "    try:\n",
    "        return carat/(x*y*z)\n",
    "    except:\n",
    "        return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 902,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_580/1906330868.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_df['x/y'] = clean_df.apply(lambda x : feature_xy(x['x'], x['y']), axis=1)\n",
      "/tmp/ipykernel_580/1906330868.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_df['td'] = clean_df.apply(lambda x : feature_xy(x['table'], x['depth']), axis=1)\n",
      "/tmp/ipykernel_580/1906330868.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_df['ad'] = clean_df.apply(lambda x : feature_ad(x['carat'],x['x'], x['y'], x['z']), axis=1)\n",
      "/tmp/ipykernel_580/1906330868.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_df['cc'] = clean_df['carat'] * clean_df['clarity']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>carat</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>city</th>\n",
       "      <th>x/y</th>\n",
       "      <th>td</th>\n",
       "      <th>ad</th>\n",
       "      <th>cc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3928.444469</td>\n",
       "      <td>0.797105</td>\n",
       "      <td>61.750399</td>\n",
       "      <td>57.440964</td>\n",
       "      <td>5.729562</td>\n",
       "      <td>5.731749</td>\n",
       "      <td>3.537448</td>\n",
       "      <td>3.904783</td>\n",
       "      <td>4.400766</td>\n",
       "      <td>4.049388</td>\n",
       "      <td>1.036433</td>\n",
       "      <td>0.999398</td>\n",
       "      <td>0.930940</td>\n",
       "      <td>0.006187</td>\n",
       "      <td>2.949350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3992.416147</td>\n",
       "      <td>0.472775</td>\n",
       "      <td>1.390932</td>\n",
       "      <td>2.202605</td>\n",
       "      <td>1.123510</td>\n",
       "      <td>1.115275</td>\n",
       "      <td>0.695006</td>\n",
       "      <td>1.117876</td>\n",
       "      <td>1.701260</td>\n",
       "      <td>1.648181</td>\n",
       "      <td>0.019357</td>\n",
       "      <td>0.011082</td>\n",
       "      <td>0.047216</td>\n",
       "      <td>0.006008</td>\n",
       "      <td>1.817597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>326.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>56.455457</td>\n",
       "      <td>49.182052</td>\n",
       "      <td>1.568917</td>\n",
       "      <td>1.490214</td>\n",
       "      <td>0.958026</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.236996</td>\n",
       "      <td>0.759091</td>\n",
       "      <td>0.002847</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>945.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>4.710000</td>\n",
       "      <td>4.720000</td>\n",
       "      <td>2.910000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.031075</td>\n",
       "      <td>0.992606</td>\n",
       "      <td>0.898876</td>\n",
       "      <td>0.006047</td>\n",
       "      <td>1.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2397.000000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>61.800000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>5.690000</td>\n",
       "      <td>5.710000</td>\n",
       "      <td>3.520000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.038843</td>\n",
       "      <td>0.995736</td>\n",
       "      <td>0.923825</td>\n",
       "      <td>0.006117</td>\n",
       "      <td>2.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5331.000000</td>\n",
       "      <td>1.040000</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>6.540000</td>\n",
       "      <td>6.540000</td>\n",
       "      <td>4.035000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.048770</td>\n",
       "      <td>1.006928</td>\n",
       "      <td>0.955519</td>\n",
       "      <td>0.006190</td>\n",
       "      <td>3.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>18823.000000</td>\n",
       "      <td>2.557219</td>\n",
       "      <td>67.050225</td>\n",
       "      <td>65.710213</td>\n",
       "      <td>9.889867</td>\n",
       "      <td>9.975423</td>\n",
       "      <td>6.116282</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.069918</td>\n",
       "      <td>1.615572</td>\n",
       "      <td>1.163930</td>\n",
       "      <td>1.004516</td>\n",
       "      <td>18.320000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              price         carat         depth         table             x  \\\n",
       "count  40455.000000  40455.000000  40455.000000  40455.000000  40455.000000   \n",
       "mean    3928.444469      0.797105     61.750399     57.440964      5.729562   \n",
       "std     3992.416147      0.472775      1.390932      2.202605      1.123510   \n",
       "min      326.000000      0.200000     56.455457     49.182052      1.568917   \n",
       "25%      945.000000      0.400000     61.000000     56.000000      4.710000   \n",
       "50%     2397.000000      0.700000     61.800000     57.000000      5.690000   \n",
       "75%     5331.000000      1.040000     62.500000     59.000000      6.540000   \n",
       "max    18823.000000      2.557219     67.050225     65.710213      9.889867   \n",
       "\n",
       "                  y             z           cut         color       clarity  \\\n",
       "count  40455.000000  40455.000000  40455.000000  40455.000000  40455.000000   \n",
       "mean       5.731749      3.537448      3.904783      4.400766      4.049388   \n",
       "std        1.115275      0.695006      1.117876      1.701260      1.648181   \n",
       "min        1.490214      0.958026      1.000000      1.000000      1.000000   \n",
       "25%        4.720000      2.910000      3.000000      3.000000      3.000000   \n",
       "50%        5.710000      3.520000      4.000000      4.000000      4.000000   \n",
       "75%        6.540000      4.035000      5.000000      6.000000      5.000000   \n",
       "max        9.975423      6.116282      5.000000      7.000000      8.000000   \n",
       "\n",
       "               city           x/y            td            ad            cc  \n",
       "count  40455.000000  40455.000000  40455.000000  40455.000000  40455.000000  \n",
       "mean       1.036433      0.999398      0.930940      0.006187      2.949350  \n",
       "std        0.019357      0.011082      0.047216      0.006008      1.817597  \n",
       "min        1.000000      0.236996      0.759091      0.002847      0.300000  \n",
       "25%        1.031075      0.992606      0.898876      0.006047      1.640000  \n",
       "50%        1.038843      0.995736      0.923825      0.006117      2.400000  \n",
       "75%        1.048770      1.006928      0.955519      0.006190      3.700000  \n",
       "max        1.069918      1.615572      1.163930      1.004516     18.320000  "
      ]
     },
     "execution_count": 902,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df['x/y'] = clean_df.apply(lambda x : feature_xy(x['x'], x['y']), axis=1)\n",
    "clean_df['td'] = clean_df.apply(lambda x : feature_xy(x['table'], x['depth']), axis=1)\n",
    "clean_df['ad'] = clean_df.apply(lambda x : feature_ad(x['carat'],x['x'], x['y'], x['z']), axis=1)\n",
    "clean_df['cc'] = clean_df['carat'] * clean_df['clarity']\n",
    "#clean_df['depth_z'] = (clean_df['depth']-clean_df['depth'].mean())**2/clean_df['depth'].std()\n",
    "\n",
    "clean_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 903,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_580/1346328751.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_df[['x/y','td','ad']] = sub_outlyers(clean_df[['x/y','td','ad']],3.7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>carat</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>city</th>\n",
       "      <th>x/y</th>\n",
       "      <th>td</th>\n",
       "      <th>ad</th>\n",
       "      <th>cc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3928.444469</td>\n",
       "      <td>0.797105</td>\n",
       "      <td>61.750399</td>\n",
       "      <td>57.440964</td>\n",
       "      <td>5.729562</td>\n",
       "      <td>5.731749</td>\n",
       "      <td>3.537448</td>\n",
       "      <td>3.904783</td>\n",
       "      <td>4.400766</td>\n",
       "      <td>4.049388</td>\n",
       "      <td>1.036433</td>\n",
       "      <td>0.999368</td>\n",
       "      <td>0.930839</td>\n",
       "      <td>0.006135</td>\n",
       "      <td>2.949350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3992.416147</td>\n",
       "      <td>0.472775</td>\n",
       "      <td>1.390932</td>\n",
       "      <td>2.202605</td>\n",
       "      <td>1.123510</td>\n",
       "      <td>1.115275</td>\n",
       "      <td>0.695006</td>\n",
       "      <td>1.117876</td>\n",
       "      <td>1.701260</td>\n",
       "      <td>1.648181</td>\n",
       "      <td>0.019357</td>\n",
       "      <td>0.008899</td>\n",
       "      <td>0.046798</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>1.817597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>326.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>56.455457</td>\n",
       "      <td>49.182052</td>\n",
       "      <td>1.568917</td>\n",
       "      <td>1.490214</td>\n",
       "      <td>0.958026</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.958396</td>\n",
       "      <td>0.759091</td>\n",
       "      <td>0.002847</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>945.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>4.710000</td>\n",
       "      <td>4.720000</td>\n",
       "      <td>2.910000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.031075</td>\n",
       "      <td>0.992606</td>\n",
       "      <td>0.898876</td>\n",
       "      <td>0.006047</td>\n",
       "      <td>1.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2397.000000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>61.800000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>5.690000</td>\n",
       "      <td>5.710000</td>\n",
       "      <td>3.520000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.038843</td>\n",
       "      <td>0.995736</td>\n",
       "      <td>0.923825</td>\n",
       "      <td>0.006117</td>\n",
       "      <td>2.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5331.000000</td>\n",
       "      <td>1.040000</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>6.540000</td>\n",
       "      <td>6.540000</td>\n",
       "      <td>4.035000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.048770</td>\n",
       "      <td>1.006928</td>\n",
       "      <td>0.955519</td>\n",
       "      <td>0.006190</td>\n",
       "      <td>3.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>18823.000000</td>\n",
       "      <td>2.557219</td>\n",
       "      <td>67.050225</td>\n",
       "      <td>65.710213</td>\n",
       "      <td>9.889867</td>\n",
       "      <td>9.975423</td>\n",
       "      <td>6.116282</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.069918</td>\n",
       "      <td>1.040400</td>\n",
       "      <td>1.105638</td>\n",
       "      <td>0.028417</td>\n",
       "      <td>18.320000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              price         carat         depth         table             x  \\\n",
       "count  40455.000000  40455.000000  40455.000000  40455.000000  40455.000000   \n",
       "mean    3928.444469      0.797105     61.750399     57.440964      5.729562   \n",
       "std     3992.416147      0.472775      1.390932      2.202605      1.123510   \n",
       "min      326.000000      0.200000     56.455457     49.182052      1.568917   \n",
       "25%      945.000000      0.400000     61.000000     56.000000      4.710000   \n",
       "50%     2397.000000      0.700000     61.800000     57.000000      5.690000   \n",
       "75%     5331.000000      1.040000     62.500000     59.000000      6.540000   \n",
       "max    18823.000000      2.557219     67.050225     65.710213      9.889867   \n",
       "\n",
       "                  y             z           cut         color       clarity  \\\n",
       "count  40455.000000  40455.000000  40455.000000  40455.000000  40455.000000   \n",
       "mean       5.731749      3.537448      3.904783      4.400766      4.049388   \n",
       "std        1.115275      0.695006      1.117876      1.701260      1.648181   \n",
       "min        1.490214      0.958026      1.000000      1.000000      1.000000   \n",
       "25%        4.720000      2.910000      3.000000      3.000000      3.000000   \n",
       "50%        5.710000      3.520000      4.000000      4.000000      4.000000   \n",
       "75%        6.540000      4.035000      5.000000      6.000000      5.000000   \n",
       "max        9.975423      6.116282      5.000000      7.000000      8.000000   \n",
       "\n",
       "               city           x/y            td            ad            cc  \n",
       "count  40455.000000  40455.000000  40455.000000  40455.000000  40455.000000  \n",
       "mean       1.036433      0.999368      0.930839      0.006135      2.949350  \n",
       "std        0.019357      0.008899      0.046798      0.000459      1.817597  \n",
       "min        1.000000      0.958396      0.759091      0.002847      0.300000  \n",
       "25%        1.031075      0.992606      0.898876      0.006047      1.640000  \n",
       "50%        1.038843      0.995736      0.923825      0.006117      2.400000  \n",
       "75%        1.048770      1.006928      0.955519      0.006190      3.700000  \n",
       "max        1.069918      1.040400      1.105638      0.028417     18.320000  "
      ]
     },
     "execution_count": 903,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df[['x/y','td','ad']] = sub_outlyers(clean_df[['x/y','td','ad']],3.7)\n",
    "clean_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40455 entries, 0 to 40454\n",
      "Data columns (total 15 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   price    40455 non-null  int64  \n",
      " 1   carat    40455 non-null  float64\n",
      " 2   depth    40455 non-null  float64\n",
      " 3   table    40455 non-null  float64\n",
      " 4   x        40455 non-null  float64\n",
      " 5   y        40455 non-null  float64\n",
      " 6   z        40455 non-null  float64\n",
      " 7   cut      40455 non-null  float64\n",
      " 8   color    40455 non-null  float64\n",
      " 9   clarity  40455 non-null  float64\n",
      " 10  city     40455 non-null  float64\n",
      " 11  x/y      40455 non-null  float64\n",
      " 12  td       40455 non-null  float64\n",
      " 13  ad       40455 non-null  float64\n",
      " 14  cc       40455 non-null  float64\n",
      "dtypes: float64(14), int64(1)\n",
      "memory usage: 4.6 MB\n"
     ]
    }
   ],
   "source": [
    "clean_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 905,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.to_csv('./data/clean/diamondsdl123fexytdadcc2xso37.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = clean_df.drop(['price'], axis=1)\n",
    "y = clean_df['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = UMAP(n_components=14).fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>city</th>\n",
       "      <th>x/y</th>\n",
       "      <th>td</th>\n",
       "      <th>ad</th>\n",
       "      <th>cc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.21</td>\n",
       "      <td>62.4</td>\n",
       "      <td>58.0</td>\n",
       "      <td>6.83</td>\n",
       "      <td>6.79</td>\n",
       "      <td>4.25</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.069918</td>\n",
       "      <td>1.005891</td>\n",
       "      <td>0.929487</td>\n",
       "      <td>0.006139</td>\n",
       "      <td>4.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.32</td>\n",
       "      <td>63.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>4.35</td>\n",
       "      <td>4.38</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.056107</td>\n",
       "      <td>0.993151</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.006107</td>\n",
       "      <td>1.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.71</td>\n",
       "      <td>65.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>5.62</td>\n",
       "      <td>5.53</td>\n",
       "      <td>3.65</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.038843</td>\n",
       "      <td>1.016275</td>\n",
       "      <td>0.839695</td>\n",
       "      <td>0.006259</td>\n",
       "      <td>3.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.41</td>\n",
       "      <td>63.8</td>\n",
       "      <td>56.0</td>\n",
       "      <td>4.68</td>\n",
       "      <td>4.72</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.056107</td>\n",
       "      <td>0.991525</td>\n",
       "      <td>0.877743</td>\n",
       "      <td>0.006187</td>\n",
       "      <td>1.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.02</td>\n",
       "      <td>60.5</td>\n",
       "      <td>59.0</td>\n",
       "      <td>6.55</td>\n",
       "      <td>6.51</td>\n",
       "      <td>3.95</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.069918</td>\n",
       "      <td>1.006144</td>\n",
       "      <td>0.975207</td>\n",
       "      <td>0.006056</td>\n",
       "      <td>3.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40450</th>\n",
       "      <td>1.34</td>\n",
       "      <td>62.7</td>\n",
       "      <td>57.0</td>\n",
       "      <td>7.10</td>\n",
       "      <td>7.04</td>\n",
       "      <td>4.43</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.035822</td>\n",
       "      <td>1.008523</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.006052</td>\n",
       "      <td>6.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40451</th>\n",
       "      <td>2.02</td>\n",
       "      <td>57.1</td>\n",
       "      <td>60.0</td>\n",
       "      <td>8.31</td>\n",
       "      <td>8.25</td>\n",
       "      <td>4.73</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.036038</td>\n",
       "      <td>1.007273</td>\n",
       "      <td>1.050788</td>\n",
       "      <td>0.006229</td>\n",
       "      <td>4.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40452</th>\n",
       "      <td>1.01</td>\n",
       "      <td>62.7</td>\n",
       "      <td>56.0</td>\n",
       "      <td>6.37</td>\n",
       "      <td>6.42</td>\n",
       "      <td>4.01</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.056107</td>\n",
       "      <td>0.992212</td>\n",
       "      <td>0.893142</td>\n",
       "      <td>0.006159</td>\n",
       "      <td>3.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40453</th>\n",
       "      <td>0.33</td>\n",
       "      <td>61.9</td>\n",
       "      <td>54.3</td>\n",
       "      <td>4.45</td>\n",
       "      <td>4.47</td>\n",
       "      <td>2.76</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.056107</td>\n",
       "      <td>0.995526</td>\n",
       "      <td>0.877221</td>\n",
       "      <td>0.006011</td>\n",
       "      <td>1.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40454</th>\n",
       "      <td>1.24</td>\n",
       "      <td>62.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>6.83</td>\n",
       "      <td>6.88</td>\n",
       "      <td>4.25</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.033233</td>\n",
       "      <td>0.992733</td>\n",
       "      <td>0.935484</td>\n",
       "      <td>0.006209</td>\n",
       "      <td>3.72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40455 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       carat  depth  table     x     y     z  cut  color  clarity      city  \\\n",
       "0       1.21   62.4   58.0  6.83  6.79  4.25  4.0    1.0      4.0  1.069918   \n",
       "1       0.32   63.0   57.0  4.35  4.38  2.75  3.0    3.0      4.0  1.056107   \n",
       "2       0.71   65.5   55.0  5.62  5.53  3.65  1.0    4.0      5.0  1.038843   \n",
       "3       0.41   63.8   56.0  4.68  4.72  3.00  2.0    7.0      3.0  1.056107   \n",
       "4       1.02   60.5   59.0  6.55  6.51  3.95  5.0    4.0      3.0  1.069918   \n",
       "...      ...    ...    ...   ...   ...   ...  ...    ...      ...       ...   \n",
       "40450   1.34   62.7   57.0  7.10  7.04  4.43  5.0    4.0      5.0  1.035822   \n",
       "40451   2.02   57.1   60.0  8.31  8.25  4.73  2.0    5.0      2.0  1.036038   \n",
       "40452   1.01   62.7   56.0  6.37  6.42  4.01  5.0    3.0      3.0  1.056107   \n",
       "40453   0.33   61.9   54.3  4.45  4.47  2.76  5.0    1.0      5.0  1.056107   \n",
       "40454   1.24   62.0   58.0  6.83  6.88  4.25  5.0    2.0      3.0  1.033233   \n",
       "\n",
       "            x/y        td        ad    cc  \n",
       "0      1.005891  0.929487  0.006139  4.84  \n",
       "1      0.993151  0.904762  0.006107  1.28  \n",
       "2      1.016275  0.839695  0.006259  3.55  \n",
       "3      0.991525  0.877743  0.006187  1.23  \n",
       "4      1.006144  0.975207  0.006056  3.06  \n",
       "...         ...       ...       ...   ...  \n",
       "40450  1.008523  0.909091  0.006052  6.70  \n",
       "40451  1.007273  1.050788  0.006229  4.04  \n",
       "40452  0.992212  0.893142  0.006159  3.03  \n",
       "40453  0.995526  0.877221  0.006011  1.65  \n",
       "40454  0.992733  0.935484  0.006209  3.72  \n",
       "\n",
       "[40455 rows x 14 columns]"
      ]
     },
     "execution_count": 886,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Initialize RFE with XGBoost model\\nrfe = RFE(estimator=xgb_model, n_features_to_select=len(X.columns)-1)\\n\\n# Fit RFE to training data\\nrfe.fit(X_train, y_train)\\n\\n# Get selected features\\nselected_features = X_train.columns[rfe.support_]\\nselected_features'"
      ]
     },
     "execution_count": 887,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the XGBoost regressor\n",
    "xgb_model = XGBRegressor( missing=np.inf, enable_categorical=True)\n",
    "\n",
    "# Optimal gridsearch with data labeling\n",
    "param_grid_dl = {\n",
    "    'n_estimators': [225, 230, 235],\n",
    "    'max_depth': [ 5, 6, 7],\n",
    "    'learning_rate': [ 0.075, 0.07, 0.065],\n",
    "    'missing': [np.inf]\n",
    "}\n",
    "\n",
    "param_grid_deep = {\n",
    "        'n_estimators': [ 210, 235, 250],\n",
    "        'min_child_weight': [ 12, 15, 17],\n",
    "        'alpha': [0.3,0.6],\n",
    "        'lambda': [0.3,0.6],\n",
    "        'gamma': [0.02, 0.05, 0.07],\n",
    "        'subsample': [0.8],\n",
    "        'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "        'max_depth': [6],\n",
    "        'learning_rate': [ 0.07],\n",
    "         }\n",
    "\n",
    "param_grid_dart = {\n",
    "    'booster': ['dart'],\n",
    "    'n_estimators': [230],\n",
    "    'max_depth': [  6],\n",
    "    'learning_rate': [ 0.07],\n",
    "    'min_child_weight': [1],\n",
    "              'rate_drop': [0.10],\n",
    "              'skip_drop': [0.5]\n",
    "}\n",
    "\n",
    "param_grid_en = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [ 7, 9, 11],\n",
    "    'learning_rate': [0.01, 0.1,0.2]\n",
    "}\n",
    "\n",
    "param_grid_aiama ={'colsample_bytree': [ 0.8, 0.9, 1],\n",
    "                    'learning_rate': [0.04, 0.05, 0.06],\n",
    "                      'max_depth': [5, 6, 7],\n",
    "                        'n_estimators': [200, 220, 240],\n",
    "                          'subsample': [0.8,0.9,1],\n",
    "                              'missing': [np.inf]} \n",
    "\n",
    "param_grid_art ={'colsample_bytree': [0.92, 0.95, 0.98],\n",
    "                    'learning_rate': [ 0.011, 0.012, 0.013],\n",
    "                      'max_depth': [7, 8, 9],\n",
    "                        'n_estimators': [ 1100, 1200, 1300],\n",
    "                          'subsample': [0.87, 0.88, 0.89],\n",
    "                              'missing': [np.inf],\n",
    "                                 'gamma': [0.1,0.2,0.3]} \n",
    "\n",
    "param_grid_artq ={'colsample_bytree': [0.92],\n",
    "                    'learning_rate': [ 0.011, 0.012, 0.013],\n",
    "                      'max_depth': [ 9, 10],\n",
    "                        'n_estimators': [1300],\n",
    "                          'subsample': [0.87, 0.88, 0.89],\n",
    "                              'missing': [np.inf]} \n",
    "\n",
    "param_grid_art2 ={'colsample_bytree': [0.92, 0.95, 0.98],\n",
    "                    'learning_rate': [ 0.011, 0.012, 0.013],\n",
    "                      'max_depth': [7, 8, 9],\n",
    "                        'n_estimators': [ 1100, 1200, 1300],\n",
    "                          'subsample': [0.87, 0.88, 0.89],\n",
    "                            'reg_lambda' : [0.001, 0.01,0.1],\n",
    "                              'missing': [np.inf]} \n",
    "\n",
    "param_grid_fine ={'colsample_bytree': [0.95],\n",
    "                    'learning_rate': [ 0.012, 0.0125, 0.013],\n",
    "                      'max_depth': [7],\n",
    "                        'n_estimators': [1120,1130,1140],\n",
    "                          'subsample': [0.79,0.8,0.82],\n",
    "                            'gamma' :[0.126,0.128,0.13],\n",
    "                              'missing': [np.inf]} \n",
    "\n",
    "param_grid_w = {'colsample_bytree': [0.95], 'gamma': [0.15], 'learning_rate': [0.012], 'max_depth': [7], 'missing': [np.inf], 'n_estimators': [1140], 'subsample': [0.8]}\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Initialize XGBoost model\n",
    "'''\n",
    "# Initialize RFE with XGBoost model\n",
    "rfe = RFE(estimator=xgb_model, n_features_to_select=len(X.columns)-1)\n",
    "\n",
    "# Fit RFE to training data\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "# Get selected features\n",
    "selected_features = X_train.columns[rfe.support_]\n",
    "selected_features'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 888,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1120, subsample=0.79;, score=-510.945 total time=   8.4s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1120, subsample=0.79;, score=-512.932 total time=   8.4s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1120, subsample=0.82;, score=-493.074 total time=   8.4s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1120, subsample=0.82;, score=-511.020 total time=   8.4s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1120, subsample=0.8;, score=-493.957 total time=   8.5s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1120, subsample=0.82;, score=-547.121 total time=   8.4s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1120, subsample=0.79;, score=-500.220 total time=   9.0s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1130, subsample=0.79;, score=-493.933 total time=   8.8s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1120, subsample=0.8;, score=-508.888 total time=   9.1s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1120, subsample=0.8;, score=-510.305 total time=   9.1s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1130, subsample=0.79;, score=-500.001 total time=   9.2s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1120, subsample=0.82;, score=-502.108 total time=   9.3s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1120, subsample=0.79;, score=-493.797 total time=  10.4s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1120, subsample=0.8;, score=-544.728 total time=  10.1s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1120, subsample=0.8;, score=-500.150 total time=  10.3s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1120, subsample=0.79;, score=-546.779 total time=  10.4s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1130, subsample=0.79;, score=-510.960 total time=  10.4s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1120, subsample=0.82;, score=-511.669 total time=  10.6s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1130, subsample=0.79;, score=-546.629 total time=  10.5s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1130, subsample=0.79;, score=-512.900 total time=  14.4s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1130, subsample=0.8;, score=-493.970 total time=   8.2s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1130, subsample=0.8;, score=-508.920 total time=   8.2s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1130, subsample=0.8;, score=-544.618 total time=   8.1s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1130, subsample=0.8;, score=-500.073 total time=   8.2s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1130, subsample=0.8;, score=-510.216 total time=   8.3s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1130, subsample=0.82;, score=-493.212 total time=   8.4s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1130, subsample=0.82;, score=-511.665 total time=   8.6s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1130, subsample=0.82;, score=-511.024 total time=   9.1s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1130, subsample=0.82;, score=-546.965 total time=   9.1s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1140, subsample=0.79;, score=-510.974 total time=   9.0s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1140, subsample=0.79;, score=-493.900 total time=   9.2s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1130, subsample=0.82;, score=-501.941 total time=  10.3s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1140, subsample=0.79;, score=-499.963 total time=   9.3s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1140, subsample=0.79;, score=-546.535 total time=  10.1s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1140, subsample=0.79;, score=-512.959 total time=  10.5s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1140, subsample=0.8;, score=-493.994 total time=  10.3s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1140, subsample=0.8;, score=-510.238 total time=  10.5s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1140, subsample=0.8;, score=-544.555 total time=  10.7s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1140, subsample=0.8;, score=-499.947 total time=   9.2s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1140, subsample=0.8;, score=-509.010 total time=  13.7s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1140, subsample=0.82;, score=-511.148 total time=   8.1s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1140, subsample=0.82;, score=-493.198 total time=   8.2s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1140, subsample=0.82;, score=-511.706 total time=   8.1s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1140, subsample=0.82;, score=-501.901 total time=   8.1s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1140, subsample=0.82;, score=-547.065 total time=   8.2s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1120, subsample=0.79;, score=-492.772 total time=   8.0s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1120, subsample=0.79;, score=-509.193 total time=   8.3s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1120, subsample=0.79;, score=-511.396 total time=   8.7s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1120, subsample=0.79;, score=-546.988 total time=   8.7s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1120, subsample=0.79;, score=-501.591 total time=   8.8s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1120, subsample=0.8;, score=-492.883 total time=   9.0s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1120, subsample=0.8;, score=-510.772 total time=   8.6s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1120, subsample=0.8;, score=-509.495 total time=   9.9s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1120, subsample=0.8;, score=-545.191 total time=   9.9s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1120, subsample=0.82;, score=-493.601 total time=   9.9s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1120, subsample=0.8;, score=-500.824 total time=  10.2s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1120, subsample=0.82;, score=-510.032 total time=  10.0s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1120, subsample=0.82;, score=-510.230 total time=  10.2s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1130, subsample=0.79;, score=-492.830 total time=   8.0s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1130, subsample=0.79;, score=-509.235 total time=   8.2s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1130, subsample=0.8;, score=-493.099 total time=   8.0s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1130, subsample=0.79;, score=-546.906 total time=   8.2s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1130, subsample=0.79;, score=-501.502 total time=   8.2s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1130, subsample=0.79;, score=-511.418 total time=   8.5s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1130, subsample=0.8;, score=-509.545 total time=   8.2s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1120, subsample=0.82;, score=-546.438 total time=  10.5s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1130, subsample=0.8;, score=-510.882 total time=   8.9s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1130, subsample=0.8;, score=-545.155 total time=   9.0s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1130, subsample=0.8;, score=-500.656 total time=   8.9s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1120, subsample=0.82;, score=-500.251 total time=  12.2s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1130, subsample=0.82;, score=-493.743 total time=   8.9s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1130, subsample=0.82;, score=-510.049 total time=   8.9s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1130, subsample=0.82;, score=-510.304 total time=   9.7s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1130, subsample=0.82;, score=-546.439 total time=  10.2s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1130, subsample=0.82;, score=-500.137 total time=   9.9s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1140, subsample=0.79;, score=-492.918 total time=   9.9s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1140, subsample=0.79;, score=-546.896 total time=   8.1s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1140, subsample=0.79;, score=-501.551 total time=   8.3s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1140, subsample=0.8;, score=-493.053 total time=   8.3s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1140, subsample=0.8;, score=-509.610 total time=   8.3s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1140, subsample=0.8;, score=-545.232 total time=   8.2s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1140, subsample=0.8;, score=-510.979 total time=   8.3s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1140, subsample=0.79;, score=-509.325 total time=  10.2s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1140, subsample=0.79;, score=-511.476 total time=  10.0s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1140, subsample=0.8;, score=-500.608 total time=   9.9s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1140, subsample=0.82;, score=-510.160 total time=   8.8s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1140, subsample=0.82;, score=-510.292 total time=   9.1s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1140, subsample=0.82;, score=-546.573 total time=   8.8s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1120, subsample=0.79;, score=-493.295 total time=   8.6s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1120, subsample=0.79;, score=-509.821 total time=   8.8s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1140, subsample=0.82;, score=-493.731 total time=  11.9s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1120, subsample=0.79;, score=-512.106 total time=   8.5s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1140, subsample=0.82;, score=-500.035 total time=  12.1s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1120, subsample=0.8;, score=-509.437 total time=   8.2s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1120, subsample=0.8;, score=-511.475 total time=   8.2s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1120, subsample=0.82;, score=-493.509 total time=   8.2s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1120, subsample=0.8;, score=-546.221 total time=   8.4s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1120, subsample=0.82;, score=-511.156 total time=   8.3s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1120, subsample=0.8;, score=-498.929 total time=   8.4s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1120, subsample=0.79;, score=-546.443 total time=   9.9s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1120, subsample=0.8;, score=-493.376 total time=   9.8s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1120, subsample=0.79;, score=-499.740 total time=  10.1s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1120, subsample=0.82;, score=-510.120 total time=  10.0s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1120, subsample=0.82;, score=-544.504 total time=  10.2s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1130, subsample=0.79;, score=-493.567 total time=   8.9s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1120, subsample=0.82;, score=-500.319 total time=   9.2s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1130, subsample=0.79;, score=-512.055 total time=   8.6s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1130, subsample=0.79;, score=-509.963 total time=   8.9s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1130, subsample=0.79;, score=-546.313 total time=   8.8s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1130, subsample=0.79;, score=-499.624 total time=   8.9s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1130, subsample=0.8;, score=-546.154 total time=   8.2s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1130, subsample=0.8;, score=-498.858 total time=   8.0s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1130, subsample=0.8;, score=-509.451 total time=  10.0s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1130, subsample=0.82;, score=-493.611 total time=   8.2s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1130, subsample=0.8;, score=-493.419 total time=  11.5s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1130, subsample=0.82;, score=-510.046 total time=   8.3s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1130, subsample=0.82;, score=-511.173 total time=   8.5s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1130, subsample=0.82;, score=-544.441 total time=   8.5s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1130, subsample=0.8;, score=-511.428 total time=  11.2s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1140, subsample=0.79;, score=-510.060 total time=   9.5s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1130, subsample=0.82;, score=-500.326 total time=  10.0s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1140, subsample=0.79;, score=-493.636 total time=  10.1s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1140, subsample=0.79;, score=-512.133 total time=  10.0s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1140, subsample=0.79;, score=-499.656 total time=   8.4s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1140, subsample=0.79;, score=-546.374 total time=   9.9s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1140, subsample=0.8;, score=-493.343 total time=   9.1s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1140, subsample=0.8;, score=-509.480 total time=   8.9s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1140, subsample=0.8;, score=-511.469 total time=   8.9s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1140, subsample=0.8;, score=-546.172 total time=   9.1s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1140, subsample=0.8;, score=-498.762 total time=   9.0s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1140, subsample=0.82;, score=-493.623 total time=   8.3s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1140, subsample=0.82;, score=-511.217 total time=   8.2s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1140, subsample=0.82;, score=-544.503 total time=   8.4s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1120, subsample=0.79;, score=-493.797 total time=   8.2s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1120, subsample=0.79;, score=-510.945 total time=   8.4s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1140, subsample=0.82;, score=-510.058 total time=  10.0s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1120, subsample=0.79;, score=-500.220 total time=   8.2s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1120, subsample=0.79;, score=-512.932 total time=  10.2s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1120, subsample=0.79;, score=-546.779 total time=   8.6s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.126, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1140, subsample=0.82;, score=-500.264 total time=  11.2s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1120, subsample=0.8;, score=-544.728 total time=   8.5s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1120, subsample=0.8;, score=-493.957 total time=  10.0s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1120, subsample=0.82;, score=-493.074 total time=   8.9s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1120, subsample=0.82;, score=-511.020 total time=   8.8s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1120, subsample=0.8;, score=-510.305 total time=  10.1s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1120, subsample=0.82;, score=-511.669 total time=   8.8s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1120, subsample=0.8;, score=-500.150 total time=   9.8s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1120, subsample=0.82;, score=-547.121 total time=   8.8s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1120, subsample=0.82;, score=-502.108 total time=   8.9s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1130, subsample=0.79;, score=-493.933 total time=   8.5s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1130, subsample=0.79;, score=-510.960 total time=   8.4s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1120, subsample=0.8;, score=-508.888 total time=  13.5s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1130, subsample=0.79;, score=-546.629 total time=   8.3s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1130, subsample=0.79;, score=-512.900 total time=   8.4s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1130, subsample=0.79;, score=-500.001 total time=   8.9s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1130, subsample=0.8;, score=-508.920 total time=   8.4s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1130, subsample=0.8;, score=-544.618 total time=   9.1s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1130, subsample=0.8;, score=-493.970 total time=  10.3s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1130, subsample=0.82;, score=-493.212 total time=   8.6s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1130, subsample=0.8;, score=-510.216 total time=  11.4s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1130, subsample=0.82;, score=-511.665 total time=   9.2s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1130, subsample=0.82;, score=-546.965 total time=   9.2s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1130, subsample=0.82;, score=-511.024 total time=  10.3s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1140, subsample=0.79;, score=-493.900 total time=   9.2s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1140, subsample=0.79;, score=-512.959 total time=   9.2s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1140, subsample=0.79;, score=-510.974 total time=  10.2s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1130, subsample=0.82;, score=-501.941 total time=  10.4s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1130, subsample=0.8;, score=-500.073 total time=  13.0s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1140, subsample=0.79;, score=-546.535 total time=   9.1s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1140, subsample=0.8;, score=-493.994 total time=   8.4s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1140, subsample=0.79;, score=-499.963 total time=   8.5s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1140, subsample=0.8;, score=-510.238 total time=   8.6s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1140, subsample=0.8;, score=-544.555 total time=   8.5s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1140, subsample=0.8;, score=-499.947 total time=   8.7s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1140, subsample=0.8;, score=-509.010 total time=  10.2s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1140, subsample=0.82;, score=-493.198 total time=   9.1s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1140, subsample=0.82;, score=-511.148 total time=   9.6s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1140, subsample=0.82;, score=-547.065 total time=   8.7s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1140, subsample=0.82;, score=-511.706 total time=  10.1s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1120, subsample=0.79;, score=-492.772 total time=   8.9s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1120, subsample=0.79;, score=-509.193 total time=   9.0s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1120, subsample=0.79;, score=-546.988 total time=   9.1s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1120, subsample=0.79;, score=-501.591 total time=   9.0s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1140, subsample=0.82;, score=-501.901 total time=  10.3s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1120, subsample=0.82;, score=-493.601 total time=   7.9s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1120, subsample=0.8;, score=-500.824 total time=   8.1s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1120, subsample=0.79;, score=-511.396 total time=   9.8s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1120, subsample=0.8;, score=-545.191 total time=   9.0s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1120, subsample=0.82;, score=-510.230 total time=   8.0s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1120, subsample=0.82;, score=-510.032 total time=   8.2s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1120, subsample=0.8;, score=-509.495 total time=  10.0s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1120, subsample=0.8;, score=-492.883 total time=  10.1s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1120, subsample=0.82;, score=-546.438 total time=   8.6s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1130, subsample=0.79;, score=-492.830 total time=   8.6s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1120, subsample=0.82;, score=-500.251 total time=  10.2s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1130, subsample=0.79;, score=-511.418 total time=   8.1s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1130, subsample=0.79;, score=-509.235 total time=   8.9s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1120, subsample=0.8;, score=-510.772 total time=  14.9s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1130, subsample=0.8;, score=-493.099 total time=   8.7s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1130, subsample=0.8;, score=-500.656 total time=   7.9s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1130, subsample=0.79;, score=-501.502 total time=   9.0s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1130, subsample=0.79;, score=-546.906 total time=  10.2s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1130, subsample=0.8;, score=-510.882 total time=   8.5s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1130, subsample=0.8;, score=-509.545 total time=   8.7s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1130, subsample=0.82;, score=-493.743 total time=   8.3s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1130, subsample=0.82;, score=-546.439 total time=   8.3s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1130, subsample=0.82;, score=-500.137 total time=   8.5s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1130, subsample=0.82;, score=-510.304 total time=   8.7s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1130, subsample=0.82;, score=-510.049 total time=  10.1s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1140, subsample=0.79;, score=-511.476 total time=   8.7s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1140, subsample=0.79;, score=-492.918 total time=   9.9s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1140, subsample=0.79;, score=-509.325 total time=  10.4s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1130, subsample=0.8;, score=-545.155 total time=  12.6s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1140, subsample=0.79;, score=-546.896 total time=   8.8s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1140, subsample=0.8;, score=-493.053 total time=   8.8s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1140, subsample=0.8;, score=-509.610 total time=   9.2s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1140, subsample=0.79;, score=-501.551 total time=  11.1s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1140, subsample=0.8;, score=-500.608 total time=   8.7s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1140, subsample=0.82;, score=-500.035 total time=   8.8s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1140, subsample=0.8;, score=-545.232 total time=   9.5s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1140, subsample=0.82;, score=-493.731 total time=   9.4s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1140, subsample=0.82;, score=-510.292 total time=   9.4s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1120, subsample=0.79;, score=-493.295 total time=   8.6s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1140, subsample=0.82;, score=-546.573 total time=   9.5s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1120, subsample=0.79;, score=-512.106 total time=   9.3s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1140, subsample=0.82;, score=-510.160 total time=  10.9s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1140, subsample=0.8;, score=-510.979 total time=  12.7s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1120, subsample=0.79;, score=-509.821 total time=  10.0s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1120, subsample=0.79;, score=-499.740 total time=   8.5s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1120, subsample=0.8;, score=-493.376 total time=   8.8s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1120, subsample=0.8;, score=-546.221 total time=   8.0s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1120, subsample=0.79;, score=-546.443 total time=  11.0s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1120, subsample=0.8;, score=-498.929 total time=   8.1s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1120, subsample=0.8;, score=-509.437 total time=  10.8s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1120, subsample=0.8;, score=-511.475 total time=  10.6s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1120, subsample=0.82;, score=-493.509 total time=   8.8s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1120, subsample=0.82;, score=-510.120 total time=   7.7s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1120, subsample=0.82;, score=-544.504 total time=   7.8s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1130, subsample=0.79;, score=-512.055 total time=   7.7s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1120, subsample=0.82;, score=-511.156 total time=  10.4s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1120, subsample=0.82;, score=-500.319 total time=   8.6s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1130, subsample=0.79;, score=-493.567 total time=   8.8s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1130, subsample=0.79;, score=-509.963 total time=   9.0s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1130, subsample=0.79;, score=-546.313 total time=   9.0s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1130, subsample=0.79;, score=-499.624 total time=   9.0s[CV 4/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1130, subsample=0.8;, score=-546.154 total time=   8.2s\n",
      "\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1130, subsample=0.8;, score=-498.858 total time=   7.7s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1130, subsample=0.82;, score=-493.611 total time=   7.7s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1130, subsample=0.8;, score=-493.419 total time=  10.3s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1130, subsample=0.8;, score=-509.451 total time=  10.2s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1130, subsample=0.82;, score=-510.046 total time=   8.2s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1130, subsample=0.8;, score=-511.428 total time=  12.1s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1130, subsample=0.82;, score=-511.173 total time=  10.1s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1140, subsample=0.79;, score=-510.060 total time=   8.2s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1140, subsample=0.79;, score=-493.636 total time=   9.1s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1140, subsample=0.79;, score=-512.133 total time=   7.9s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1130, subsample=0.82;, score=-544.441 total time=  10.2s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1140, subsample=0.79;, score=-546.374 total time=   7.9s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1130, subsample=0.82;, score=-500.326 total time=  10.5s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1140, subsample=0.8;, score=-493.343 total time=   9.3s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1140, subsample=0.8;, score=-509.480 total time=   9.3s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1140, subsample=0.8;, score=-511.469 total time=   9.2s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1140, subsample=0.82;, score=-511.217 total time=   8.1s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1140, subsample=0.79;, score=-499.656 total time=  10.6s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1140, subsample=0.8;, score=-546.172 total time=   9.5s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1140, subsample=0.8;, score=-498.762 total time=   8.6s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1140, subsample=0.82;, score=-510.058 total time=   8.0s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1120, subsample=0.79;, score=-493.797 total time=   8.1s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1140, subsample=0.82;, score=-544.503 total time=  10.6s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1140, subsample=0.82;, score=-500.264 total time=  10.5s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1120, subsample=0.79;, score=-546.779 total time=   8.0s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1120, subsample=0.79;, score=-512.932 total time=   9.6s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1120, subsample=0.8;, score=-493.957 total time=   8.2s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1120, subsample=0.8;, score=-544.728 total time=   8.2s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1120, subsample=0.79;, score=-510.945 total time=  10.6s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1120, subsample=0.79;, score=-500.220 total time=   9.6s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.128, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1140, subsample=0.82;, score=-493.623 total time=  15.1s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1120, subsample=0.8;, score=-508.888 total time=  10.6s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1120, subsample=0.8;, score=-510.305 total time=  10.7s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1120, subsample=0.82;, score=-511.669 total time=   8.1s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1120, subsample=0.8;, score=-500.150 total time=   9.3s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1130, subsample=0.79;, score=-510.960 total time=   8.1s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1130, subsample=0.79;, score=-493.933 total time=   8.4s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1120, subsample=0.82;, score=-493.074 total time=   9.3s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1120, subsample=0.82;, score=-511.020 total time=   9.3s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1120, subsample=0.82;, score=-502.108 total time=   9.2s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1130, subsample=0.79;, score=-512.900 total time=   7.9s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1120, subsample=0.82;, score=-547.121 total time=  10.2s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1130, subsample=0.8;, score=-493.970 total time=   8.0s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1130, subsample=0.8;, score=-510.216 total time=   8.0s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1130, subsample=0.8;, score=-544.618 total time=   8.0s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1130, subsample=0.79;, score=-546.629 total time=  10.2s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1130, subsample=0.79;, score=-500.001 total time=  10.3s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1130, subsample=0.8;, score=-508.920 total time=   9.6s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1130, subsample=0.8;, score=-500.073 total time=  10.3s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1130, subsample=0.82;, score=-493.212 total time=  10.1s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1140, subsample=0.79;, score=-510.974 total time=   8.5s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1130, subsample=0.82;, score=-501.941 total time=   9.1s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1140, subsample=0.79;, score=-512.959 total time=   8.7s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1140, subsample=0.79;, score=-493.900 total time=   9.9s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1130, subsample=0.82;, score=-511.665 total time=  10.9s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1140, subsample=0.8;, score=-509.010 total time=   8.4s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1140, subsample=0.79;, score=-546.535 total time=   9.8s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1130, subsample=0.82;, score=-546.965 total time=  10.8s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1140, subsample=0.79;, score=-499.963 total time=  10.3s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1140, subsample=0.8;, score=-493.994 total time=  10.1s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1130, subsample=0.82;, score=-511.024 total time=  13.7s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1140, subsample=0.8;, score=-544.555 total time=   8.4s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1140, subsample=0.8;, score=-499.947 total time=   8.5s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1140, subsample=0.8;, score=-510.238 total time=  11.1s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1140, subsample=0.82;, score=-493.198 total time=   8.4s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1140, subsample=0.82;, score=-511.148 total time=   9.9s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1140, subsample=0.82;, score=-501.901 total time=   8.2s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1140, subsample=0.82;, score=-547.065 total time=  10.1s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1140, subsample=0.82;, score=-511.706 total time=  10.6s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1120, subsample=0.79;, score=-509.193 total time=   8.0s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1120, subsample=0.79;, score=-546.988 total time=   8.1s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1120, subsample=0.8;, score=-509.495 total time=   8.0s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1120, subsample=0.79;, score=-492.772 total time=  10.7s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1120, subsample=0.79;, score=-501.591 total time=   8.9s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1120, subsample=0.8;, score=-510.772 total time=   8.8s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1120, subsample=0.79;, score=-511.396 total time=  10.2s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1120, subsample=0.8;, score=-492.883 total time=   9.9s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1120, subsample=0.82;, score=-510.230 total time=   7.8s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1120, subsample=0.82;, score=-493.601 total time=   9.3s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1120, subsample=0.8;, score=-545.191 total time=  10.1s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1120, subsample=0.8;, score=-500.824 total time=   9.8s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1120, subsample=0.82;, score=-546.438 total time=   7.9s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1130, subsample=0.79;, score=-492.830 total time=   7.9s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1120, subsample=0.82;, score=-500.251 total time=  10.3s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1130, subsample=0.79;, score=-511.418 total time=   7.7s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1130, subsample=0.8;, score=-493.099 total time=   7.7s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1130, subsample=0.79;, score=-509.235 total time=   9.1s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1130, subsample=0.8;, score=-509.545 total time=   8.1s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1120, subsample=0.82;, score=-510.032 total time=  13.4s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1130, subsample=0.79;, score=-546.906 total time=   9.3s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1130, subsample=0.8;, score=-510.882 total time=   7.6s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1130, subsample=0.79;, score=-501.502 total time=  10.4s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1130, subsample=0.8;, score=-545.155 total time=   9.1s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1130, subsample=0.8;, score=-500.656 total time=   9.0s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1130, subsample=0.82;, score=-546.439 total time=   7.9s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1130, subsample=0.82;, score=-493.743 total time=   9.2s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1140, subsample=0.79;, score=-511.476 total time=   7.8s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1130, subsample=0.82;, score=-510.049 total time=  10.1s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1130, subsample=0.82;, score=-500.137 total time=   9.0s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1140, subsample=0.79;, score=-546.896 total time=   8.0s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1130, subsample=0.82;, score=-510.304 total time=  10.2s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1140, subsample=0.79;, score=-509.325 total time=  10.3s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1140, subsample=0.79;, score=-492.918 total time=  10.5s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1140, subsample=0.8;, score=-493.053 total time=   7.7s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1140, subsample=0.8;, score=-509.610 total time=   7.9s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1140, subsample=0.8;, score=-510.979 total time=   8.9s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1140, subsample=0.8;, score=-545.232 total time=   8.3s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1140, subsample=0.82;, score=-510.160 total time=   8.0s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1140, subsample=0.79;, score=-501.551 total time=  10.3s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1140, subsample=0.82;, score=-493.731 total time=   9.7s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1120, subsample=0.79;, score=-493.295 total time=   8.0s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1120, subsample=0.79;, score=-512.106 total time=   7.9s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1140, subsample=0.82;, score=-500.035 total time=   9.1s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1120, subsample=0.79;, score=-509.821 total time=   8.7s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1140, subsample=0.82;, score=-510.292 total time=  10.4s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1140, subsample=0.82;, score=-546.573 total time=   9.6s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1120, subsample=0.8;, score=-493.376 total time=   7.8s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1120, subsample=0.79;, score=-499.740 total time=   9.4s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.0125, max_depth=7, missing=inf, n_estimators=1140, subsample=0.8;, score=-500.608 total time=  13.8s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1120, subsample=0.8;, score=-498.929 total time=   7.9s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1120, subsample=0.79;, score=-546.443 total time=  10.3s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1120, subsample=0.82;, score=-493.509 total time=   8.0s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1120, subsample=0.8;, score=-509.437 total time=  10.6s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1120, subsample=0.8;, score=-511.475 total time=  10.6s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1120, subsample=0.8;, score=-546.221 total time=  10.5s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1120, subsample=0.82;, score=-510.120 total time=   8.2s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1120, subsample=0.82;, score=-544.504 total time=   8.2s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1120, subsample=0.82;, score=-511.156 total time=   9.1s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1130, subsample=0.79;, score=-509.963 total time=   7.8s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1120, subsample=0.82;, score=-500.319 total time=  10.4s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1130, subsample=0.79;, score=-512.055 total time=   8.0s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1130, subsample=0.79;, score=-493.567 total time=   9.5s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1130, subsample=0.8;, score=-511.428 total time=   7.8s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1130, subsample=0.79;, score=-546.313 total time=   9.1s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1130, subsample=0.79;, score=-499.624 total time=   9.3s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1130, subsample=0.82;, score=-493.611 total time=   7.9s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1130, subsample=0.8;, score=-493.419 total time=  10.3s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1130, subsample=0.8;, score=-546.154 total time=   9.0s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1130, subsample=0.82;, score=-510.046 total time=   7.9s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1130, subsample=0.8;, score=-498.858 total time=   9.7s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1130, subsample=0.82;, score=-511.173 total time=  10.2s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1140, subsample=0.79;, score=-512.133 total time=   8.0s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1140, subsample=0.79;, score=-510.060 total time=   8.3s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1130, subsample=0.8;, score=-509.451 total time=  13.1s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1130, subsample=0.82;, score=-544.441 total time=   9.9s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1130, subsample=0.82;, score=-500.326 total time=   9.6s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1140, subsample=0.79;, score=-546.374 total time=   8.7s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1140, subsample=0.79;, score=-493.636 total time=   9.7s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1140, subsample=0.79;, score=-499.656 total time=   7.6s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1140, subsample=0.8;, score=-509.480 total time=   7.3s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1140, subsample=0.8;, score=-511.469 total time=   7.4s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1140, subsample=0.8;, score=-493.343 total time=   8.1s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1140, subsample=0.8;, score=-546.172 total time=   7.0s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1140, subsample=0.8;, score=-498.762 total time=   6.6s\n",
      "[CV 1/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1140, subsample=0.82;, score=-493.623 total time=   6.4s\n",
      "[CV 2/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1140, subsample=0.82;, score=-511.217 total time=   5.7s\n",
      "[CV 4/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1140, subsample=0.82;, score=-544.503 total time=   5.5s\n",
      "[CV 3/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1140, subsample=0.82;, score=-510.058 total time=   5.7s\n",
      "[CV 5/5] END colsample_bytree=0.95, gamma=0.13, learning_rate=0.013, max_depth=7, missing=inf, n_estimators=1140, subsample=0.82;, score=-500.264 total time=   5.3s\n",
      "Best parameters found:  {'colsample_bytree': 0.95, 'gamma': 0.126, 'learning_rate': 0.012, 'max_depth': 7, 'missing': inf, 'n_estimators': 1140, 'subsample': 0.8}\n",
      "Best score found:  -511.54902916207436\n",
      "Test score with best parameters:  0.9934548081316547\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid_fine, cv=5, n_jobs=-1, verbose=3, scoring='neg_root_mean_squared_error')\n",
    "\n",
    "# Fit the GridSearchCV instance to the training data\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Get the best parameters and the best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Best parameters found: \", best_params)\n",
    "print(\"Best score found: \", best_score)\n",
    "\n",
    "# Evaluate the model on the test set using the best parameters\n",
    "best_model = grid_search.best_estimator_\n",
    "test_score = best_model.score(X_test, y_test)\n",
    "print(\"Test score with best parameters: \", test_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "ax = plot_importance(best_model, max_num_features=len(X.columns)+1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rf_regressor = RandomForestRegressor()\\n\\n# Define the parameter grid to search through\\nparam_grid_rf = {\\n    \\'n_estimators\\': [300, 400, 500],\\n    \\'max_depth\\': [None],\\n    \\'min_samples_split\\': [2, 3],\\n    \\'min_samples_leaf\\': [2,3]\\n}\\n\\n# Initialize GridSearchCV\\ngrid_search = GridSearchCV(estimator=rf_regressor, param_grid=param_grid_rf, cv=3, n_jobs=-1, verbose=2, scoring=\\'neg_root_mean_squared_error\\')\\nX_nan = np.nan_to_num(X.astype(np.float32))\\n# Fit the GridSearchCV instance to the training data\\ngrid_search.fit(X_nan, y)\\n\\n# Get the best parameters and the best score\\nbest_params = grid_search.best_params_\\nbest_score = grid_search.best_score_\\nbest_model = grid_search.best_estimator_\\n\\nprint(\"Best parameters found: \", best_params)\\nprint(\"Best score found: \", best_score)'"
      ]
     },
     "execution_count": 850,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''rf_regressor = RandomForestRegressor()\n",
    "\n",
    "# Define the parameter grid to search through\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [300, 400, 500],\n",
    "    'max_depth': [None],\n",
    "    'min_samples_split': [2, 3],\n",
    "    'min_samples_leaf': [2,3]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=rf_regressor, param_grid=param_grid_rf, cv=3, n_jobs=-1, verbose=2, scoring='neg_root_mean_squared_error')\n",
    "X_nan = np.nan_to_num(X.astype(np.float32))\n",
    "# Fit the GridSearchCV instance to the training data\n",
    "grid_search.fit(X_nan, y)\n",
    "\n",
    "# Get the best parameters and the best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "print(\"Best parameters found: \", best_params)\n",
    "print(\"Best score found: \", best_score)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'param_grid_lr = {\\n    \\'penalty\\': [\\'l1\\', \\'l2\\'],\\n    \\'C\\': [ 0.1, 1, 10,]\\n}\\nlogistic = LogisticRegression()\\ngrid_search = GridSearchCV(estimator=logistic, param_grid=param_grid_lr, cv=3, n_jobs=None, verbose=2, scoring=\\'neg_root_mean_squared_error\\')\\n\\n# Fit the GridSearchCV instance to the training data\\ngrid_search.fit(X_nan, y)\\n\\n# Get the best parameters and the best score\\nbest_params = grid_search.best_params_\\nbest_score = grid_search.best_score_\\n\\nprint(\"Best parameters found: \", best_params)\\nprint(\"Best score found: \", best_score)\\n'"
      ]
     },
     "execution_count": 851,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''param_grid_lr = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [ 0.1, 1, 10,]\n",
    "}\n",
    "logistic = LogisticRegression()\n",
    "grid_search = GridSearchCV(estimator=logistic, param_grid=param_grid_lr, cv=3, n_jobs=None, verbose=2, scoring='neg_root_mean_squared_error')\n",
    "\n",
    "# Fit the GridSearchCV instance to the training data\n",
    "grid_search.fit(X_nan, y)\n",
    "\n",
    "# Get the best parameters and the best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Best parameters found: \", best_params)\n",
    "print(\"Best score found: \", best_score)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.79</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>F</td>\n",
       "      <td>SI1</td>\n",
       "      <td>62.7</td>\n",
       "      <td>60.0</td>\n",
       "      <td>5.82</td>\n",
       "      <td>5.89</td>\n",
       "      <td>3.67</td>\n",
       "      <td>Amsterdam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.20</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>J</td>\n",
       "      <td>VS1</td>\n",
       "      <td>61.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>6.81</td>\n",
       "      <td>6.89</td>\n",
       "      <td>4.18</td>\n",
       "      <td>Surat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.57</td>\n",
       "      <td>Premium</td>\n",
       "      <td>H</td>\n",
       "      <td>SI1</td>\n",
       "      <td>62.2</td>\n",
       "      <td>61.0</td>\n",
       "      <td>7.38</td>\n",
       "      <td>7.32</td>\n",
       "      <td>4.57</td>\n",
       "      <td>Kimberly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.90</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>F</td>\n",
       "      <td>SI1</td>\n",
       "      <td>63.8</td>\n",
       "      <td>54.0</td>\n",
       "      <td>6.09</td>\n",
       "      <td>6.13</td>\n",
       "      <td>3.90</td>\n",
       "      <td>Kimberly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.50</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>F</td>\n",
       "      <td>VS1</td>\n",
       "      <td>62.9</td>\n",
       "      <td>58.0</td>\n",
       "      <td>5.05</td>\n",
       "      <td>5.09</td>\n",
       "      <td>3.19</td>\n",
       "      <td>Amsterdam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13480</th>\n",
       "      <td>0.57</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>E</td>\n",
       "      <td>SI1</td>\n",
       "      <td>61.9</td>\n",
       "      <td>56.0</td>\n",
       "      <td>5.35</td>\n",
       "      <td>5.32</td>\n",
       "      <td>3.30</td>\n",
       "      <td>Amsterdam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13481</th>\n",
       "      <td>0.71</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>I</td>\n",
       "      <td>VS2</td>\n",
       "      <td>62.2</td>\n",
       "      <td>55.0</td>\n",
       "      <td>5.71</td>\n",
       "      <td>5.73</td>\n",
       "      <td>3.56</td>\n",
       "      <td>New York City</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13482</th>\n",
       "      <td>0.70</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>F</td>\n",
       "      <td>VS1</td>\n",
       "      <td>61.6</td>\n",
       "      <td>55.0</td>\n",
       "      <td>5.75</td>\n",
       "      <td>5.71</td>\n",
       "      <td>3.53</td>\n",
       "      <td>Tel Aviv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13483</th>\n",
       "      <td>0.70</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>F</td>\n",
       "      <td>SI2</td>\n",
       "      <td>58.8</td>\n",
       "      <td>57.0</td>\n",
       "      <td>5.85</td>\n",
       "      <td>5.89</td>\n",
       "      <td>3.45</td>\n",
       "      <td>Surat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13484</th>\n",
       "      <td>0.40</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>I</td>\n",
       "      <td>VVS2</td>\n",
       "      <td>62.4</td>\n",
       "      <td>55.0</td>\n",
       "      <td>4.70</td>\n",
       "      <td>4.73</td>\n",
       "      <td>2.94</td>\n",
       "      <td>New York City</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13485 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       carat        cut color clarity  depth  table     x     y     z  \\\n",
       "0       0.79  Very Good     F     SI1   62.7   60.0  5.82  5.89  3.67   \n",
       "1       1.20      Ideal     J     VS1   61.0   57.0  6.81  6.89  4.18   \n",
       "2       1.57    Premium     H     SI1   62.2   61.0  7.38  7.32  4.57   \n",
       "3       0.90  Very Good     F     SI1   63.8   54.0  6.09  6.13  3.90   \n",
       "4       0.50  Very Good     F     VS1   62.9   58.0  5.05  5.09  3.19   \n",
       "...      ...        ...   ...     ...    ...    ...   ...   ...   ...   \n",
       "13480   0.57      Ideal     E     SI1   61.9   56.0  5.35  5.32  3.30   \n",
       "13481   0.71      Ideal     I     VS2   62.2   55.0  5.71  5.73  3.56   \n",
       "13482   0.70      Ideal     F     VS1   61.6   55.0  5.75  5.71  3.53   \n",
       "13483   0.70  Very Good     F     SI2   58.8   57.0  5.85  5.89  3.45   \n",
       "13484   0.40      Ideal     I    VVS2   62.4   55.0  4.70  4.73  2.94   \n",
       "\n",
       "                city  \n",
       "0          Amsterdam  \n",
       "1              Surat  \n",
       "2           Kimberly  \n",
       "3           Kimberly  \n",
       "4          Amsterdam  \n",
       "...              ...  \n",
       "13480      Amsterdam  \n",
       "13481  New York City  \n",
       "13482       Tel Aviv  \n",
       "13483          Surat  \n",
       "13484  New York City  \n",
       "\n",
       "[13485 rows x 10 columns]"
      ]
     },
     "execution_count": 852,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('./data/src/diamonds_test.csv')\n",
    "test['price'] = 0\n",
    "submit_df = test[['id', 'price']]\n",
    "test = test.drop(['id','price'], axis = 1)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>13485.000000</td>\n",
       "      <td>13485.000000</td>\n",
       "      <td>13485.000000</td>\n",
       "      <td>13485.000000</td>\n",
       "      <td>13485.000000</td>\n",
       "      <td>13485.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.798642</td>\n",
       "      <td>61.739095</td>\n",
       "      <td>57.490337</td>\n",
       "      <td>5.736454</td>\n",
       "      <td>5.739648</td>\n",
       "      <td>3.543474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.469399</td>\n",
       "      <td>1.435310</td>\n",
       "      <td>2.237109</td>\n",
       "      <td>1.113671</td>\n",
       "      <td>1.128507</td>\n",
       "      <td>0.731005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>50.800000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>4.730000</td>\n",
       "      <td>4.730000</td>\n",
       "      <td>2.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.700000</td>\n",
       "      <td>61.900000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>5.700000</td>\n",
       "      <td>5.720000</td>\n",
       "      <td>3.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.040000</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>6.530000</td>\n",
       "      <td>6.530000</td>\n",
       "      <td>4.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.010000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>10.740000</td>\n",
       "      <td>31.800000</td>\n",
       "      <td>31.800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              carat         depth         table             x             y  \\\n",
       "count  13485.000000  13485.000000  13485.000000  13485.000000  13485.000000   \n",
       "mean       0.798642     61.739095     57.490337      5.736454      5.739648   \n",
       "std        0.469399      1.435310      2.237109      1.113671      1.128507   \n",
       "min        0.200000     50.800000     51.000000      0.000000      0.000000   \n",
       "25%        0.400000     61.000000     56.000000      4.730000      4.730000   \n",
       "50%        0.700000     61.900000     57.000000      5.700000      5.720000   \n",
       "75%        1.040000     62.500000     59.000000      6.530000      6.530000   \n",
       "max        5.010000     79.000000     73.000000     10.740000     31.800000   \n",
       "\n",
       "                  z  \n",
       "count  13485.000000  \n",
       "mean       3.543474  \n",
       "std        0.731005  \n",
       "min        0.000000  \n",
       "25%        2.920000  \n",
       "50%        3.530000  \n",
       "75%        4.040000  \n",
       "max       31.800000  "
      ]
     },
     "execution_count": 853,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>city</th>\n",
       "      <th>x/y</th>\n",
       "      <th>td</th>\n",
       "      <th>ad</th>\n",
       "      <th>cc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.797105</td>\n",
       "      <td>61.750399</td>\n",
       "      <td>57.440964</td>\n",
       "      <td>5.729562</td>\n",
       "      <td>5.731749</td>\n",
       "      <td>3.537448</td>\n",
       "      <td>3.904783</td>\n",
       "      <td>4.400766</td>\n",
       "      <td>4.049388</td>\n",
       "      <td>1.036433</td>\n",
       "      <td>0.999368</td>\n",
       "      <td>0.930839</td>\n",
       "      <td>0.006135</td>\n",
       "      <td>2.949350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.472775</td>\n",
       "      <td>1.390932</td>\n",
       "      <td>2.202605</td>\n",
       "      <td>1.123510</td>\n",
       "      <td>1.115275</td>\n",
       "      <td>0.695006</td>\n",
       "      <td>1.117876</td>\n",
       "      <td>1.701260</td>\n",
       "      <td>1.648181</td>\n",
       "      <td>0.019357</td>\n",
       "      <td>0.008899</td>\n",
       "      <td>0.046798</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>1.817597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>56.455457</td>\n",
       "      <td>49.182052</td>\n",
       "      <td>1.568917</td>\n",
       "      <td>1.490214</td>\n",
       "      <td>0.958026</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.958396</td>\n",
       "      <td>0.759091</td>\n",
       "      <td>0.002847</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>4.710000</td>\n",
       "      <td>4.720000</td>\n",
       "      <td>2.910000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.031075</td>\n",
       "      <td>0.992606</td>\n",
       "      <td>0.898876</td>\n",
       "      <td>0.006047</td>\n",
       "      <td>1.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.700000</td>\n",
       "      <td>61.800000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>5.690000</td>\n",
       "      <td>5.710000</td>\n",
       "      <td>3.520000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.038843</td>\n",
       "      <td>0.995736</td>\n",
       "      <td>0.923825</td>\n",
       "      <td>0.006117</td>\n",
       "      <td>2.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.040000</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>6.540000</td>\n",
       "      <td>6.540000</td>\n",
       "      <td>4.035000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.048770</td>\n",
       "      <td>1.006928</td>\n",
       "      <td>0.955519</td>\n",
       "      <td>0.006190</td>\n",
       "      <td>3.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.557219</td>\n",
       "      <td>67.050225</td>\n",
       "      <td>65.710213</td>\n",
       "      <td>9.889867</td>\n",
       "      <td>9.975423</td>\n",
       "      <td>6.116282</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.069918</td>\n",
       "      <td>1.040400</td>\n",
       "      <td>1.105638</td>\n",
       "      <td>0.028417</td>\n",
       "      <td>18.320000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              carat         depth         table             x             y  \\\n",
       "count  40455.000000  40455.000000  40455.000000  40455.000000  40455.000000   \n",
       "mean       0.797105     61.750399     57.440964      5.729562      5.731749   \n",
       "std        0.472775      1.390932      2.202605      1.123510      1.115275   \n",
       "min        0.200000     56.455457     49.182052      1.568917      1.490214   \n",
       "25%        0.400000     61.000000     56.000000      4.710000      4.720000   \n",
       "50%        0.700000     61.800000     57.000000      5.690000      5.710000   \n",
       "75%        1.040000     62.500000     59.000000      6.540000      6.540000   \n",
       "max        2.557219     67.050225     65.710213      9.889867      9.975423   \n",
       "\n",
       "                  z           cut         color       clarity          city  \\\n",
       "count  40455.000000  40455.000000  40455.000000  40455.000000  40455.000000   \n",
       "mean       3.537448      3.904783      4.400766      4.049388      1.036433   \n",
       "std        0.695006      1.117876      1.701260      1.648181      0.019357   \n",
       "min        0.958026      1.000000      1.000000      1.000000      1.000000   \n",
       "25%        2.910000      3.000000      3.000000      3.000000      1.031075   \n",
       "50%        3.520000      4.000000      4.000000      4.000000      1.038843   \n",
       "75%        4.035000      5.000000      6.000000      5.000000      1.048770   \n",
       "max        6.116282      5.000000      7.000000      8.000000      1.069918   \n",
       "\n",
       "                x/y            td            ad            cc  \n",
       "count  40455.000000  40455.000000  40455.000000  40455.000000  \n",
       "mean       0.999368      0.930839      0.006135      2.949350  \n",
       "std        0.008899      0.046798      0.000459      1.817597  \n",
       "min        0.958396      0.759091      0.002847      0.300000  \n",
       "25%        0.992606      0.898876      0.006047      1.640000  \n",
       "50%        0.995736      0.923825      0.006117      2.400000  \n",
       "75%        1.006928      0.955519      0.006190      3.700000  \n",
       "max        1.040400      1.105638      0.028417     18.320000  "
      ]
     },
     "execution_count": 854,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labeling_test(test, ['cut', 'color', 'clarity','city'],dic2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[outlier_columns] = sub_outlyers(test[outlier_columns],3.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['x/y'] = test.apply(lambda x : feature_xy(x['x'], x['y']), axis=1)\n",
    "test['td'] = test.apply(lambda x : feature_xy(x['table'], x['depth']), axis=1)\n",
    "test['ad'] = test.apply(lambda x : feature_ad(x['carat'],x['x'], x['y'], x['z']), axis=1)\n",
    "test['cc'] = test['carat'] * test['clarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = pd.concat([test, pd.get_dummies(test['city'], drop_first=True)], axis = 1).drop('city',axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['x/y','td', 'ad']] = sub_outlyers(test[['x/y','td', 'ad']],3.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def filter_transform2(df,selected_features,target):\\n    data_df = df[selected_features]\\n    to_dummy = [feature for feature in selected_features if len(df[feature].unique()) < 15]\\n    for feature in to_dummy:\\n        dummies_df = pd.get_dummies(data_df[feature], prefix=feature, drop_first=True)\\n        data_df = pd.concat([data_df, dummies_df],axis=1).drop(feature, axis=1)   \\n    data_df = pd.concat([data_df.select_dtypes(exclude=['object']), df[target]], axis=1)\\n    return data_df\""
      ]
     },
     "execution_count": 860,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def filter_transform2(df,selected_features,target):\n",
    "    data_df = df[selected_features]\n",
    "    to_dummy = [feature for feature in selected_features if len(df[feature].unique()) < 15]\n",
    "    for feature in to_dummy:\n",
    "        dummies_df = pd.get_dummies(data_df[feature], prefix=feature, drop_first=True)\n",
    "        data_df = pd.concat([data_df, dummies_df],axis=1).drop(feature, axis=1)   \n",
    "    data_df = pd.concat([data_df.select_dtypes(exclude=['object']), df[target]], axis=1)\n",
    "    return data_df'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test[X.columns]\n",
    "test.to_csv('./data/clean/testdl123fexydatdccso3.7x2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_submit = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_list = best_model.predict(X_submit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13480</th>\n",
       "      <td>13480</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13481</th>\n",
       "      <td>13481</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13482</th>\n",
       "      <td>13482</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13483</th>\n",
       "      <td>13483</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13484</th>\n",
       "      <td>13484</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13485 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  price\n",
       "0          0      0\n",
       "1          1      0\n",
       "2          2      0\n",
       "3          3      0\n",
       "4          4      0\n",
       "...      ...    ...\n",
       "13480  13480      0\n",
       "13481  13481      0\n",
       "13482  13482      0\n",
       "13483  13483      0\n",
       "13484  13484      0\n",
       "\n",
       "[13485 rows x 2 columns]"
      ]
     },
     "execution_count": 864,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit_df['price'] = 0\n",
    "submit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2905.942627\n",
       "1        5506.895508\n",
       "2        9689.235352\n",
       "3        4005.036621\n",
       "4        1605.623413\n",
       "            ...     \n",
       "13480    1672.314697\n",
       "13481    2471.592041\n",
       "13482    3086.186035\n",
       "13483    2118.776123\n",
       "13484     791.683960\n",
       "Name: price, Length: 13485, dtype: float32"
      ]
     },
     "execution_count": 865,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit_df['price'] = price_list\n",
    "submit_df['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 866,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_df.set_index('id')['price'].to_csv('./submissions/xgart2l123gridtdccso3.7x2_510x.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(best_model, open('./models/xgartdl123dlgridtdccso3.7x2_510.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open('./models/xgartdl123dlgridtdso3.7x2_511.sav', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streamlit_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
