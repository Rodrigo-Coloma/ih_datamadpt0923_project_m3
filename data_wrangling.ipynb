{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate, train_test_split\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBRegressor, plot_importance\n",
    "import sqlite3 as lite\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = lite.connect('./data/src/diamonds_train.db/diamonds_train.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = con.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.read_sql('''SELECT *\n",
    "FROM diamonds_transactional dt JOIN diamonds_dimensions ddi \n",
    "\t\tON dt.index_id = ddi.index_id \n",
    "\tJOIN diamonds_properties dp ON dp.index_id = dt.index_id \n",
    "\tJOIN diamonds_cut dc ON dc.cut_id = dp.cut_id \n",
    "\tJOIN diamonds_color dc2 ON dc2.color_id = dp.color_id \n",
    "\tJOIN diamonds_clarity dc3 ON dc3.clarity_id = dp.clarity_id \n",
    "\tJOIN diamonds_city dc4 ON dc4.city_id = dt.city_id;''', con )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_id</th>\n",
       "      <th>price</th>\n",
       "      <th>city_id</th>\n",
       "      <th>carat</th>\n",
       "      <th>index_id</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>...</th>\n",
       "      <th>color_id</th>\n",
       "      <th>clarity_id</th>\n",
       "      <th>cut_id</th>\n",
       "      <th>cut</th>\n",
       "      <th>color_id</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity_id</th>\n",
       "      <th>clarity</th>\n",
       "      <th>city_id</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91...</td>\n",
       "      <td>4268</td>\n",
       "      <td>6c425048aa7badd9d84615bd8620ca1864efd81cfdb69d...</td>\n",
       "      <td>1.21</td>\n",
       "      <td>5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91...</td>\n",
       "      <td>62.4</td>\n",
       "      <td>58.0</td>\n",
       "      <td>6.83</td>\n",
       "      <td>6.79</td>\n",
       "      <td>4.25</td>\n",
       "      <td>...</td>\n",
       "      <td>6da43b944e494e885e69af021f93c6d9331c78aa228084...</td>\n",
       "      <td>f0b2a1d0db08cc64f85d74f1d15c2191e0e49039f4d8f2...</td>\n",
       "      <td>de88c121a82a06352bf1aaceba20578356408a334ba046...</td>\n",
       "      <td>Premium</td>\n",
       "      <td>6da43b944e494e885e69af021f93c6d9331c78aa228084...</td>\n",
       "      <td>J</td>\n",
       "      <td>f0b2a1d0db08cc64f85d74f1d15c2191e0e49039f4d8f2...</td>\n",
       "      <td>VS2</td>\n",
       "      <td>6c425048aa7badd9d84615bd8620ca1864efd81cfdb69d...</td>\n",
       "      <td>Dubai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d...</td>\n",
       "      <td>505</td>\n",
       "      <td>89c7286890f7347ab235234e74d406596a127ae3679042...</td>\n",
       "      <td>0.32</td>\n",
       "      <td>6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d...</td>\n",
       "      <td>63.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>4.35</td>\n",
       "      <td>4.38</td>\n",
       "      <td>2.75</td>\n",
       "      <td>...</td>\n",
       "      <td>44bd7ae60f478fae1061e11a7739f4b94d1daf917982d3...</td>\n",
       "      <td>f0b2a1d0db08cc64f85d74f1d15c2191e0e49039f4d8f2...</td>\n",
       "      <td>388655e25e91872329272fc10128ef5354b3b19a05d7e8...</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>44bd7ae60f478fae1061e11a7739f4b94d1daf917982d3...</td>\n",
       "      <td>H</td>\n",
       "      <td>f0b2a1d0db08cc64f85d74f1d15c2191e0e49039f4d8f2...</td>\n",
       "      <td>VS2</td>\n",
       "      <td>89c7286890f7347ab235234e74d406596a127ae3679042...</td>\n",
       "      <td>Kimberly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f...</td>\n",
       "      <td>2686</td>\n",
       "      <td>2bd25cd960aba8b706e2b67f2bb38b750ee5384b0e9883...</td>\n",
       "      <td>0.71</td>\n",
       "      <td>d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f...</td>\n",
       "      <td>65.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>5.62</td>\n",
       "      <td>5.53</td>\n",
       "      <td>3.65</td>\n",
       "      <td>...</td>\n",
       "      <td>333e0a1e27815d0ceee55c473fe3dc93d56c63e3bee2b3...</td>\n",
       "      <td>ef736c1f91cd1900c3d9cde925b1bf4f013adc0211a9ee...</td>\n",
       "      <td>f7b19afcde965ea4942b878d266f89f8ba9a5a833e60f7...</td>\n",
       "      <td>Fair</td>\n",
       "      <td>333e0a1e27815d0ceee55c473fe3dc93d56c63e3bee2b3...</td>\n",
       "      <td>G</td>\n",
       "      <td>ef736c1f91cd1900c3d9cde925b1bf4f013adc0211a9ee...</td>\n",
       "      <td>VS1</td>\n",
       "      <td>2bd25cd960aba8b706e2b67f2bb38b750ee5384b0e9883...</td>\n",
       "      <td>Las Vegas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4e07408562bedb8b60ce05c1decfe3ad16b72230967de0...</td>\n",
       "      <td>738</td>\n",
       "      <td>89c7286890f7347ab235234e74d406596a127ae3679042...</td>\n",
       "      <td>0.41</td>\n",
       "      <td>4e07408562bedb8b60ce05c1decfe3ad16b72230967de0...</td>\n",
       "      <td>63.8</td>\n",
       "      <td>56.0</td>\n",
       "      <td>4.68</td>\n",
       "      <td>4.72</td>\n",
       "      <td>3.00</td>\n",
       "      <td>...</td>\n",
       "      <td>3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0e...</td>\n",
       "      <td>bd4f4a250da88f87729febc739ae97f439a14f9d38f0e3...</td>\n",
       "      <td>c939327ca16dcf97ca32521d8b834bf1de16573d21deda...</td>\n",
       "      <td>Good</td>\n",
       "      <td>3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0e...</td>\n",
       "      <td>D</td>\n",
       "      <td>bd4f4a250da88f87729febc739ae97f439a14f9d38f0e3...</td>\n",
       "      <td>SI1</td>\n",
       "      <td>89c7286890f7347ab235234e74d406596a127ae3679042...</td>\n",
       "      <td>Kimberly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4b227777d4dd1fc61c6f884f48641d02b4d121d3fd328c...</td>\n",
       "      <td>4882</td>\n",
       "      <td>6c425048aa7badd9d84615bd8620ca1864efd81cfdb69d...</td>\n",
       "      <td>1.02</td>\n",
       "      <td>4b227777d4dd1fc61c6f884f48641d02b4d121d3fd328c...</td>\n",
       "      <td>60.5</td>\n",
       "      <td>59.0</td>\n",
       "      <td>6.55</td>\n",
       "      <td>6.51</td>\n",
       "      <td>3.95</td>\n",
       "      <td>...</td>\n",
       "      <td>333e0a1e27815d0ceee55c473fe3dc93d56c63e3bee2b3...</td>\n",
       "      <td>bd4f4a250da88f87729febc739ae97f439a14f9d38f0e3...</td>\n",
       "      <td>4e3cfaa334cbafb57a399a98fad8d3812ece460018f457...</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>333e0a1e27815d0ceee55c473fe3dc93d56c63e3bee2b3...</td>\n",
       "      <td>G</td>\n",
       "      <td>bd4f4a250da88f87729febc739ae97f439a14f9d38f0e3...</td>\n",
       "      <td>SI1</td>\n",
       "      <td>6c425048aa7badd9d84615bd8620ca1864efd81cfdb69d...</td>\n",
       "      <td>Dubai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40450</th>\n",
       "      <td>f0bc79169405ebeb24e308055156b946ffd819db9b4f75...</td>\n",
       "      <td>10070</td>\n",
       "      <td>ca3aa21a5b70c3e88cc6336682c8c7da928a0c66a5ead4...</td>\n",
       "      <td>1.34</td>\n",
       "      <td>f0bc79169405ebeb24e308055156b946ffd819db9b4f75...</td>\n",
       "      <td>62.7</td>\n",
       "      <td>57.0</td>\n",
       "      <td>7.10</td>\n",
       "      <td>7.04</td>\n",
       "      <td>4.43</td>\n",
       "      <td>...</td>\n",
       "      <td>333e0a1e27815d0ceee55c473fe3dc93d56c63e3bee2b3...</td>\n",
       "      <td>ef736c1f91cd1900c3d9cde925b1bf4f013adc0211a9ee...</td>\n",
       "      <td>4e3cfaa334cbafb57a399a98fad8d3812ece460018f457...</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>333e0a1e27815d0ceee55c473fe3dc93d56c63e3bee2b3...</td>\n",
       "      <td>G</td>\n",
       "      <td>ef736c1f91cd1900c3d9cde925b1bf4f013adc0211a9ee...</td>\n",
       "      <td>VS1</td>\n",
       "      <td>ca3aa21a5b70c3e88cc6336682c8c7da928a0c66a5ead4...</td>\n",
       "      <td>Antwerp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40451</th>\n",
       "      <td>339916a23bf22b052b54cb2a9b36ee8418c1c68b46acad...</td>\n",
       "      <td>12615</td>\n",
       "      <td>e9c722cbefc2f055ae60b4e2cbe73a2d99537eab0c37f3...</td>\n",
       "      <td>2.02</td>\n",
       "      <td>339916a23bf22b052b54cb2a9b36ee8418c1c68b46acad...</td>\n",
       "      <td>57.1</td>\n",
       "      <td>60.0</td>\n",
       "      <td>8.31</td>\n",
       "      <td>8.25</td>\n",
       "      <td>4.73</td>\n",
       "      <td>...</td>\n",
       "      <td>f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d378...</td>\n",
       "      <td>03c358cbd92e83278fd523f58dc6a9b4b198d00728af65...</td>\n",
       "      <td>c939327ca16dcf97ca32521d8b834bf1de16573d21deda...</td>\n",
       "      <td>Good</td>\n",
       "      <td>f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d378...</td>\n",
       "      <td>F</td>\n",
       "      <td>03c358cbd92e83278fd523f58dc6a9b4b198d00728af65...</td>\n",
       "      <td>SI2</td>\n",
       "      <td>e9c722cbefc2f055ae60b4e2cbe73a2d99537eab0c37f3...</td>\n",
       "      <td>Madrid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40452</th>\n",
       "      <td>46957922b99954654c1deb8d854c3f069bf118b2ce9415...</td>\n",
       "      <td>5457</td>\n",
       "      <td>89c7286890f7347ab235234e74d406596a127ae3679042...</td>\n",
       "      <td>1.01</td>\n",
       "      <td>46957922b99954654c1deb8d854c3f069bf118b2ce9415...</td>\n",
       "      <td>62.7</td>\n",
       "      <td>56.0</td>\n",
       "      <td>6.37</td>\n",
       "      <td>6.42</td>\n",
       "      <td>4.01</td>\n",
       "      <td>...</td>\n",
       "      <td>44bd7ae60f478fae1061e11a7739f4b94d1daf917982d3...</td>\n",
       "      <td>bd4f4a250da88f87729febc739ae97f439a14f9d38f0e3...</td>\n",
       "      <td>4e3cfaa334cbafb57a399a98fad8d3812ece460018f457...</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>44bd7ae60f478fae1061e11a7739f4b94d1daf917982d3...</td>\n",
       "      <td>H</td>\n",
       "      <td>bd4f4a250da88f87729febc739ae97f439a14f9d38f0e3...</td>\n",
       "      <td>SI1</td>\n",
       "      <td>89c7286890f7347ab235234e74d406596a127ae3679042...</td>\n",
       "      <td>Kimberly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40453</th>\n",
       "      <td>9d733392d362d5c6f1d9b9659b601c7d4b5a1c1c8df579...</td>\n",
       "      <td>456</td>\n",
       "      <td>89c7286890f7347ab235234e74d406596a127ae3679042...</td>\n",
       "      <td>0.33</td>\n",
       "      <td>9d733392d362d5c6f1d9b9659b601c7d4b5a1c1c8df579...</td>\n",
       "      <td>61.9</td>\n",
       "      <td>54.3</td>\n",
       "      <td>4.45</td>\n",
       "      <td>4.47</td>\n",
       "      <td>2.76</td>\n",
       "      <td>...</td>\n",
       "      <td>6da43b944e494e885e69af021f93c6d9331c78aa228084...</td>\n",
       "      <td>ef736c1f91cd1900c3d9cde925b1bf4f013adc0211a9ee...</td>\n",
       "      <td>4e3cfaa334cbafb57a399a98fad8d3812ece460018f457...</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>6da43b944e494e885e69af021f93c6d9331c78aa228084...</td>\n",
       "      <td>J</td>\n",
       "      <td>ef736c1f91cd1900c3d9cde925b1bf4f013adc0211a9ee...</td>\n",
       "      <td>VS1</td>\n",
       "      <td>89c7286890f7347ab235234e74d406596a127ae3679042...</td>\n",
       "      <td>Kimberly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40454</th>\n",
       "      <td>a02744a70faa594d240b067f21fcc23a8d17cd1098a9fb...</td>\n",
       "      <td>6232</td>\n",
       "      <td>ecc0e7dc084f141b29479058967d0bc07dee25d9690a98...</td>\n",
       "      <td>1.24</td>\n",
       "      <td>a02744a70faa594d240b067f21fcc23a8d17cd1098a9fb...</td>\n",
       "      <td>62.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>6.83</td>\n",
       "      <td>6.88</td>\n",
       "      <td>4.25</td>\n",
       "      <td>...</td>\n",
       "      <td>a83dd0ccbffe39d071cc317ddf6e97f5c6b1c87af91919...</td>\n",
       "      <td>bd4f4a250da88f87729febc739ae97f439a14f9d38f0e3...</td>\n",
       "      <td>4e3cfaa334cbafb57a399a98fad8d3812ece460018f457...</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>a83dd0ccbffe39d071cc317ddf6e97f5c6b1c87af91919...</td>\n",
       "      <td>I</td>\n",
       "      <td>bd4f4a250da88f87729febc739ae97f439a14f9d38f0e3...</td>\n",
       "      <td>SI1</td>\n",
       "      <td>ecc0e7dc084f141b29479058967d0bc07dee25d9690a98...</td>\n",
       "      <td>London</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40455 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                index_id  price  \\\n",
       "0      5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91...   4268   \n",
       "1      6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d...    505   \n",
       "2      d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f...   2686   \n",
       "3      4e07408562bedb8b60ce05c1decfe3ad16b72230967de0...    738   \n",
       "4      4b227777d4dd1fc61c6f884f48641d02b4d121d3fd328c...   4882   \n",
       "...                                                  ...    ...   \n",
       "40450  f0bc79169405ebeb24e308055156b946ffd819db9b4f75...  10070   \n",
       "40451  339916a23bf22b052b54cb2a9b36ee8418c1c68b46acad...  12615   \n",
       "40452  46957922b99954654c1deb8d854c3f069bf118b2ce9415...   5457   \n",
       "40453  9d733392d362d5c6f1d9b9659b601c7d4b5a1c1c8df579...    456   \n",
       "40454  a02744a70faa594d240b067f21fcc23a8d17cd1098a9fb...   6232   \n",
       "\n",
       "                                                 city_id  carat  \\\n",
       "0      6c425048aa7badd9d84615bd8620ca1864efd81cfdb69d...   1.21   \n",
       "1      89c7286890f7347ab235234e74d406596a127ae3679042...   0.32   \n",
       "2      2bd25cd960aba8b706e2b67f2bb38b750ee5384b0e9883...   0.71   \n",
       "3      89c7286890f7347ab235234e74d406596a127ae3679042...   0.41   \n",
       "4      6c425048aa7badd9d84615bd8620ca1864efd81cfdb69d...   1.02   \n",
       "...                                                  ...    ...   \n",
       "40450  ca3aa21a5b70c3e88cc6336682c8c7da928a0c66a5ead4...   1.34   \n",
       "40451  e9c722cbefc2f055ae60b4e2cbe73a2d99537eab0c37f3...   2.02   \n",
       "40452  89c7286890f7347ab235234e74d406596a127ae3679042...   1.01   \n",
       "40453  89c7286890f7347ab235234e74d406596a127ae3679042...   0.33   \n",
       "40454  ecc0e7dc084f141b29479058967d0bc07dee25d9690a98...   1.24   \n",
       "\n",
       "                                                index_id  depth  table     x  \\\n",
       "0      5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91...   62.4   58.0  6.83   \n",
       "1      6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d...   63.0   57.0  4.35   \n",
       "2      d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f...   65.5   55.0  5.62   \n",
       "3      4e07408562bedb8b60ce05c1decfe3ad16b72230967de0...   63.8   56.0  4.68   \n",
       "4      4b227777d4dd1fc61c6f884f48641d02b4d121d3fd328c...   60.5   59.0  6.55   \n",
       "...                                                  ...    ...    ...   ...   \n",
       "40450  f0bc79169405ebeb24e308055156b946ffd819db9b4f75...   62.7   57.0  7.10   \n",
       "40451  339916a23bf22b052b54cb2a9b36ee8418c1c68b46acad...   57.1   60.0  8.31   \n",
       "40452  46957922b99954654c1deb8d854c3f069bf118b2ce9415...   62.7   56.0  6.37   \n",
       "40453  9d733392d362d5c6f1d9b9659b601c7d4b5a1c1c8df579...   61.9   54.3  4.45   \n",
       "40454  a02744a70faa594d240b067f21fcc23a8d17cd1098a9fb...   62.0   58.0  6.83   \n",
       "\n",
       "          y     z  ...                                           color_id  \\\n",
       "0      6.79  4.25  ...  6da43b944e494e885e69af021f93c6d9331c78aa228084...   \n",
       "1      4.38  2.75  ...  44bd7ae60f478fae1061e11a7739f4b94d1daf917982d3...   \n",
       "2      5.53  3.65  ...  333e0a1e27815d0ceee55c473fe3dc93d56c63e3bee2b3...   \n",
       "3      4.72  3.00  ...  3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0e...   \n",
       "4      6.51  3.95  ...  333e0a1e27815d0ceee55c473fe3dc93d56c63e3bee2b3...   \n",
       "...     ...   ...  ...                                                ...   \n",
       "40450  7.04  4.43  ...  333e0a1e27815d0ceee55c473fe3dc93d56c63e3bee2b3...   \n",
       "40451  8.25  4.73  ...  f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d378...   \n",
       "40452  6.42  4.01  ...  44bd7ae60f478fae1061e11a7739f4b94d1daf917982d3...   \n",
       "40453  4.47  2.76  ...  6da43b944e494e885e69af021f93c6d9331c78aa228084...   \n",
       "40454  6.88  4.25  ...  a83dd0ccbffe39d071cc317ddf6e97f5c6b1c87af91919...   \n",
       "\n",
       "                                              clarity_id  \\\n",
       "0      f0b2a1d0db08cc64f85d74f1d15c2191e0e49039f4d8f2...   \n",
       "1      f0b2a1d0db08cc64f85d74f1d15c2191e0e49039f4d8f2...   \n",
       "2      ef736c1f91cd1900c3d9cde925b1bf4f013adc0211a9ee...   \n",
       "3      bd4f4a250da88f87729febc739ae97f439a14f9d38f0e3...   \n",
       "4      bd4f4a250da88f87729febc739ae97f439a14f9d38f0e3...   \n",
       "...                                                  ...   \n",
       "40450  ef736c1f91cd1900c3d9cde925b1bf4f013adc0211a9ee...   \n",
       "40451  03c358cbd92e83278fd523f58dc6a9b4b198d00728af65...   \n",
       "40452  bd4f4a250da88f87729febc739ae97f439a14f9d38f0e3...   \n",
       "40453  ef736c1f91cd1900c3d9cde925b1bf4f013adc0211a9ee...   \n",
       "40454  bd4f4a250da88f87729febc739ae97f439a14f9d38f0e3...   \n",
       "\n",
       "                                                  cut_id        cut  \\\n",
       "0      de88c121a82a06352bf1aaceba20578356408a334ba046...    Premium   \n",
       "1      388655e25e91872329272fc10128ef5354b3b19a05d7e8...  Very Good   \n",
       "2      f7b19afcde965ea4942b878d266f89f8ba9a5a833e60f7...       Fair   \n",
       "3      c939327ca16dcf97ca32521d8b834bf1de16573d21deda...       Good   \n",
       "4      4e3cfaa334cbafb57a399a98fad8d3812ece460018f457...      Ideal   \n",
       "...                                                  ...        ...   \n",
       "40450  4e3cfaa334cbafb57a399a98fad8d3812ece460018f457...      Ideal   \n",
       "40451  c939327ca16dcf97ca32521d8b834bf1de16573d21deda...       Good   \n",
       "40452  4e3cfaa334cbafb57a399a98fad8d3812ece460018f457...      Ideal   \n",
       "40453  4e3cfaa334cbafb57a399a98fad8d3812ece460018f457...      Ideal   \n",
       "40454  4e3cfaa334cbafb57a399a98fad8d3812ece460018f457...      Ideal   \n",
       "\n",
       "                                                color_id color  \\\n",
       "0      6da43b944e494e885e69af021f93c6d9331c78aa228084...     J   \n",
       "1      44bd7ae60f478fae1061e11a7739f4b94d1daf917982d3...     H   \n",
       "2      333e0a1e27815d0ceee55c473fe3dc93d56c63e3bee2b3...     G   \n",
       "3      3f39d5c348e5b79d06e842c114e6cc571583bbf44e4b0e...     D   \n",
       "4      333e0a1e27815d0ceee55c473fe3dc93d56c63e3bee2b3...     G   \n",
       "...                                                  ...   ...   \n",
       "40450  333e0a1e27815d0ceee55c473fe3dc93d56c63e3bee2b3...     G   \n",
       "40451  f67ab10ad4e4c53121b6a5fe4da9c10ddee905b978d378...     F   \n",
       "40452  44bd7ae60f478fae1061e11a7739f4b94d1daf917982d3...     H   \n",
       "40453  6da43b944e494e885e69af021f93c6d9331c78aa228084...     J   \n",
       "40454  a83dd0ccbffe39d071cc317ddf6e97f5c6b1c87af91919...     I   \n",
       "\n",
       "                                              clarity_id clarity  \\\n",
       "0      f0b2a1d0db08cc64f85d74f1d15c2191e0e49039f4d8f2...     VS2   \n",
       "1      f0b2a1d0db08cc64f85d74f1d15c2191e0e49039f4d8f2...     VS2   \n",
       "2      ef736c1f91cd1900c3d9cde925b1bf4f013adc0211a9ee...     VS1   \n",
       "3      bd4f4a250da88f87729febc739ae97f439a14f9d38f0e3...     SI1   \n",
       "4      bd4f4a250da88f87729febc739ae97f439a14f9d38f0e3...     SI1   \n",
       "...                                                  ...     ...   \n",
       "40450  ef736c1f91cd1900c3d9cde925b1bf4f013adc0211a9ee...     VS1   \n",
       "40451  03c358cbd92e83278fd523f58dc6a9b4b198d00728af65...     SI2   \n",
       "40452  bd4f4a250da88f87729febc739ae97f439a14f9d38f0e3...     SI1   \n",
       "40453  ef736c1f91cd1900c3d9cde925b1bf4f013adc0211a9ee...     VS1   \n",
       "40454  bd4f4a250da88f87729febc739ae97f439a14f9d38f0e3...     SI1   \n",
       "\n",
       "                                                 city_id       city  \n",
       "0      6c425048aa7badd9d84615bd8620ca1864efd81cfdb69d...      Dubai  \n",
       "1      89c7286890f7347ab235234e74d406596a127ae3679042...   Kimberly  \n",
       "2      2bd25cd960aba8b706e2b67f2bb38b750ee5384b0e9883...  Las Vegas  \n",
       "3      89c7286890f7347ab235234e74d406596a127ae3679042...   Kimberly  \n",
       "4      6c425048aa7badd9d84615bd8620ca1864efd81cfdb69d...      Dubai  \n",
       "...                                                  ...        ...  \n",
       "40450  ca3aa21a5b70c3e88cc6336682c8c7da928a0c66a5ead4...    Antwerp  \n",
       "40451  e9c722cbefc2f055ae60b4e2cbe73a2d99537eab0c37f3...     Madrid  \n",
       "40452  89c7286890f7347ab235234e74d406596a127ae3679042...   Kimberly  \n",
       "40453  89c7286890f7347ab235234e74d406596a127ae3679042...   Kimberly  \n",
       "40454  ecc0e7dc084f141b29479058967d0bc07dee25d9690a98...     London  \n",
       "\n",
       "[40455 rows x 22 columns]"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>carat</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3928.444469</td>\n",
       "      <td>0.797706</td>\n",
       "      <td>61.752841</td>\n",
       "      <td>57.446133</td>\n",
       "      <td>5.729392</td>\n",
       "      <td>5.732819</td>\n",
       "      <td>3.537154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3992.416147</td>\n",
       "      <td>0.475544</td>\n",
       "      <td>1.431725</td>\n",
       "      <td>2.233535</td>\n",
       "      <td>1.124453</td>\n",
       "      <td>1.146650</td>\n",
       "      <td>0.697062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>326.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>945.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>4.710000</td>\n",
       "      <td>4.720000</td>\n",
       "      <td>2.910000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2397.000000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>61.800000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>5.690000</td>\n",
       "      <td>5.710000</td>\n",
       "      <td>3.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5331.000000</td>\n",
       "      <td>1.040000</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>6.540000</td>\n",
       "      <td>6.540000</td>\n",
       "      <td>4.035000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>18823.000000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>10.230000</td>\n",
       "      <td>58.900000</td>\n",
       "      <td>8.060000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              price         carat         depth         table             x  \\\n",
       "count  40455.000000  40455.000000  40455.000000  40455.000000  40455.000000   \n",
       "mean    3928.444469      0.797706     61.752841     57.446133      5.729392   \n",
       "std     3992.416147      0.475544      1.431725      2.233535      1.124453   \n",
       "min      326.000000      0.200000     43.000000     43.000000      0.000000   \n",
       "25%      945.000000      0.400000     61.000000     56.000000      4.710000   \n",
       "50%     2397.000000      0.700000     61.800000     57.000000      5.690000   \n",
       "75%     5331.000000      1.040000     62.500000     59.000000      6.540000   \n",
       "max    18823.000000      4.500000     79.000000     95.000000     10.230000   \n",
       "\n",
       "                  y             z  \n",
       "count  40455.000000  40455.000000  \n",
       "mean       5.732819      3.537154  \n",
       "std        1.146650      0.697062  \n",
       "min        0.000000      0.000000  \n",
       "25%        4.720000      2.910000  \n",
       "50%        5.710000      3.520000  \n",
       "75%        6.540000      4.035000  \n",
       "max       58.900000      8.060000  "
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    13485.000000\n",
       "mean         4.420912\n",
       "std          1.700614\n",
       "min          1.000000\n",
       "25%          3.000000\n",
       "50%          4.000000\n",
       "75%          6.000000\n",
       "max          7.000000\n",
       "Name: color, dtype: float64"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['color'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index_id', 'price', 'city_id', 'carat', 'index_id', 'depth', 'table',\n",
       "       'x', 'y', 'z', 'index_id', 'cut_id', 'color_id', 'clarity_id', 'cut_id',\n",
       "       'cut', 'color_id', 'color', 'clarity_id', 'clarity', 'city_id', 'city'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = raw_df[ ['price', 'carat',  'depth', 'table','x', 'y', 'z','cut', 'color', 'clarity', 'city']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>carat</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4268</td>\n",
       "      <td>1.21</td>\n",
       "      <td>62.4</td>\n",
       "      <td>58.0</td>\n",
       "      <td>6.83</td>\n",
       "      <td>6.79</td>\n",
       "      <td>4.25</td>\n",
       "      <td>Premium</td>\n",
       "      <td>J</td>\n",
       "      <td>VS2</td>\n",
       "      <td>Dubai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>505</td>\n",
       "      <td>0.32</td>\n",
       "      <td>63.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>4.35</td>\n",
       "      <td>4.38</td>\n",
       "      <td>2.75</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>H</td>\n",
       "      <td>VS2</td>\n",
       "      <td>Kimberly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2686</td>\n",
       "      <td>0.71</td>\n",
       "      <td>65.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>5.62</td>\n",
       "      <td>5.53</td>\n",
       "      <td>3.65</td>\n",
       "      <td>Fair</td>\n",
       "      <td>G</td>\n",
       "      <td>VS1</td>\n",
       "      <td>Las Vegas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>738</td>\n",
       "      <td>0.41</td>\n",
       "      <td>63.8</td>\n",
       "      <td>56.0</td>\n",
       "      <td>4.68</td>\n",
       "      <td>4.72</td>\n",
       "      <td>3.00</td>\n",
       "      <td>Good</td>\n",
       "      <td>D</td>\n",
       "      <td>SI1</td>\n",
       "      <td>Kimberly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4882</td>\n",
       "      <td>1.02</td>\n",
       "      <td>60.5</td>\n",
       "      <td>59.0</td>\n",
       "      <td>6.55</td>\n",
       "      <td>6.51</td>\n",
       "      <td>3.95</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>G</td>\n",
       "      <td>SI1</td>\n",
       "      <td>Dubai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40450</th>\n",
       "      <td>10070</td>\n",
       "      <td>1.34</td>\n",
       "      <td>62.7</td>\n",
       "      <td>57.0</td>\n",
       "      <td>7.10</td>\n",
       "      <td>7.04</td>\n",
       "      <td>4.43</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>G</td>\n",
       "      <td>VS1</td>\n",
       "      <td>Antwerp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40451</th>\n",
       "      <td>12615</td>\n",
       "      <td>2.02</td>\n",
       "      <td>57.1</td>\n",
       "      <td>60.0</td>\n",
       "      <td>8.31</td>\n",
       "      <td>8.25</td>\n",
       "      <td>4.73</td>\n",
       "      <td>Good</td>\n",
       "      <td>F</td>\n",
       "      <td>SI2</td>\n",
       "      <td>Madrid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40452</th>\n",
       "      <td>5457</td>\n",
       "      <td>1.01</td>\n",
       "      <td>62.7</td>\n",
       "      <td>56.0</td>\n",
       "      <td>6.37</td>\n",
       "      <td>6.42</td>\n",
       "      <td>4.01</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>H</td>\n",
       "      <td>SI1</td>\n",
       "      <td>Kimberly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40453</th>\n",
       "      <td>456</td>\n",
       "      <td>0.33</td>\n",
       "      <td>61.9</td>\n",
       "      <td>54.3</td>\n",
       "      <td>4.45</td>\n",
       "      <td>4.47</td>\n",
       "      <td>2.76</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>J</td>\n",
       "      <td>VS1</td>\n",
       "      <td>Kimberly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40454</th>\n",
       "      <td>6232</td>\n",
       "      <td>1.24</td>\n",
       "      <td>62.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>6.83</td>\n",
       "      <td>6.88</td>\n",
       "      <td>4.25</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>I</td>\n",
       "      <td>SI1</td>\n",
       "      <td>London</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40455 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       price  carat  depth  table     x     y     z        cut color clarity  \\\n",
       "0       4268   1.21   62.4   58.0  6.83  6.79  4.25    Premium     J     VS2   \n",
       "1        505   0.32   63.0   57.0  4.35  4.38  2.75  Very Good     H     VS2   \n",
       "2       2686   0.71   65.5   55.0  5.62  5.53  3.65       Fair     G     VS1   \n",
       "3        738   0.41   63.8   56.0  4.68  4.72  3.00       Good     D     SI1   \n",
       "4       4882   1.02   60.5   59.0  6.55  6.51  3.95      Ideal     G     SI1   \n",
       "...      ...    ...    ...    ...   ...   ...   ...        ...   ...     ...   \n",
       "40450  10070   1.34   62.7   57.0  7.10  7.04  4.43      Ideal     G     VS1   \n",
       "40451  12615   2.02   57.1   60.0  8.31  8.25  4.73       Good     F     SI2   \n",
       "40452   5457   1.01   62.7   56.0  6.37  6.42  4.01      Ideal     H     SI1   \n",
       "40453    456   0.33   61.9   54.3  4.45  4.47  2.76      Ideal     J     VS1   \n",
       "40454   6232   1.24   62.0   58.0  6.83  6.88  4.25      Ideal     I     SI1   \n",
       "\n",
       "            city  \n",
       "0          Dubai  \n",
       "1       Kimberly  \n",
       "2      Las Vegas  \n",
       "3       Kimberly  \n",
       "4          Dubai  \n",
       "...          ...  \n",
       "40450    Antwerp  \n",
       "40451     Madrid  \n",
       "40452   Kimberly  \n",
       "40453   Kimberly  \n",
       "40454     London  \n",
       "\n",
       "[40455 rows x 11 columns]"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.to_csv('./data/clean/clean.csv')\n",
    "clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_labeling(df, cols):\n",
    "   dic = {}\n",
    "   for col in cols:\n",
    "      group_by = df[[col,'price']].groupby(col).median().sort_values('price')\n",
    "      for key in group_by.index:\n",
    "         dic[key] = sorted(list(group_by['price'])).index(group_by.loc[key,'price']) + 1\n",
    "      print(dic)\n",
    "      for key,value in dic.items():\n",
    "         df.loc[df[col] == key,col] = value\n",
    "      df[col] = df[col].astype(float)\n",
    "   return dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_labeling_test(df, cols, dic):\n",
    "   for col in cols:\n",
    "      for key,value in dic.items():\n",
    "         df.loc[df[col] == key,col] = value\n",
    "      df[col] = df[col].astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dic = data_labeling(clean_df, ['cut', 'color', 'clarity','city'])\n",
    "#dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic2 = {'Ideal': 5,\n",
    " 'Very Good': 3,\n",
    " 'Good': 2,\n",
    " 'Premium': 4,\n",
    " 'Fair': 1,\n",
    " 'E': 6,\n",
    " 'D': 7,\n",
    " 'G': 4,\n",
    " 'F': 5,\n",
    " 'H': 3,\n",
    " 'I': 2,\n",
    " 'J': 1,\n",
    " 'IF': 8,\n",
    " 'VVS1': 7,\n",
    " 'VVS2': 6,\n",
    " 'VS1': 5,\n",
    " 'VS2': 4,\n",
    " 'SI1': 3,\n",
    " 'I1': 1,\n",
    " 'SI2': 2,\n",
    " 'Paris': 1.0,\n",
    " 'Luxembourg': 1.0017263703064307,\n",
    " 'Tel Aviv': 1.0051791109192922,\n",
    " 'Zurich': 1.0310746655157532,\n",
    " 'London': 1.0332326283987916,\n",
    " 'Antwerp': 1.0358221838584376,\n",
    " 'Madrid': 1.0360379801467414,\n",
    " 'Las Vegas': 1.0388433318946915,\n",
    " 'Surat': 1.0401381096245144,\n",
    " 'New York City': 1.048769961156668,\n",
    " 'Amsterdam': 1.0548122572291756,\n",
    " 'Kimberly': 1.0561070349589987,\n",
    " 'Dubai': 1.0699179974104445}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31332/1822649115.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = df[col].astype(float)\n",
      "/tmp/ipykernel_31332/1822649115.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = df[col].astype(float)\n",
      "/tmp/ipykernel_31332/1822649115.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = df[col].astype(float)\n",
      "/tmp/ipykernel_31332/1822649115.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = df[col].astype(float)\n"
     ]
    }
   ],
   "source": [
    "#data_labeling(clean_df, ['cut', 'color', 'clarity','city'])\n",
    "data_labeling_test(clean_df, ['cut', 'color', 'clarity','city'],dic2)\n",
    "#clean_df = pd.concat([clean_df, pd.get_dummies(clean_df[['cut', 'color', 'clarity','city']], drop_first=True)], axis = 1).drop(['cut', 'color', 'clarity','city'],axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>carat</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3928.444469</td>\n",
       "      <td>0.797706</td>\n",
       "      <td>61.752841</td>\n",
       "      <td>57.446133</td>\n",
       "      <td>5.729392</td>\n",
       "      <td>5.732819</td>\n",
       "      <td>3.537154</td>\n",
       "      <td>3.904783</td>\n",
       "      <td>4.400766</td>\n",
       "      <td>4.049388</td>\n",
       "      <td>1.036433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3992.416147</td>\n",
       "      <td>0.475544</td>\n",
       "      <td>1.431725</td>\n",
       "      <td>2.233535</td>\n",
       "      <td>1.124453</td>\n",
       "      <td>1.146650</td>\n",
       "      <td>0.697062</td>\n",
       "      <td>1.117876</td>\n",
       "      <td>1.701260</td>\n",
       "      <td>1.648181</td>\n",
       "      <td>0.019357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>326.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>945.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>4.710000</td>\n",
       "      <td>4.720000</td>\n",
       "      <td>2.910000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.031075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2397.000000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>61.800000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>5.690000</td>\n",
       "      <td>5.710000</td>\n",
       "      <td>3.520000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.038843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5331.000000</td>\n",
       "      <td>1.040000</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>6.540000</td>\n",
       "      <td>6.540000</td>\n",
       "      <td>4.035000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.048770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>18823.000000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>10.230000</td>\n",
       "      <td>58.900000</td>\n",
       "      <td>8.060000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.069918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              price         carat         depth         table             x  \\\n",
       "count  40455.000000  40455.000000  40455.000000  40455.000000  40455.000000   \n",
       "mean    3928.444469      0.797706     61.752841     57.446133      5.729392   \n",
       "std     3992.416147      0.475544      1.431725      2.233535      1.124453   \n",
       "min      326.000000      0.200000     43.000000     43.000000      0.000000   \n",
       "25%      945.000000      0.400000     61.000000     56.000000      4.710000   \n",
       "50%     2397.000000      0.700000     61.800000     57.000000      5.690000   \n",
       "75%     5331.000000      1.040000     62.500000     59.000000      6.540000   \n",
       "max    18823.000000      4.500000     79.000000     95.000000     10.230000   \n",
       "\n",
       "                  y             z           cut         color       clarity  \\\n",
       "count  40455.000000  40455.000000  40455.000000  40455.000000  40455.000000   \n",
       "mean       5.732819      3.537154      3.904783      4.400766      4.049388   \n",
       "std        1.146650      0.697062      1.117876      1.701260      1.648181   \n",
       "min        0.000000      0.000000      1.000000      1.000000      1.000000   \n",
       "25%        4.720000      2.910000      3.000000      3.000000      3.000000   \n",
       "50%        5.710000      3.520000      4.000000      4.000000      4.000000   \n",
       "75%        6.540000      4.035000      5.000000      6.000000      5.000000   \n",
       "max       58.900000      8.060000      5.000000      7.000000      8.000000   \n",
       "\n",
       "               city  \n",
       "count  40455.000000  \n",
       "mean       1.036433  \n",
       "std        0.019357  \n",
       "min        1.000000  \n",
       "25%        1.031075  \n",
       "50%        1.038843  \n",
       "75%        1.048770  \n",
       "max        1.069918  "
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_outlyers(df,deviations):\n",
    "    df2 = df.copy()\n",
    "    for col in df.columns:\n",
    "        min = df[col].mean() - deviations * df[col].std()\n",
    "        max = df[col].mean() + deviations * df[col].std()\n",
    "        df2.loc[df[col] < min,col] = min    \n",
    "        df2.loc[df[col] > max,col] = max\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31332/1832486247.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_df[outlier_columns] = sub_outlyers(clean_df[outlier_columns],3.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>carat</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>city</th>\n",
       "      <th>x/y</th>\n",
       "      <th>td</th>\n",
       "      <th>ad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3928.444469</td>\n",
       "      <td>0.796861</td>\n",
       "      <td>61.749873</td>\n",
       "      <td>57.439239</td>\n",
       "      <td>5.729562</td>\n",
       "      <td>5.731737</td>\n",
       "      <td>3.537474</td>\n",
       "      <td>3.904783</td>\n",
       "      <td>4.400766</td>\n",
       "      <td>4.049388</td>\n",
       "      <td>1.036433</td>\n",
       "      <td>0.999372</td>\n",
       "      <td>0.930886</td>\n",
       "      <td>0.006141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3992.416147</td>\n",
       "      <td>0.471897</td>\n",
       "      <td>1.381551</td>\n",
       "      <td>2.195948</td>\n",
       "      <td>1.123308</td>\n",
       "      <td>1.114987</td>\n",
       "      <td>0.694681</td>\n",
       "      <td>1.117876</td>\n",
       "      <td>1.701260</td>\n",
       "      <td>1.648181</td>\n",
       "      <td>0.019357</td>\n",
       "      <td>0.008915</td>\n",
       "      <td>0.047052</td>\n",
       "      <td>0.000793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>326.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>56.855746</td>\n",
       "      <td>49.711387</td>\n",
       "      <td>1.796384</td>\n",
       "      <td>1.827576</td>\n",
       "      <td>1.103360</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.954618</td>\n",
       "      <td>0.759091</td>\n",
       "      <td>0.002847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>945.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>4.710000</td>\n",
       "      <td>4.720000</td>\n",
       "      <td>2.910000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.031075</td>\n",
       "      <td>0.992606</td>\n",
       "      <td>0.898876</td>\n",
       "      <td>0.006048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2397.000000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>61.800000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>5.690000</td>\n",
       "      <td>5.710000</td>\n",
       "      <td>3.520000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.038843</td>\n",
       "      <td>0.995736</td>\n",
       "      <td>0.923825</td>\n",
       "      <td>0.006117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5331.000000</td>\n",
       "      <td>1.040000</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>6.540000</td>\n",
       "      <td>6.540000</td>\n",
       "      <td>4.035000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.048770</td>\n",
       "      <td>1.006928</td>\n",
       "      <td>0.955519</td>\n",
       "      <td>0.006190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>18823.000000</td>\n",
       "      <td>2.454322</td>\n",
       "      <td>66.646117</td>\n",
       "      <td>65.173617</td>\n",
       "      <td>9.662704</td>\n",
       "      <td>9.635886</td>\n",
       "      <td>5.971430</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.069918</td>\n",
       "      <td>1.044181</td>\n",
       "      <td>1.120570</td>\n",
       "      <td>0.057262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              price         carat         depth         table             x  \\\n",
       "count  40455.000000  40455.000000  40455.000000  40455.000000  40455.000000   \n",
       "mean    3928.444469      0.796861     61.749873     57.439239      5.729562   \n",
       "std     3992.416147      0.471897      1.381551      2.195948      1.123308   \n",
       "min      326.000000      0.200000     56.855746     49.711387      1.796384   \n",
       "25%      945.000000      0.400000     61.000000     56.000000      4.710000   \n",
       "50%     2397.000000      0.700000     61.800000     57.000000      5.690000   \n",
       "75%     5331.000000      1.040000     62.500000     59.000000      6.540000   \n",
       "max    18823.000000      2.454322     66.646117     65.173617      9.662704   \n",
       "\n",
       "                  y             z           cut         color       clarity  \\\n",
       "count  40455.000000  40455.000000  40455.000000  40455.000000  40455.000000   \n",
       "mean       5.731737      3.537474      3.904783      4.400766      4.049388   \n",
       "std        1.114987      0.694681      1.117876      1.701260      1.648181   \n",
       "min        1.827576      1.103360      1.000000      1.000000      1.000000   \n",
       "25%        4.720000      2.910000      3.000000      3.000000      3.000000   \n",
       "50%        5.710000      3.520000      4.000000      4.000000      4.000000   \n",
       "75%        6.540000      4.035000      5.000000      6.000000      5.000000   \n",
       "max        9.635886      5.971430      5.000000      7.000000      8.000000   \n",
       "\n",
       "               city           x/y            td            ad  \n",
       "count  40455.000000  40455.000000  40455.000000  40455.000000  \n",
       "mean       1.036433      0.999372      0.930886      0.006141  \n",
       "std        0.019357      0.008915      0.047052      0.000793  \n",
       "min        1.000000      0.954618      0.759091      0.002847  \n",
       "25%        1.031075      0.992606      0.898876      0.006048  \n",
       "50%        1.038843      0.995736      0.923825      0.006117  \n",
       "75%        1.048770      1.006928      0.955519      0.006190  \n",
       "max        1.069918      1.044181      1.120570      0.057262  "
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outlier_columns = ['carat','depth','table', 'x', 'y', 'z']\n",
    "clean_df[outlier_columns] = sub_outlyers(clean_df[outlier_columns],3.5)\n",
    "clean_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_xy(x,y):\n",
    "    try:\n",
    "        return x/y\n",
    "    except:\n",
    "        return 1\n",
    "    \n",
    "def feature_ad(carat,x,y,z,):\n",
    "    try:\n",
    "        return carat/(x*y*z)\n",
    "    except:\n",
    "        return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31332/2433330840.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_df['x/y'] = clean_df.apply(lambda x : feature_xy(x['x'], x['y']), axis=1)\n",
      "/tmp/ipykernel_31332/2433330840.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_df['td'] = clean_df.apply(lambda x : feature_xy(x['table'], x['depth']), axis=1)\n",
      "/tmp/ipykernel_31332/2433330840.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_df['ad'] = clean_df.apply(lambda x : feature_ad(x['carat'],x['x'], x['y'], x['z']), axis=1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>carat</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>city</th>\n",
       "      <th>x/y</th>\n",
       "      <th>td</th>\n",
       "      <th>ad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3928.444469</td>\n",
       "      <td>0.796861</td>\n",
       "      <td>61.749873</td>\n",
       "      <td>57.439239</td>\n",
       "      <td>5.729562</td>\n",
       "      <td>5.731737</td>\n",
       "      <td>3.537474</td>\n",
       "      <td>3.904783</td>\n",
       "      <td>4.400766</td>\n",
       "      <td>4.049388</td>\n",
       "      <td>1.036433</td>\n",
       "      <td>0.999393</td>\n",
       "      <td>0.930911</td>\n",
       "      <td>0.006165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3992.416147</td>\n",
       "      <td>0.471897</td>\n",
       "      <td>1.381551</td>\n",
       "      <td>2.195948</td>\n",
       "      <td>1.123308</td>\n",
       "      <td>1.114987</td>\n",
       "      <td>0.694681</td>\n",
       "      <td>1.117876</td>\n",
       "      <td>1.701260</td>\n",
       "      <td>1.648181</td>\n",
       "      <td>0.019357</td>\n",
       "      <td>0.011001</td>\n",
       "      <td>0.047004</td>\n",
       "      <td>0.003709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>326.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>56.855746</td>\n",
       "      <td>49.711387</td>\n",
       "      <td>1.796384</td>\n",
       "      <td>1.827576</td>\n",
       "      <td>1.103360</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.271357</td>\n",
       "      <td>0.759091</td>\n",
       "      <td>0.002847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>945.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>4.710000</td>\n",
       "      <td>4.720000</td>\n",
       "      <td>2.910000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.031075</td>\n",
       "      <td>0.992606</td>\n",
       "      <td>0.898876</td>\n",
       "      <td>0.006047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2397.000000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>61.800000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>5.690000</td>\n",
       "      <td>5.710000</td>\n",
       "      <td>3.520000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.038843</td>\n",
       "      <td>0.995736</td>\n",
       "      <td>0.923825</td>\n",
       "      <td>0.006116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5331.000000</td>\n",
       "      <td>1.040000</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>6.540000</td>\n",
       "      <td>6.540000</td>\n",
       "      <td>4.035000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.048770</td>\n",
       "      <td>1.006928</td>\n",
       "      <td>0.955519</td>\n",
       "      <td>0.006189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>18823.000000</td>\n",
       "      <td>2.454322</td>\n",
       "      <td>66.646117</td>\n",
       "      <td>65.173617</td>\n",
       "      <td>9.662704</td>\n",
       "      <td>9.635886</td>\n",
       "      <td>5.971430</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.069918</td>\n",
       "      <td>1.615572</td>\n",
       "      <td>1.146298</td>\n",
       "      <td>0.621142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              price         carat         depth         table             x  \\\n",
       "count  40455.000000  40455.000000  40455.000000  40455.000000  40455.000000   \n",
       "mean    3928.444469      0.796861     61.749873     57.439239      5.729562   \n",
       "std     3992.416147      0.471897      1.381551      2.195948      1.123308   \n",
       "min      326.000000      0.200000     56.855746     49.711387      1.796384   \n",
       "25%      945.000000      0.400000     61.000000     56.000000      4.710000   \n",
       "50%     2397.000000      0.700000     61.800000     57.000000      5.690000   \n",
       "75%     5331.000000      1.040000     62.500000     59.000000      6.540000   \n",
       "max    18823.000000      2.454322     66.646117     65.173617      9.662704   \n",
       "\n",
       "                  y             z           cut         color       clarity  \\\n",
       "count  40455.000000  40455.000000  40455.000000  40455.000000  40455.000000   \n",
       "mean       5.731737      3.537474      3.904783      4.400766      4.049388   \n",
       "std        1.114987      0.694681      1.117876      1.701260      1.648181   \n",
       "min        1.827576      1.103360      1.000000      1.000000      1.000000   \n",
       "25%        4.720000      2.910000      3.000000      3.000000      3.000000   \n",
       "50%        5.710000      3.520000      4.000000      4.000000      4.000000   \n",
       "75%        6.540000      4.035000      5.000000      6.000000      5.000000   \n",
       "max        9.635886      5.971430      5.000000      7.000000      8.000000   \n",
       "\n",
       "               city           x/y            td            ad  \n",
       "count  40455.000000  40455.000000  40455.000000  40455.000000  \n",
       "mean       1.036433      0.999393      0.930911      0.006165  \n",
       "std        0.019357      0.011001      0.047004      0.003709  \n",
       "min        1.000000      0.271357      0.759091      0.002847  \n",
       "25%        1.031075      0.992606      0.898876      0.006047  \n",
       "50%        1.038843      0.995736      0.923825      0.006116  \n",
       "75%        1.048770      1.006928      0.955519      0.006189  \n",
       "max        1.069918      1.615572      1.146298      0.621142  "
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df['x/y'] = clean_df.apply(lambda x : feature_xy(x['x'], x['y']), axis=1)\n",
    "clean_df['td'] = clean_df.apply(lambda x : feature_xy(x['table'], x['depth']), axis=1)\n",
    "clean_df['ad'] = clean_df.apply(lambda x : feature_ad(x['carat'],x['x'], x['y'], x['z']), axis=1)\n",
    "clean_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31332/1921149856.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_df[['x/y', 'td', 'ad']] = sub_outlyers(clean_df[['x/y', 'td', 'ad']],3.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>carat</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>city</th>\n",
       "      <th>x/y</th>\n",
       "      <th>td</th>\n",
       "      <th>ad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3928.444469</td>\n",
       "      <td>0.796861</td>\n",
       "      <td>61.749873</td>\n",
       "      <td>57.439239</td>\n",
       "      <td>5.729562</td>\n",
       "      <td>5.731737</td>\n",
       "      <td>3.537474</td>\n",
       "      <td>3.904783</td>\n",
       "      <td>4.400766</td>\n",
       "      <td>4.049388</td>\n",
       "      <td>1.036433</td>\n",
       "      <td>0.999362</td>\n",
       "      <td>0.930796</td>\n",
       "      <td>0.006131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3992.416147</td>\n",
       "      <td>0.471897</td>\n",
       "      <td>1.381551</td>\n",
       "      <td>2.195948</td>\n",
       "      <td>1.123308</td>\n",
       "      <td>1.114987</td>\n",
       "      <td>0.694681</td>\n",
       "      <td>1.117876</td>\n",
       "      <td>1.701260</td>\n",
       "      <td>1.648181</td>\n",
       "      <td>0.019357</td>\n",
       "      <td>0.008881</td>\n",
       "      <td>0.046552</td>\n",
       "      <td>0.000308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>326.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>56.855746</td>\n",
       "      <td>49.711387</td>\n",
       "      <td>1.796384</td>\n",
       "      <td>1.827576</td>\n",
       "      <td>1.103360</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.960889</td>\n",
       "      <td>0.766397</td>\n",
       "      <td>0.002847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>945.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>4.710000</td>\n",
       "      <td>4.720000</td>\n",
       "      <td>2.910000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.031075</td>\n",
       "      <td>0.992606</td>\n",
       "      <td>0.898876</td>\n",
       "      <td>0.006047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2397.000000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>61.800000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>5.690000</td>\n",
       "      <td>5.710000</td>\n",
       "      <td>3.520000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.038843</td>\n",
       "      <td>0.995736</td>\n",
       "      <td>0.923825</td>\n",
       "      <td>0.006116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5331.000000</td>\n",
       "      <td>1.040000</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>6.540000</td>\n",
       "      <td>6.540000</td>\n",
       "      <td>4.035000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.048770</td>\n",
       "      <td>1.006928</td>\n",
       "      <td>0.955519</td>\n",
       "      <td>0.006189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>18823.000000</td>\n",
       "      <td>2.454322</td>\n",
       "      <td>66.646117</td>\n",
       "      <td>65.173617</td>\n",
       "      <td>9.662704</td>\n",
       "      <td>9.635886</td>\n",
       "      <td>5.971430</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.069918</td>\n",
       "      <td>1.037898</td>\n",
       "      <td>1.095425</td>\n",
       "      <td>0.019147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              price         carat         depth         table             x  \\\n",
       "count  40455.000000  40455.000000  40455.000000  40455.000000  40455.000000   \n",
       "mean    3928.444469      0.796861     61.749873     57.439239      5.729562   \n",
       "std     3992.416147      0.471897      1.381551      2.195948      1.123308   \n",
       "min      326.000000      0.200000     56.855746     49.711387      1.796384   \n",
       "25%      945.000000      0.400000     61.000000     56.000000      4.710000   \n",
       "50%     2397.000000      0.700000     61.800000     57.000000      5.690000   \n",
       "75%     5331.000000      1.040000     62.500000     59.000000      6.540000   \n",
       "max    18823.000000      2.454322     66.646117     65.173617      9.662704   \n",
       "\n",
       "                  y             z           cut         color       clarity  \\\n",
       "count  40455.000000  40455.000000  40455.000000  40455.000000  40455.000000   \n",
       "mean       5.731737      3.537474      3.904783      4.400766      4.049388   \n",
       "std        1.114987      0.694681      1.117876      1.701260      1.648181   \n",
       "min        1.827576      1.103360      1.000000      1.000000      1.000000   \n",
       "25%        4.720000      2.910000      3.000000      3.000000      3.000000   \n",
       "50%        5.710000      3.520000      4.000000      4.000000      4.000000   \n",
       "75%        6.540000      4.035000      5.000000      6.000000      5.000000   \n",
       "max        9.635886      5.971430      5.000000      7.000000      8.000000   \n",
       "\n",
       "               city           x/y            td            ad  \n",
       "count  40455.000000  40455.000000  40455.000000  40455.000000  \n",
       "mean       1.036433      0.999362      0.930796      0.006131  \n",
       "std        0.019357      0.008881      0.046552      0.000308  \n",
       "min        1.000000      0.960889      0.766397      0.002847  \n",
       "25%        1.031075      0.992606      0.898876      0.006047  \n",
       "50%        1.038843      0.995736      0.923825      0.006116  \n",
       "75%        1.048770      1.006928      0.955519      0.006189  \n",
       "max        1.069918      1.037898      1.095425      0.019147  "
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df[['x/y', 'td', 'ad']] = sub_outlyers(clean_df[['x/y', 'td', 'ad']],3.5)\n",
    "clean_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40455 entries, 0 to 40454\n",
      "Data columns (total 14 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   price    40455 non-null  int64  \n",
      " 1   carat    40455 non-null  float64\n",
      " 2   depth    40455 non-null  float64\n",
      " 3   table    40455 non-null  float64\n",
      " 4   x        40455 non-null  float64\n",
      " 5   y        40455 non-null  float64\n",
      " 6   z        40455 non-null  float64\n",
      " 7   cut      40455 non-null  float64\n",
      " 8   color    40455 non-null  float64\n",
      " 9   clarity  40455 non-null  float64\n",
      " 10  city     40455 non-null  float64\n",
      " 11  x/y      40455 non-null  float64\n",
      " 12  td       40455 non-null  float64\n",
      " 13  ad       40455 non-null  float64\n",
      "dtypes: float64(13), int64(1)\n",
      "memory usage: 4.3 MB\n"
     ]
    }
   ],
   "source": [
    "clean_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['price', 'carat', 'depth', 'table', 'x', 'y', 'z', 'cut', 'color',\n",
       "       'clarity', 'city', 'x/y', 'td', 'ad'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"selected_features = ['carat', 'depth', 'table', 'x', 'y', 'z', 'cut', 'color',\\n       'clarity', 'city']#,'x/y']#, 'y/z', 'x/z', 'apparent_density']\\ntarget = 'price\""
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''selected_features = ['carat', 'depth', 'table', 'x', 'y', 'z', 'cut', 'color',\n",
    "       'clarity', 'city']#,'x/y']#, 'y/z', 'x/z', 'apparent_density']\n",
    "target = 'price'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def filter_transform(df,selected_features,target):\\n    data_df = df[selected_features]\\n    to_dummy = ['city']\\n    for feature in to_dummy:\\n        dummies_df = pd.get_dummies(data_df[feature], prefix=feature, drop_first=True)\\n        data_df = pd.concat([data_df, dummies_df],axis=1).drop(feature, axis=1)   \\n    data_df = pd.concat([data_df.select_dtypes(exclude=['object']), df[target]], axis=1).dropna()\\n    return data_df\""
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def filter_transform(df,selected_features,target):\n",
    "    data_df = df[selected_features]\n",
    "    to_dummy = ['city']\n",
    "    for feature in to_dummy:\n",
    "        dummies_df = pd.get_dummies(data_df[feature], prefix=feature, drop_first=True)\n",
    "        data_df = pd.concat([data_df, dummies_df],axis=1).drop(feature, axis=1)   \n",
    "    data_df = pd.concat([data_df.select_dtypes(exclude=['object']), df[target]], axis=1).dropna()\n",
    "    return data_df'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_df = pd.concat([clean_df, pd.get_dummies(clean_df[['cut', 'color', 'clarity','city']], drop_first=True)], axis = 1).drop(['cut', 'color', 'clarity','city'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_df = filter_transform(clean_df,selected_features,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.to_csv('./data/clean/diamondsdlfe2xso.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = clean_df.drop(['price'], axis=1)\n",
    "y = clean_df['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reducer = UMAP(n_components=10)\n",
    "#scaler = StandardScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nX =X.fillna(0)\\nX = np.nan_to_num(X.astype(np.float32))\\nX = scaler.fit_transform(X)\\nX = reducer.fit_transform(X)\\nX'"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "X =X.fillna(0)\n",
    "X = np.nan_to_num(X.astype(np.float32))\n",
    "X = scaler.fit_transform(X)\n",
    "X = reducer.fit_transform(X)\n",
    "X'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>city</th>\n",
       "      <th>x/y</th>\n",
       "      <th>td</th>\n",
       "      <th>ad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.796861</td>\n",
       "      <td>61.749873</td>\n",
       "      <td>57.439239</td>\n",
       "      <td>5.729562</td>\n",
       "      <td>5.731737</td>\n",
       "      <td>3.537474</td>\n",
       "      <td>3.904783</td>\n",
       "      <td>4.400766</td>\n",
       "      <td>4.049388</td>\n",
       "      <td>1.036433</td>\n",
       "      <td>0.999362</td>\n",
       "      <td>0.930796</td>\n",
       "      <td>0.006131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.471897</td>\n",
       "      <td>1.381551</td>\n",
       "      <td>2.195948</td>\n",
       "      <td>1.123308</td>\n",
       "      <td>1.114987</td>\n",
       "      <td>0.694681</td>\n",
       "      <td>1.117876</td>\n",
       "      <td>1.701260</td>\n",
       "      <td>1.648181</td>\n",
       "      <td>0.019357</td>\n",
       "      <td>0.008881</td>\n",
       "      <td>0.046552</td>\n",
       "      <td>0.000308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>56.855746</td>\n",
       "      <td>49.711387</td>\n",
       "      <td>1.796384</td>\n",
       "      <td>1.827576</td>\n",
       "      <td>1.103360</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.960889</td>\n",
       "      <td>0.766397</td>\n",
       "      <td>0.002847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>4.710000</td>\n",
       "      <td>4.720000</td>\n",
       "      <td>2.910000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.031075</td>\n",
       "      <td>0.992606</td>\n",
       "      <td>0.898876</td>\n",
       "      <td>0.006047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.700000</td>\n",
       "      <td>61.800000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>5.690000</td>\n",
       "      <td>5.710000</td>\n",
       "      <td>3.520000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.038843</td>\n",
       "      <td>0.995736</td>\n",
       "      <td>0.923825</td>\n",
       "      <td>0.006116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.040000</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>6.540000</td>\n",
       "      <td>6.540000</td>\n",
       "      <td>4.035000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.048770</td>\n",
       "      <td>1.006928</td>\n",
       "      <td>0.955519</td>\n",
       "      <td>0.006189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.454322</td>\n",
       "      <td>66.646117</td>\n",
       "      <td>65.173617</td>\n",
       "      <td>9.662704</td>\n",
       "      <td>9.635886</td>\n",
       "      <td>5.971430</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.069918</td>\n",
       "      <td>1.037898</td>\n",
       "      <td>1.095425</td>\n",
       "      <td>0.019147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              carat         depth         table             x             y  \\\n",
       "count  40455.000000  40455.000000  40455.000000  40455.000000  40455.000000   \n",
       "mean       0.796861     61.749873     57.439239      5.729562      5.731737   \n",
       "std        0.471897      1.381551      2.195948      1.123308      1.114987   \n",
       "min        0.200000     56.855746     49.711387      1.796384      1.827576   \n",
       "25%        0.400000     61.000000     56.000000      4.710000      4.720000   \n",
       "50%        0.700000     61.800000     57.000000      5.690000      5.710000   \n",
       "75%        1.040000     62.500000     59.000000      6.540000      6.540000   \n",
       "max        2.454322     66.646117     65.173617      9.662704      9.635886   \n",
       "\n",
       "                  z           cut         color       clarity          city  \\\n",
       "count  40455.000000  40455.000000  40455.000000  40455.000000  40455.000000   \n",
       "mean       3.537474      3.904783      4.400766      4.049388      1.036433   \n",
       "std        0.694681      1.117876      1.701260      1.648181      0.019357   \n",
       "min        1.103360      1.000000      1.000000      1.000000      1.000000   \n",
       "25%        2.910000      3.000000      3.000000      3.000000      1.031075   \n",
       "50%        3.520000      4.000000      4.000000      4.000000      1.038843   \n",
       "75%        4.035000      5.000000      6.000000      5.000000      1.048770   \n",
       "max        5.971430      5.000000      7.000000      8.000000      1.069918   \n",
       "\n",
       "                x/y            td            ad  \n",
       "count  40455.000000  40455.000000  40455.000000  \n",
       "mean       0.999362      0.930796      0.006131  \n",
       "std        0.008881      0.046552      0.000308  \n",
       "min        0.960889      0.766397      0.002847  \n",
       "25%        0.992606      0.898876      0.006047  \n",
       "50%        0.995736      0.923825      0.006116  \n",
       "75%        1.006928      0.955519      0.006189  \n",
       "max        1.037898      1.095425      0.019147  "
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Initialize RFE with XGBoost model\\nrfe = RFE(estimator=xgb_model, n_features_to_select=len(X.columns)-1)\\n\\n# Fit RFE to training data\\nrfe.fit(X_train, y_train)\\n\\n# Get selected features\\nselected_features = X_train.columns[rfe.support_]\\nselected_features'"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the XGBoost regressor\n",
    "xgb_model = XGBRegressor( missing= np.inf)\n",
    "\n",
    "# Optimal gridsearch with data labeling\n",
    "param_grid_dl = {\n",
    "    'n_estimators': [225, 230, 235],\n",
    "    'max_depth': [ 5, 6, 7],\n",
    "    'learning_rate': [ 0.075, 0.07, 0.065],\n",
    "    'missing': [np.inf]\n",
    "}\n",
    "\n",
    "param_grid_deep = {\n",
    "        'n_estimators': [ 210, 235, 250],\n",
    "        'min_child_weight': [ 12, 15, 17],\n",
    "        'alpha': [0.3,0.6],\n",
    "        'lambda': [0.3,0.6],\n",
    "        'gamma': [0.02, 0.05, 0.07],\n",
    "        'subsample': [0.8],\n",
    "        'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "        'max_depth': [6],\n",
    "        'learning_rate': [ 0.07],\n",
    "         }\n",
    "\n",
    "param_grid_dart = {\n",
    "    'booster': ['dart'],\n",
    "    'n_estimators': [230],\n",
    "    'max_depth': [  6],\n",
    "    'learning_rate': [ 0.07],\n",
    "    'min_child_weight': [1],\n",
    "              'rate_drop': [0.10],\n",
    "              'skip_drop': [0.5]\n",
    "}\n",
    "\n",
    "param_grid_en = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [ 7, 9, 11],\n",
    "    'learning_rate': [0.01, 0.1,0.2]\n",
    "}\n",
    "\n",
    "param_grid_aiama ={'colsample_bytree': [ 0.8, 0.9, 1],\n",
    "                    'learning_rate': [0.04, 0.05, 0.06],\n",
    "                      'max_depth': [5, 6, 7],\n",
    "                        'n_estimators': [200, 220, 240],\n",
    "                          'subsample': [0.8,0.9,1],\n",
    "                              'missing': [np.inf]} \n",
    "\n",
    "param_grid_art ={'colsample_bytree': [0.9, 0.95, 1],\n",
    "                    'learning_rate': [ 0.01, 0.012, 0.014],\n",
    "                      'max_depth': [ 6, 7, 8],\n",
    "                        'n_estimators': [ 1000, 1200, 1400],\n",
    "                          'subsample': [0.8,0.85],\n",
    "                              'missing': [np.inf]} \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Initialize XGBoost model\n",
    "'''\n",
    "# Initialize RFE with XGBoost model\n",
    "rfe = RFE(estimator=xgb_model, n_features_to_select=len(X.columns)-1)\n",
    "\n",
    "# Fit RFE to training data\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "# Get selected features\n",
    "selected_features = X_train.columns[rfe.support_]\n",
    "selected_features'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n",
      "[CV 3/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1000, subsample=0.8;, score=-512.542 total time=   4.6s\n",
      "[CV 4/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1000, subsample=0.85;, score=-553.397 total time=   4.5s\n",
      "[CV 1/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1000, subsample=0.8;, score=-502.658 total time=   4.6s\n",
      "[CV 2/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1000, subsample=0.8;, score=-504.133 total time=   4.7s\n",
      "[CV 4/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1000, subsample=0.8;, score=-553.861 total time=   4.8s\n",
      "[CV 2/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1000, subsample=0.85;, score=-504.067 total time=   4.8s\n",
      "[CV 5/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1000, subsample=0.8;, score=-506.995 total time=   4.9s\n",
      "[CV 1/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1000, subsample=0.85;, score=-503.769 total time=   4.9s\n",
      "[CV 3/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1000, subsample=0.85;, score=-513.460 total time=   5.4s\n",
      "[CV 5/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1000, subsample=0.85;, score=-506.362 total time=   5.5s\n",
      "[CV 1/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1200, subsample=0.85;, score=-502.562 total time=   5.4s\n",
      "[CV 3/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1200, subsample=0.85;, score=-512.693 total time=   5.4s\n",
      "[CV 4/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1200, subsample=0.8;, score=-552.166 total time=   5.5s\n",
      "[CV 1/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1200, subsample=0.8;, score=-502.881 total time=   5.6s\n",
      "[CV 5/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1200, subsample=0.8;, score=-506.829 total time=   5.6s\n",
      "[CV 2/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1200, subsample=0.8;, score=-502.609 total time=   5.7s\n",
      "[CV 3/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1200, subsample=0.8;, score=-511.836 total time=   6.1s\n",
      "[CV 4/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1200, subsample=0.85;, score=-551.293 total time=   6.0s\n",
      "[CV 2/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1200, subsample=0.85;, score=-502.970 total time=   6.3s\n",
      "[CV 5/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1200, subsample=0.85;, score=-505.789 total time=   6.2s\n",
      "[CV 4/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1400, subsample=0.8;, score=-551.208 total time=   6.1s\n",
      "[CV 3/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1400, subsample=0.8;, score=-511.635 total time=   6.2s\n",
      "[CV 1/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1400, subsample=0.8;, score=-502.776 total time=   6.3s\n",
      "[CV 2/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1400, subsample=0.8;, score=-501.910 total time=   6.5s\n",
      "[CV 3/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1400, subsample=0.85;, score=-512.464 total time=   6.6s\n",
      "[CV 1/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1400, subsample=0.85;, score=-502.347 total time=   6.7s\n",
      "[CV 5/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1400, subsample=0.8;, score=-506.255 total time=   6.7s\n",
      "[CV 2/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1400, subsample=0.85;, score=-502.130 total time=   6.7s\n",
      "[CV 2/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1000, subsample=0.8;, score=-502.845 total time=   6.8s\n",
      "[CV 3/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1000, subsample=0.8;, score=-510.836 total time=   6.9s\n",
      "[CV 1/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1000, subsample=0.8;, score=-499.312 total time=   7.0s\n",
      "[CV 4/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1400, subsample=0.85;, score=-550.284 total time=   7.3s\n",
      "[CV 5/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1400, subsample=0.85;, score=-505.126 total time=   7.2s\n",
      "[CV 4/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1000, subsample=0.8;, score=-551.070 total time=   7.2s\n",
      "[CV 5/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1000, subsample=0.8;, score=-505.590 total time=   7.3s\n",
      "[CV 1/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1000, subsample=0.85;, score=-499.901 total time=   7.3s\n",
      "[CV 3/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1000, subsample=0.85;, score=-510.501 total time=   8.3s\n",
      "[CV 2/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1000, subsample=0.85;, score=-504.234 total time=   8.5s\n",
      "[CV 4/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1000, subsample=0.85;, score=-552.810 total time=   8.5s\n",
      "[CV 5/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1000, subsample=0.85;, score=-505.098 total time=   8.6s\n",
      "[CV 1/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1200, subsample=0.8;, score=-499.833 total time=   8.2s\n",
      "[CV 2/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1200, subsample=0.8;, score=-502.033 total time=   8.2s\n",
      "[CV 3/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1200, subsample=0.8;, score=-510.191 total time=   8.3s\n",
      "[CV 4/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1200, subsample=0.8;, score=-550.393 total time=   8.4s\n",
      "[CV 5/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1200, subsample=0.8;, score=-505.196 total time=   8.5s\n",
      "[CV 2/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1200, subsample=0.85;, score=-503.538 total time=   8.8s\n",
      "[CV 1/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1200, subsample=0.85;, score=-500.197 total time=   8.9s\n",
      "[CV 3/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1200, subsample=0.85;, score=-510.500 total time=   8.8s\n",
      "[CV 4/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1200, subsample=0.85;, score=-552.065 total time=   8.5s\n",
      "[CV 5/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1200, subsample=0.85;, score=-504.332 total time=   8.3s\n",
      "[CV 1/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1400, subsample=0.8;, score=-499.941 total time=   9.8s\n",
      "[CV 4/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1400, subsample=0.8;, score=-549.943 total time=  10.2s\n",
      "[CV 5/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1400, subsample=0.8;, score=-504.990 total time=  10.1s\n",
      "[CV 1/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1400, subsample=0.85;, score=-500.960 total time=  10.3s\n",
      "[CV 3/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1400, subsample=0.8;, score=-510.080 total time=  11.3s\n",
      "[CV 2/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1400, subsample=0.8;, score=-502.201 total time=  11.5s\n",
      "[CV 2/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1400, subsample=0.85;, score=-503.352 total time=  11.6s\n",
      "[CV 3/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1400, subsample=0.85;, score=-510.716 total time=  11.7s\n",
      "[CV 4/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1400, subsample=0.85;, score=-551.747 total time=  11.4s\n",
      "[CV 5/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1400, subsample=0.85;, score=-504.094 total time=  11.7s\n",
      "[CV 1/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1000, subsample=0.8;, score=-499.146 total time=  11.2s\n",
      "[CV 2/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1000, subsample=0.8;, score=-507.406 total time=  11.4s\n",
      "[CV 4/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1000, subsample=0.8;, score=-552.150 total time=  11.2s\n",
      "[CV 3/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1000, subsample=0.8;, score=-513.560 total time=  12.0s\n",
      "[CV 5/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1000, subsample=0.8;, score=-508.298 total time=  11.9s\n",
      "[CV 1/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1000, subsample=0.85;, score=-500.103 total time=  12.0s\n",
      "[CV 4/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1000, subsample=0.85;, score=-551.105 total time=  11.5s\n",
      "[CV 5/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1000, subsample=0.85;, score=-508.269 total time=  11.6s\n",
      "[CV 3/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1000, subsample=0.85;, score=-515.129 total time=  12.2s\n",
      "[CV 2/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1000, subsample=0.85;, score=-507.186 total time=  12.6s\n",
      "[CV 1/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1200, subsample=0.8;, score=-499.646 total time=  13.5s\n",
      "[CV 2/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1200, subsample=0.8;, score=-507.433 total time=  14.4s\n",
      "[CV 3/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1200, subsample=0.8;, score=-513.258 total time=  14.4s\n",
      "[CV 4/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1200, subsample=0.8;, score=-551.275 total time=  14.2s\n",
      "[CV 5/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1200, subsample=0.8;, score=-507.746 total time=  17.0s\n",
      "[CV 1/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1200, subsample=0.85;, score=-500.601 total time=  17.1s\n",
      "[CV 1/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1000, subsample=0.8;, score=-503.118 total time=   5.6s\n",
      "[CV 2/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1000, subsample=0.8;, score=-504.083 total time=   5.3s\n",
      "[CV 3/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1000, subsample=0.8;, score=-513.254 total time=   5.2s\n",
      "[CV 4/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1000, subsample=0.8;, score=-551.104 total time=   5.1s\n",
      "[CV 3/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1200, subsample=0.85;, score=-515.121 total time=  16.6s\n",
      "[CV 2/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1200, subsample=0.85;, score=-506.586 total time=  16.9s\n",
      "[CV 4/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1200, subsample=0.85;, score=-550.890 total time=  16.7s\n",
      "[CV 5/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1200, subsample=0.85;, score=-507.625 total time=  16.5s\n",
      "[CV 1/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1400, subsample=0.8;, score=-500.170 total time=  14.9s\n",
      "[CV 2/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1400, subsample=0.8;, score=-507.589 total time=  15.1s\n",
      "[CV 3/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1400, subsample=0.8;, score=-513.489 total time=  15.0s\n",
      "[CV 2/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1000, subsample=0.85;, score=-503.386 total time=   4.7s\n",
      "[CV 5/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1000, subsample=0.8;, score=-506.526 total time=   5.3s\n",
      "[CV 4/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1400, subsample=0.8;, score=-551.031 total time=  15.3s\n",
      "[CV 1/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1000, subsample=0.85;, score=-502.673 total time=   5.4s\n",
      "[CV 3/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1400, subsample=0.85;, score=-515.341 total time=  14.8s\n",
      "[CV 3/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1000, subsample=0.85;, score=-510.662 total time=   4.6s\n",
      "[CV 2/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1400, subsample=0.85;, score=-506.925 total time=  15.0s\n",
      "[CV 5/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1400, subsample=0.8;, score=-507.706 total time=  15.4s\n",
      "[CV 4/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1000, subsample=0.85;, score=-550.663 total time=   4.7s\n",
      "[CV 5/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1000, subsample=0.85;, score=-505.366 total time=   4.7s\n",
      "[CV 1/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1400, subsample=0.85;, score=-501.201 total time=  15.4s\n",
      "[CV 4/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1400, subsample=0.85;, score=-551.023 total time=  15.7s\n",
      "[CV 5/5] END colsample_bytree=0.9, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1400, subsample=0.85;, score=-507.659 total time=  15.6s\n",
      "[CV 2/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1200, subsample=0.8;, score=-503.573 total time=   5.7s\n",
      "[CV 1/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1200, subsample=0.8;, score=-503.783 total time=   5.8s\n",
      "[CV 3/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1200, subsample=0.8;, score=-512.639 total time=   5.8s\n",
      "[CV 4/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1200, subsample=0.8;, score=-550.249 total time=   5.8s\n",
      "[CV 5/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1200, subsample=0.8;, score=-506.018 total time=   5.2s\n",
      "[CV 1/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1200, subsample=0.85;, score=-502.633 total time=   5.0s\n",
      "[CV 2/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1200, subsample=0.85;, score=-503.467 total time=   5.1s\n",
      "[CV 3/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1200, subsample=0.85;, score=-510.958 total time=   5.4s\n",
      "[CV 5/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1200, subsample=0.85;, score=-505.329 total time=   5.1s\n",
      "[CV 4/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1200, subsample=0.85;, score=-549.121 total time=   5.9s\n",
      "[CV 2/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1400, subsample=0.8;, score=-503.265 total time=   6.0s\n",
      "[CV 4/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1400, subsample=0.8;, score=-549.213 total time=   6.1s\n",
      "[CV 1/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1400, subsample=0.8;, score=-503.995 total time=   6.8s\n",
      "[CV 3/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1400, subsample=0.8;, score=-512.621 total time=   6.1s\n",
      "[CV 5/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1400, subsample=0.8;, score=-505.928 total time=   6.1s\n",
      "[CV 1/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1400, subsample=0.85;, score=-503.276 total time=   6.3s\n",
      "[CV 3/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1400, subsample=0.85;, score=-511.413 total time=   6.0s\n",
      "[CV 2/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1400, subsample=0.85;, score=-503.873 total time=   6.3s\n",
      "[CV 4/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1400, subsample=0.85;, score=-547.929 total time=   6.3s\n",
      "[CV 5/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1400, subsample=0.85;, score=-505.219 total time=   6.4s\n",
      "[CV 1/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1000, subsample=0.8;, score=-498.090 total time=   8.0s\n",
      "[CV 2/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1000, subsample=0.8;, score=-502.773 total time=   7.9s\n",
      "[CV 5/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1000, subsample=0.8;, score=-505.357 total time=   6.6s\n",
      "[CV 3/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1000, subsample=0.8;, score=-510.713 total time=   7.9s\n",
      "[CV 4/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1000, subsample=0.8;, score=-550.851 total time=   8.2s\n",
      "[CV 2/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1000, subsample=0.85;, score=-504.064 total time=   6.5s\n",
      "[CV 1/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1000, subsample=0.85;, score=-497.936 total time=   6.8s\n",
      "[CV 3/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1000, subsample=0.85;, score=-510.095 total time=   6.6s\n",
      "[CV 4/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1000, subsample=0.85;, score=-550.361 total time=   7.1s\n",
      "[CV 5/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1000, subsample=0.85;, score=-504.582 total time=   8.4s\n",
      "[CV 1/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1200, subsample=0.8;, score=-498.505 total time=   8.1s\n",
      "[CV 2/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1200, subsample=0.8;, score=-502.681 total time=   8.3s\n",
      "[CV 5/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1200, subsample=0.8;, score=-504.901 total time=   8.3s\n",
      "[CV 4/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1200, subsample=0.8;, score=-549.783 total time=   8.5s\n",
      "[CV 2/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1200, subsample=0.85;, score=-503.405 total time=   8.6s\n",
      "[CV 1/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1200, subsample=0.85;, score=-498.722 total time=   8.6s\n",
      "[CV 3/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1200, subsample=0.85;, score=-509.920 total time=   8.8s\n",
      "[CV 4/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1200, subsample=0.85;, score=-549.471 total time=   8.4s\n",
      "[CV 3/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1200, subsample=0.8;, score=-510.982 total time=   9.8s\n",
      "[CV 5/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1200, subsample=0.85;, score=-504.108 total time=   8.7s\n",
      "[CV 3/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1400, subsample=0.8;, score=-510.874 total time=   9.6s\n",
      "[CV 1/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1400, subsample=0.85;, score=-499.432 total time=   9.4s\n",
      "[CV 2/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1400, subsample=0.85;, score=-503.813 total time=   9.8s\n",
      "[CV 3/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1400, subsample=0.85;, score=-510.217 total time=   9.8s\n",
      "[CV 1/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1400, subsample=0.8;, score=-499.066 total time=  11.7s\n",
      "[CV 4/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1400, subsample=0.8;, score=-549.376 total time=  11.6s\n",
      "[CV 4/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1400, subsample=0.85;, score=-548.994 total time=  10.0s\n",
      "[CV 2/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1400, subsample=0.8;, score=-502.693 total time=  12.2s\n",
      "[CV 5/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1400, subsample=0.8;, score=-504.739 total time=  11.8s\n",
      "[CV 5/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1400, subsample=0.85;, score=-504.204 total time=  11.6s\n",
      "[CV 1/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1000, subsample=0.8;, score=-499.325 total time=  11.3s\n",
      "[CV 2/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1000, subsample=0.8;, score=-508.649 total time=  11.1s\n",
      "[CV 3/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1000, subsample=0.8;, score=-512.400 total time=  11.6s\n",
      "[CV 5/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1000, subsample=0.8;, score=-505.820 total time=  11.6s\n",
      "[CV 4/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1000, subsample=0.8;, score=-551.988 total time=  11.9s\n",
      "[CV 1/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1000, subsample=0.85;, score=-499.774 total time=  11.9s\n",
      "[CV 2/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1000, subsample=0.85;, score=-507.917 total time=  11.9s\n",
      "[CV 3/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1000, subsample=0.85;, score=-514.387 total time=  11.8s\n",
      "[CV 5/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1000, subsample=0.85;, score=-507.684 total time=  11.9s\n",
      "[CV 4/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1000, subsample=0.85;, score=-552.342 total time=  14.5s\n",
      "[CV 1/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1200, subsample=0.8;, score=-499.668 total time=  13.2s\n",
      "[CV 2/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1200, subsample=0.8;, score=-508.710 total time=  13.2s\n",
      "[CV 3/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1200, subsample=0.8;, score=-513.148 total time=  13.4s\n",
      "[CV 4/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1200, subsample=0.8;, score=-551.406 total time=  13.5s\n",
      "[CV 2/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1200, subsample=0.85;, score=-507.725 total time=  14.4s\n",
      "[CV 1/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1000, subsample=0.8;, score=-502.362 total time=   5.0s\n",
      "[CV 2/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1000, subsample=0.8;, score=-503.604 total time=   4.9s\n",
      "[CV 5/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1200, subsample=0.8;, score=-505.745 total time=  16.7s\n",
      "[CV 1/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1200, subsample=0.85;, score=-500.275 total time=  16.7s\n",
      "[CV 3/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1000, subsample=0.8;, score=-512.511 total time=   4.9s\n",
      "[CV 3/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1200, subsample=0.85;, score=-514.877 total time=  16.8s\n",
      "[CV 4/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1200, subsample=0.85;, score=-552.532 total time=  16.9s\n",
      "[CV 4/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1000, subsample=0.8;, score=-550.634 total time=   5.0s\n",
      "[CV 2/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1400, subsample=0.8;, score=-508.918 total time=  15.0s\n",
      "[CV 1/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1400, subsample=0.8;, score=-500.413 total time=  15.4s\n",
      "[CV 5/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1000, subsample=0.8;, score=-505.682 total time=   5.0s\n",
      "[CV 5/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1200, subsample=0.85;, score=-507.517 total time=  16.1s\n",
      "[CV 3/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1400, subsample=0.8;, score=-513.287 total time=  15.3s\n",
      "[CV 1/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1000, subsample=0.85;, score=-502.855 total time=   4.5s\n",
      "[CV 2/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1000, subsample=0.85;, score=-503.386 total time=   4.4s\n",
      "[CV 5/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1400, subsample=0.8;, score=-505.795 total time=  15.5s\n",
      "[CV 1/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1400, subsample=0.85;, score=-500.652 total time=  15.3s\n",
      "[CV 4/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1400, subsample=0.8;, score=-551.395 total time=  15.7s\n",
      "[CV 5/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1000, subsample=0.85;, score=-504.525 total time=   4.3s\n",
      "[CV 3/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1400, subsample=0.85;, score=-515.325 total time=  15.1s\n",
      "[CV 2/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1400, subsample=0.85;, score=-508.163 total time=  15.7s\n",
      "[CV 3/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1000, subsample=0.85;, score=-510.760 total time=   5.1s\n",
      "[CV 4/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1000, subsample=0.85;, score=-551.357 total time=   5.1s\n",
      "[CV 4/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1400, subsample=0.85;, score=-552.670 total time=  15.1s\n",
      "[CV 1/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1200, subsample=0.8;, score=-503.750 total time=   6.1s[CV 3/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1200, subsample=0.8;, score=-512.620 total time=   5.3s\n",
      "\n",
      "[CV 2/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1200, subsample=0.8;, score=-503.010 total time=   6.2s\n",
      "[CV 4/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1200, subsample=0.8;, score=-549.237 total time=   5.4s\n",
      "[CV 5/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1200, subsample=0.8;, score=-505.647 total time=   5.3s\n",
      "[CV 1/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1200, subsample=0.85;, score=-502.811 total time=   5.4s\n",
      "[CV 3/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1200, subsample=0.85;, score=-510.585 total time=   5.3s\n",
      "[CV 4/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1200, subsample=0.85;, score=-550.304 total time=   5.1s\n",
      "[CV 5/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1200, subsample=0.85;, score=-504.430 total time=   5.2s\n",
      "[CV 2/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1200, subsample=0.85;, score=-503.602 total time=   6.1s\n",
      "[CV 2/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1400, subsample=0.8;, score=-503.156 total time=   6.1s\n",
      "[CV 1/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1400, subsample=0.8;, score=-503.862 total time=   6.3s\n",
      "[CV 4/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1400, subsample=0.8;, score=-548.622 total time=   5.9s\n",
      "[CV 3/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1400, subsample=0.8;, score=-512.692 total time=   6.3s\n",
      "[CV 5/5] END colsample_bytree=0.9, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1400, subsample=0.85;, score=-507.274 total time=  18.1s\n",
      "[CV 5/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1400, subsample=0.8;, score=-505.232 total time=   6.4s\n",
      "[CV 1/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1400, subsample=0.85;, score=-503.993 total time=   6.6s\n",
      "[CV 4/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1400, subsample=0.85;, score=-549.469 total time=   6.4s\n",
      "[CV 2/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1400, subsample=0.85;, score=-503.975 total time=   7.2s\n",
      "[CV 3/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1400, subsample=0.85;, score=-510.593 total time=   7.3s\n",
      "[CV 1/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1000, subsample=0.8;, score=-500.702 total time=   6.7s\n",
      "[CV 5/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1400, subsample=0.85;, score=-504.806 total time=   7.7s\n",
      "[CV 3/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1000, subsample=0.8;, score=-510.031 total time=   6.7s\n",
      "[CV 4/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1000, subsample=0.8;, score=-549.770 total time=   6.7s\n",
      "[CV 5/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1000, subsample=0.8;, score=-505.655 total time=   7.1s\n",
      "[CV 2/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1000, subsample=0.8;, score=-503.775 total time=   8.5s\n",
      "[CV 2/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1000, subsample=0.85;, score=-505.009 total time=   6.6s\n",
      "[CV 3/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1000, subsample=0.85;, score=-510.108 total time=   6.6s\n",
      "[CV 1/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1000, subsample=0.85;, score=-499.526 total time=   7.3s\n",
      "[CV 5/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1000, subsample=0.85;, score=-503.339 total time=   6.7s\n",
      "[CV 4/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1000, subsample=0.85;, score=-551.204 total time=   8.4s\n",
      "[CV 2/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1200, subsample=0.8;, score=-503.521 total time=   8.0s\n",
      "[CV 3/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1200, subsample=0.8;, score=-510.302 total time=   8.5s\n",
      "[CV 1/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1200, subsample=0.8;, score=-501.602 total time=   8.9s\n",
      "[CV 5/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1200, subsample=0.8;, score=-505.046 total time=   8.7s\n",
      "[CV 1/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1200, subsample=0.85;, score=-499.999 total time=   8.9s\n",
      "[CV 2/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1200, subsample=0.85;, score=-504.995 total time=   8.8s\n",
      "[CV 4/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1200, subsample=0.8;, score=-549.072 total time=  10.1s\n",
      "[CV 5/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1200, subsample=0.85;, score=-502.736 total time=   8.5s\n",
      "[CV 3/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1200, subsample=0.85;, score=-510.155 total time=  10.0s\n",
      "[CV 4/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1200, subsample=0.85;, score=-550.706 total time=  10.4s\n",
      "[CV 3/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1400, subsample=0.8;, score=-510.673 total time=   9.3s\n",
      "[CV 2/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1400, subsample=0.8;, score=-503.875 total time=   9.5s\n",
      "[CV 1/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1400, subsample=0.85;, score=-500.768 total time=   9.4s\n",
      "[CV 2/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1400, subsample=0.85;, score=-505.163 total time=   9.5s\n",
      "[CV 4/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1400, subsample=0.8;, score=-548.787 total time=  10.1s\n",
      "[CV 1/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1400, subsample=0.8;, score=-502.460 total time=  11.3s\n",
      "[CV 3/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1400, subsample=0.85;, score=-510.363 total time=   9.8s\n",
      "[CV 4/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1400, subsample=0.85;, score=-550.632 total time=   9.5s\n",
      "[CV 5/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1400, subsample=0.8;, score=-505.210 total time=  11.5s\n",
      "[CV 1/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1000, subsample=0.8;, score=-499.189 total time=  10.7s\n",
      "[CV 5/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1400, subsample=0.85;, score=-502.423 total time=  12.1s\n",
      "[CV 2/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1000, subsample=0.8;, score=-508.017 total time=  11.5s\n",
      "[CV 3/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1000, subsample=0.8;, score=-513.024 total time=  11.3s\n",
      "[CV 4/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1000, subsample=0.8;, score=-552.923 total time=  11.4s\n",
      "[CV 5/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1000, subsample=0.8;, score=-506.400 total time=  11.5s\n",
      "[CV 1/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1000, subsample=0.85;, score=-500.524 total time=  11.6s\n",
      "[CV 3/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1000, subsample=0.85;, score=-515.550 total time=  11.7s\n",
      "[CV 2/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1000, subsample=0.85;, score=-507.624 total time=  14.1s\n",
      "[CV 4/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1000, subsample=0.85;, score=-552.693 total time=  13.9s\n",
      "[CV 5/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1000, subsample=0.85;, score=-506.889 total time=  14.0s\n",
      "[CV 1/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1200, subsample=0.8;, score=-499.900 total time=  13.5s\n",
      "[CV 2/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1200, subsample=0.8;, score=-508.187 total time=  13.6s\n",
      "[CV 3/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1200, subsample=0.8;, score=-513.076 total time=  13.1s\n",
      "[CV 4/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1200, subsample=0.8;, score=-553.117 total time=  13.2s\n",
      "[CV 5/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1200, subsample=0.8;, score=-506.560 total time=  13.5s\n",
      "[CV 2/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1200, subsample=0.85;, score=-507.671 total time=  13.6s\n",
      "[CV 3/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1200, subsample=0.85;, score=-516.299 total time=  13.3s\n",
      "[CV 1/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1200, subsample=0.85;, score=-500.742 total time=  16.5s\n",
      "[CV 5/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1200, subsample=0.85;, score=-507.058 total time=  13.0s\n",
      "[CV 1/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1000, subsample=0.8;, score=-503.088 total time=   5.0s\n",
      "[CV 2/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1000, subsample=0.8;, score=-504.254 total time=   5.2s\n",
      "[CV 3/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1000, subsample=0.8;, score=-513.231 total time=   5.1s\n",
      "[CV 4/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1200, subsample=0.85;, score=-552.715 total time=  16.4s\n",
      "[CV 4/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1000, subsample=0.8;, score=-552.880 total time=   5.1s\n",
      "[CV 5/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1000, subsample=0.8;, score=-506.407 total time=   5.3s\n",
      "[CV 1/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1000, subsample=0.85;, score=-503.440 total time=   5.1s\n",
      "[CV 2/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1000, subsample=0.85;, score=-504.525 total time=   5.1s\n",
      "[CV 3/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1400, subsample=0.8;, score=-513.862 total time=  14.9s\n",
      "[CV 2/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1400, subsample=0.8;, score=-508.606 total time=  15.1s\n",
      "[CV 4/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1400, subsample=0.8;, score=-553.224 total time=  15.2s\n",
      "[CV 1/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1400, subsample=0.85;, score=-501.456 total time=  14.8s\n",
      "[CV 3/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1000, subsample=0.85;, score=-512.947 total time=   4.8s\n",
      "[CV 5/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1400, subsample=0.8;, score=-506.912 total time=  15.3s\n",
      "[CV 5/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1000, subsample=0.85;, score=-506.970 total time=   4.8s\n",
      "[CV 4/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1000, subsample=0.85;, score=-552.593 total time=   5.5s\n",
      "[CV 1/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1400, subsample=0.8;, score=-500.500 total time=  18.1s\n",
      "[CV 2/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1400, subsample=0.85;, score=-508.216 total time=  14.6s\n",
      "[CV 1/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1200, subsample=0.8;, score=-503.736 total time=   5.5s\n",
      "[CV 2/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1200, subsample=0.8;, score=-503.015 total time=   5.7s\n",
      "[CV 4/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1200, subsample=0.8;, score=-551.319 total time=   5.4s\n",
      "[CV 3/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1200, subsample=0.8;, score=-512.118 total time=   6.5s\n",
      "[CV 5/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1200, subsample=0.8;, score=-506.160 total time=   5.6s\n",
      "[CV 1/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1200, subsample=0.85;, score=-503.117 total time=   5.7s\n",
      "[CV 2/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1200, subsample=0.85;, score=-503.502 total time=   5.7s\n",
      "[CV 3/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1200, subsample=0.85;, score=-511.451 total time=   5.6s\n",
      "[CV 4/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1200, subsample=0.85;, score=-550.909 total time=   5.5s\n",
      "[CV 5/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1200, subsample=0.85;, score=-506.429 total time=   5.9s\n",
      "[CV 3/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1400, subsample=0.85;, score=-516.422 total time=  17.7s\n",
      "[CV 2/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1400, subsample=0.8;, score=-502.217 total time=   6.2s\n",
      "[CV 4/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1400, subsample=0.85;, score=-552.678 total time=  17.1s\n",
      "[CV 3/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1400, subsample=0.8;, score=-512.248 total time=   6.4s\n",
      "[CV 4/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1400, subsample=0.8;, score=-550.179 total time=   6.2s\n",
      "[CV 1/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1400, subsample=0.8;, score=-503.398 total time=   6.6s\n",
      "[CV 5/5] END colsample_bytree=0.9, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1400, subsample=0.85;, score=-507.456 total time=  16.9s\n",
      "[CV 3/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1400, subsample=0.85;, score=-511.395 total time=   6.4s\n",
      "[CV 2/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1400, subsample=0.85;, score=-502.905 total time=   6.6s\n",
      "[CV 5/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1400, subsample=0.8;, score=-505.685 total time=   7.1s\n",
      "[CV 4/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1400, subsample=0.85;, score=-549.971 total time=   6.3s\n",
      "[CV 1/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1400, subsample=0.85;, score=-503.690 total time=   7.3s\n",
      "[CV 5/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1400, subsample=0.85;, score=-505.901 total time=   6.4s\n",
      "[CV 2/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1000, subsample=0.8;, score=-504.379 total time=   7.3s\n",
      "[CV 3/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1000, subsample=0.8;, score=-509.490 total time=   7.2s\n",
      "[CV 4/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1000, subsample=0.8;, score=-550.597 total time=   7.1s\n",
      "[CV 1/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1000, subsample=0.8;, score=-498.315 total time=   8.0s\n",
      "[CV 5/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1000, subsample=0.8;, score=-505.358 total time=   7.3s\n",
      "[CV 1/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1000, subsample=0.85;, score=-499.367 total time=   7.4s\n",
      "[CV 2/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1000, subsample=0.85;, score=-504.709 total time=   7.5s\n",
      "[CV 4/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1000, subsample=0.85;, score=-551.880 total time=   7.2s\n",
      "[CV 3/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1000, subsample=0.85;, score=-511.765 total time=   8.4s\n",
      "[CV 5/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1000, subsample=0.85;, score=-505.747 total time=   8.7s\n",
      "[CV 3/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1200, subsample=0.8;, score=-509.264 total time=   8.5s\n",
      "[CV 1/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1200, subsample=0.8;, score=-498.892 total time=   8.7s\n",
      "[CV 2/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1200, subsample=0.8;, score=-504.164 total time=   8.7s\n",
      "[CV 5/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1200, subsample=0.8;, score=-505.101 total time=   8.6s\n",
      "[CV 3/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1200, subsample=0.85;, score=-511.174 total time=   8.5s\n",
      "[CV 4/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1200, subsample=0.8;, score=-549.592 total time=  10.1s\n",
      "[CV 1/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1200, subsample=0.85;, score=-499.722 total time=   9.2s\n",
      "[CV 5/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1200, subsample=0.85;, score=-505.066 total time=   8.8s\n",
      "[CV 2/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1200, subsample=0.85;, score=-504.235 total time=  10.1s\n",
      "[CV 4/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1200, subsample=0.85;, score=-551.239 total time=  10.1s\n",
      "[CV 1/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1400, subsample=0.8;, score=-498.984 total time=  10.1s\n",
      "[CV 2/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1400, subsample=0.8;, score=-504.388 total time=  10.2s\n",
      "[CV 3/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1400, subsample=0.8;, score=-509.374 total time=  10.4s\n",
      "[CV 5/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1400, subsample=0.8;, score=-505.400 total time=  10.2s\n",
      "[CV 1/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1400, subsample=0.85;, score=-500.051 total time=  10.1s\n",
      "[CV 4/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1400, subsample=0.8;, score=-549.023 total time=  11.6s\n",
      "[CV 2/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1400, subsample=0.85;, score=-504.080 total time=  10.2s\n",
      "[CV 3/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1400, subsample=0.85;, score=-511.091 total time=   9.8s\n",
      "[CV 4/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1400, subsample=0.85;, score=-550.762 total time=  11.9s\n",
      "[CV 3/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1000, subsample=0.8;, score=-513.082 total time=  11.2s\n",
      "[CV 5/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1400, subsample=0.85;, score=-504.831 total time=  11.5s\n",
      "[CV 1/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1000, subsample=0.8;, score=-499.308 total time=  11.7s\n",
      "[CV 2/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1000, subsample=0.8;, score=-506.490 total time=  11.8s\n",
      "[CV 5/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1000, subsample=0.8;, score=-505.897 total time=  11.8s\n",
      "[CV 3/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1000, subsample=0.85;, score=-513.493 total time=  11.6s\n",
      "[CV 2/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1000, subsample=0.85;, score=-506.592 total time=  11.9s\n",
      "[CV 4/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1000, subsample=0.8;, score=-551.457 total time=  12.6s\n",
      "[CV 1/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1000, subsample=0.85;, score=-500.074 total time=  14.3s\n",
      "[CV 4/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1000, subsample=0.85;, score=-554.868 total time=  14.5s\n",
      "[CV 5/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1000, subsample=0.85;, score=-507.775 total time=  14.5s\n",
      "[CV 1/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1200, subsample=0.8;, score=-500.195 total time=  14.1s\n",
      "[CV 2/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1200, subsample=0.8;, score=-506.278 total time=  14.1s\n",
      "[CV 3/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1200, subsample=0.8;, score=-513.311 total time=  14.2s\n",
      "[CV 4/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1200, subsample=0.8;, score=-550.683 total time=  14.1s\n",
      "[CV 5/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1200, subsample=0.8;, score=-505.353 total time=  14.3s\n",
      "[CV 1/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1200, subsample=0.85;, score=-500.397 total time=  14.2s\n",
      "[CV 3/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1200, subsample=0.85;, score=-513.726 total time=  13.7s\n",
      "[CV 2/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1200, subsample=0.85;, score=-506.333 total time=  14.2s\n",
      "[CV 1/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1000, subsample=0.8;, score=-501.795 total time=   5.0s\n",
      "[CV 2/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1000, subsample=0.8;, score=-504.575 total time=   5.0s\n",
      "[CV 5/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1200, subsample=0.85;, score=-507.203 total time=  13.5s\n",
      "[CV 3/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1000, subsample=0.8;, score=-512.092 total time=   5.2s\n",
      "[CV 4/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1000, subsample=0.8;, score=-551.140 total time=   5.2s\n",
      "[CV 5/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1000, subsample=0.8;, score=-506.834 total time=   5.2s\n",
      "[CV 1/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1000, subsample=0.85;, score=-501.840 total time=   5.0s\n",
      "[CV 2/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1000, subsample=0.85;, score=-502.515 total time=   5.0s\n",
      "[CV 3/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1000, subsample=0.85;, score=-510.994 total time=   5.0s\n",
      "[CV 2/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1400, subsample=0.8;, score=-506.365 total time=  15.6s\n",
      "[CV 4/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1200, subsample=0.85;, score=-554.381 total time=  16.7s\n",
      "[CV 3/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1400, subsample=0.8;, score=-513.632 total time=  15.6s\n",
      "[CV 4/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1400, subsample=0.8;, score=-550.401 total time=  15.4s\n",
      "[CV 1/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1400, subsample=0.85;, score=-500.783 total time=  15.4s\n",
      "[CV 5/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1400, subsample=0.8;, score=-505.526 total time=  15.4s\n",
      "[CV 4/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1000, subsample=0.85;, score=-551.144 total time=   4.7s\n",
      "[CV 1/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1400, subsample=0.8;, score=-500.465 total time=  18.0s\n",
      "[CV 5/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1000, subsample=0.85;, score=-504.592 total time=   4.8s\n",
      "[CV 1/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1200, subsample=0.8;, score=-502.796 total time=   5.7s\n",
      "[CV 2/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1200, subsample=0.8;, score=-504.130 total time=   5.8s\n",
      "[CV 3/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1200, subsample=0.8;, score=-511.991 total time=   5.6s\n",
      "[CV 4/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1200, subsample=0.8;, score=-549.701 total time=   5.7s\n",
      "[CV 5/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1200, subsample=0.8;, score=-506.235 total time=   5.5s\n",
      "[CV 1/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1200, subsample=0.85;, score=-501.689 total time=   5.6s\n",
      "[CV 2/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1200, subsample=0.85;, score=-502.505 total time=   5.6s\n",
      "[CV 3/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1200, subsample=0.85;, score=-510.367 total time=   5.5s\n",
      "[CV 2/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1400, subsample=0.85;, score=-506.629 total time=  18.8s\n",
      "[CV 5/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1200, subsample=0.85;, score=-504.059 total time=   5.6s\n",
      "[CV 4/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1200, subsample=0.85;, score=-549.742 total time=   6.5s\n",
      "[CV 3/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1400, subsample=0.85;, score=-514.185 total time=  18.1s\n",
      "[CV 1/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1400, subsample=0.8;, score=-503.267 total time=   6.2s\n",
      "[CV 3/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1400, subsample=0.8;, score=-511.554 total time=   6.0s\n",
      "[CV 5/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1400, subsample=0.85;, score=-506.786 total time=  17.4s\n",
      "[CV 4/5] END colsample_bytree=0.95, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1400, subsample=0.85;, score=-554.161 total time=  17.9s\n",
      "[CV 2/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1400, subsample=0.8;, score=-504.509 total time=   6.2s\n",
      "[CV 4/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1400, subsample=0.8;, score=-549.231 total time=   6.3s\n",
      "[CV 1/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1400, subsample=0.85;, score=-501.941 total time=   6.3s\n",
      "[CV 5/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1400, subsample=0.8;, score=-506.089 total time=   7.0s\n",
      "[CV 2/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1400, subsample=0.85;, score=-502.430 total time=   6.3s\n",
      "[CV 4/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1400, subsample=0.85;, score=-549.333 total time=   6.3s\n",
      "[CV 3/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1400, subsample=0.85;, score=-510.473 total time=   6.6s\n",
      "[CV 5/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1400, subsample=0.85;, score=-503.743 total time=   6.3s\n",
      "[CV 1/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1000, subsample=0.8;, score=-497.637 total time=   7.0s\n",
      "[CV 3/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1000, subsample=0.8;, score=-510.242 total time=   6.9s\n",
      "[CV 2/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1000, subsample=0.8;, score=-502.509 total time=   7.1s\n",
      "[CV 4/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1000, subsample=0.8;, score=-548.940 total time=   7.0s\n",
      "[CV 1/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1000, subsample=0.85;, score=-497.935 total time=   7.0s\n",
      "[CV 5/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1000, subsample=0.8;, score=-505.798 total time=   8.1s\n",
      "[CV 3/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1000, subsample=0.85;, score=-510.214 total time=   7.9s\n",
      "[CV 4/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1000, subsample=0.85;, score=-550.203 total time=   7.2s\n",
      "[CV 5/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1000, subsample=0.85;, score=-505.738 total time=   6.9s\n",
      "[CV 2/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1000, subsample=0.85;, score=-503.361 total time=   8.4s\n",
      "[CV 3/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1200, subsample=0.8;, score=-509.983 total time=   8.3s\n",
      "[CV 4/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1200, subsample=0.8;, score=-548.128 total time=   8.6s\n",
      "[CV 5/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1200, subsample=0.8;, score=-505.534 total time=   8.6s\n",
      "[CV 1/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1200, subsample=0.8;, score=-498.467 total time=   9.9s\n",
      "[CV 2/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1200, subsample=0.85;, score=-503.316 total time=   8.4s\n",
      "[CV 2/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1200, subsample=0.8;, score=-502.402 total time=   9.9s\n",
      "[CV 1/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1200, subsample=0.85;, score=-498.564 total time=   9.6s\n",
      "[CV 3/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1200, subsample=0.85;, score=-509.958 total time=   8.4s\n",
      "[CV 4/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1200, subsample=0.85;, score=-550.104 total time=   8.9s\n",
      "[CV 5/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1200, subsample=0.85;, score=-504.967 total time=   8.6s\n",
      "[CV 1/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1400, subsample=0.8;, score=-499.070 total time=   9.8s\n",
      "[CV 3/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1400, subsample=0.8;, score=-510.465 total time=   9.8s\n",
      "[CV 4/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1400, subsample=0.8;, score=-548.104 total time=   9.8s\n",
      "[CV 2/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1400, subsample=0.8;, score=-502.882 total time=  10.1s\n",
      "[CV 5/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1400, subsample=0.8;, score=-505.612 total time=  10.1s\n",
      "[CV 4/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1400, subsample=0.85;, score=-549.883 total time=   9.7s\n",
      "[CV 3/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1400, subsample=0.85;, score=-510.415 total time=   9.9s\n",
      "[CV 1/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1400, subsample=0.85;, score=-499.046 total time=  11.7s\n",
      "[CV 2/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1400, subsample=0.85;, score=-503.436 total time=  11.7s\n",
      "[CV 5/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1400, subsample=0.85;, score=-504.654 total time=  11.6s\n",
      "[CV 1/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1000, subsample=0.8;, score=-498.933 total time=  11.4s\n",
      "[CV 2/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1000, subsample=0.8;, score=-506.478 total time=  11.6s\n",
      "[CV 3/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1000, subsample=0.8;, score=-513.621 total time=  11.4s\n",
      "[CV 5/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1000, subsample=0.8;, score=-506.484 total time=  11.7s\n",
      "[CV 3/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1000, subsample=0.85;, score=-514.917 total time=  11.7s\n",
      "[CV 4/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1000, subsample=0.8;, score=-552.019 total time=  13.4s\n",
      "[CV 4/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1000, subsample=0.85;, score=-553.433 total time=  12.0s\n",
      "[CV 5/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1000, subsample=0.85;, score=-506.069 total time=  12.1s\n",
      "[CV 1/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1000, subsample=0.85;, score=-498.190 total time=  13.9s\n",
      "[CV 2/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1000, subsample=0.85;, score=-506.828 total time=  14.4s\n",
      "[CV 1/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1200, subsample=0.8;, score=-499.311 total time=  14.0s\n",
      "[CV 3/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1200, subsample=0.8;, score=-513.636 total time=  13.6s\n",
      "[CV 2/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1200, subsample=0.8;, score=-506.591 total time=  13.9s\n",
      "[CV 4/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1200, subsample=0.8;, score=-551.968 total time=  14.1s\n",
      "[CV 5/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1200, subsample=0.8;, score=-506.591 total time=  14.1s\n",
      "[CV 1/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1200, subsample=0.85;, score=-499.026 total time=  13.3s\n",
      "[CV 2/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1200, subsample=0.85;, score=-506.775 total time=  13.7s\n",
      "[CV 2/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1000, subsample=0.8;, score=-504.476 total time=   5.0s\n",
      "[CV 3/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1000, subsample=0.8;, score=-510.716 total time=   5.0s\n",
      "[CV 1/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1000, subsample=0.8;, score=-503.168 total time=   5.4s\n",
      "[CV 4/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1000, subsample=0.8;, score=-550.149 total time=   5.0s\n",
      "[CV 3/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1200, subsample=0.85;, score=-515.391 total time=  16.6s\n",
      "[CV 5/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1000, subsample=0.8;, score=-505.384 total time=   5.0s\n",
      "[CV 1/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1000, subsample=0.85;, score=-501.962 total time=   4.9s\n",
      "[CV 4/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1200, subsample=0.85;, score=-553.016 total time=  16.6s\n",
      "[CV 1/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1400, subsample=0.8;, score=-500.078 total time=  15.4s\n",
      "[CV 5/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1200, subsample=0.85;, score=-505.769 total time=  16.4s\n",
      "[CV 2/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1000, subsample=0.85;, score=-504.300 total time=   4.8s\n",
      "[CV 3/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1400, subsample=0.8;, score=-514.007 total time=  15.2s\n",
      "[CV 2/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1400, subsample=0.8;, score=-506.971 total time=  15.7s\n",
      "[CV 4/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1400, subsample=0.8;, score=-552.055 total time=  14.9s\n",
      "[CV 4/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1000, subsample=0.85;, score=-549.294 total time=   4.6s\n",
      "[CV 3/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1000, subsample=0.85;, score=-510.274 total time=   4.7s\n",
      "[CV 5/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1000, subsample=0.85;, score=-505.510 total time=   4.9s\n",
      "[CV 5/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1400, subsample=0.8;, score=-506.784 total time=  15.3s\n",
      "[CV 3/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1400, subsample=0.85;, score=-515.775 total time=  15.2s\n",
      "[CV 2/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1400, subsample=0.85;, score=-506.879 total time=  15.3s\n",
      "[CV 1/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1200, subsample=0.8;, score=-503.732 total time=   5.5s\n",
      "[CV 3/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1200, subsample=0.8;, score=-511.082 total time=   5.4s\n",
      "[CV 2/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1200, subsample=0.8;, score=-504.464 total time=   6.1s\n",
      "[CV 4/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1200, subsample=0.8;, score=-549.545 total time=   5.2s\n",
      "[CV 1/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1200, subsample=0.85;, score=-502.581 total time=   5.3s\n",
      "[CV 3/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1200, subsample=0.85;, score=-510.212 total time=   5.2s\n",
      "[CV 4/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1200, subsample=0.85;, score=-548.508 total time=   5.3s\n",
      "[CV 5/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1200, subsample=0.8;, score=-505.126 total time=   6.0s\n",
      "[CV 1/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1400, subsample=0.85;, score=-499.330 total time=  17.9s\n",
      "[CV 2/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1200, subsample=0.85;, score=-504.176 total time=   6.2s\n",
      "[CV 5/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1200, subsample=0.85;, score=-505.103 total time=   5.4s\n",
      "[CV 4/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1400, subsample=0.85;, score=-553.456 total time=  18.0s\n",
      "[CV 1/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1400, subsample=0.8;, score=-503.993 total time=   5.9s\n",
      "[CV 5/5] END colsample_bytree=0.95, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1400, subsample=0.85;, score=-505.632 total time=  17.7s\n",
      "[CV 3/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1400, subsample=0.8;, score=-511.596 total time=   6.1s\n",
      "[CV 2/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1400, subsample=0.8;, score=-504.719 total time=   6.2s\n",
      "[CV 5/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1400, subsample=0.8;, score=-505.064 total time=   6.2s\n",
      "[CV 4/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1400, subsample=0.8;, score=-549.240 total time=   6.4s\n",
      "[CV 1/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1400, subsample=0.85;, score=-503.176 total time=   6.1s\n",
      "[CV 3/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1400, subsample=0.85;, score=-510.704 total time=   6.2s\n",
      "[CV 2/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1400, subsample=0.85;, score=-504.082 total time=   6.4s\n",
      "[CV 4/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1400, subsample=0.85;, score=-548.061 total time=   6.3s\n",
      "[CV 1/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1000, subsample=0.8;, score=-499.142 total time=   6.7s\n",
      "[CV 3/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1000, subsample=0.8;, score=-509.402 total time=   6.7s\n",
      "[CV 2/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1000, subsample=0.8;, score=-505.509 total time=   6.9s\n",
      "[CV 5/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1400, subsample=0.85;, score=-505.214 total time=   7.3s\n",
      "[CV 4/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1000, subsample=0.8;, score=-550.075 total time=   7.0s\n",
      "[CV 3/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1000, subsample=0.85;, score=-510.653 total time=   7.0s\n",
      "[CV 1/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1000, subsample=0.85;, score=-498.966 total time=   8.0s\n",
      "[CV 5/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1000, subsample=0.8;, score=-504.833 total time=   8.1s\n",
      "[CV 5/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1000, subsample=0.85;, score=-505.347 total time=   6.9s\n",
      "[CV 2/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1000, subsample=0.85;, score=-504.668 total time=   8.1s\n",
      "[CV 4/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1000, subsample=0.85;, score=-549.115 total time=   7.9s\n",
      "[CV 3/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1200, subsample=0.8;, score=-510.124 total time=   8.2s\n",
      "[CV 2/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1200, subsample=0.8;, score=-505.679 total time=   8.4s\n",
      "[CV 1/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1200, subsample=0.8;, score=-499.413 total time=   9.6s\n",
      "[CV 4/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1200, subsample=0.8;, score=-549.670 total time=   8.3s\n",
      "[CV 5/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1200, subsample=0.8;, score=-504.501 total time=   8.5s\n",
      "[CV 1/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1200, subsample=0.85;, score=-499.512 total time=   8.5s\n",
      "[CV 3/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1200, subsample=0.85;, score=-510.640 total time=   8.4s\n",
      "[CV 2/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1200, subsample=0.85;, score=-504.867 total time=   8.5s\n",
      "[CV 4/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1200, subsample=0.85;, score=-549.270 total time=   8.7s\n",
      "[CV 5/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1200, subsample=0.85;, score=-504.759 total time=   8.4s\n",
      "[CV 2/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1400, subsample=0.8;, score=-506.145 total time=   9.9s\n",
      "[CV 1/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1400, subsample=0.8;, score=-499.941 total time=  10.2s\n",
      "[CV 4/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1400, subsample=0.8;, score=-550.079 total time=   9.9s\n",
      "[CV 5/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1400, subsample=0.8;, score=-504.275 total time=   9.8s\n",
      "[CV 3/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1400, subsample=0.8;, score=-510.123 total time=  11.2s\n",
      "[CV 3/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1400, subsample=0.85;, score=-510.837 total time=  10.1s\n",
      "[CV 2/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1400, subsample=0.85;, score=-505.130 total time=  11.3s\n",
      "[CV 1/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1400, subsample=0.85;, score=-500.359 total time=  11.8s\n",
      "[CV 4/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1400, subsample=0.85;, score=-549.244 total time=  11.5s\n",
      "[CV 5/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1400, subsample=0.85;, score=-504.673 total time=  11.9s\n",
      "[CV 1/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1000, subsample=0.8;, score=-499.487 total time=  11.4s\n",
      "[CV 2/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1000, subsample=0.8;, score=-507.776 total time=  11.6s\n",
      "[CV 4/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1000, subsample=0.8;, score=-552.066 total time=  11.5s\n",
      "[CV 1/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1000, subsample=0.85;, score=-499.233 total time=  11.4s\n",
      "[CV 5/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1000, subsample=0.8;, score=-506.298 total time=  11.9s\n",
      "[CV 3/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1000, subsample=0.85;, score=-514.112 total time=  11.3s\n",
      "[CV 2/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1000, subsample=0.85;, score=-507.451 total time=  11.8s\n",
      "[CV 3/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1000, subsample=0.8;, score=-512.408 total time=  13.3s\n",
      "[CV 5/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1000, subsample=0.85;, score=-507.752 total time=  11.2s[CV 4/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1000, subsample=0.85;, score=-552.748 total time=  11.7s\n",
      "\n",
      "[CV 2/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1200, subsample=0.8;, score=-507.855 total time=  13.2s\n",
      "[CV 1/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1200, subsample=0.8;, score=-500.143 total time=  13.4s\n",
      "[CV 3/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1200, subsample=0.8;, score=-512.606 total time=  13.8s\n",
      "[CV 4/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1200, subsample=0.8;, score=-552.063 total time=  13.8s\n",
      "[CV 1/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1200, subsample=0.85;, score=-499.322 total time=  13.5s\n",
      "[CV 5/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1200, subsample=0.8;, score=-506.424 total time=  16.8s\n",
      "[CV 1/5] END colsample_bytree=1, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1000, subsample=0.8;, score=-503.390 total time=   5.3s\n",
      "[CV 2/5] END colsample_bytree=1, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1000, subsample=0.8;, score=-504.648 total time=   5.4s\n",
      "[CV 5/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1200, subsample=0.85;, score=-507.107 total time=  14.8s\n",
      "[CV 3/5] END colsample_bytree=1, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1000, subsample=0.8;, score=-511.534 total time=   5.5s\n",
      "[CV 4/5] END colsample_bytree=1, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1000, subsample=0.8;, score=-552.304 total time=   5.4s\n",
      "[CV 2/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1200, subsample=0.85;, score=-507.431 total time=  17.1s\n",
      "[CV 3/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1200, subsample=0.85;, score=-514.461 total time=  16.9s\n",
      "[CV 5/5] END colsample_bytree=1, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1000, subsample=0.8;, score=-507.084 total time=   5.2s\n",
      "[CV 4/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1200, subsample=0.85;, score=-553.092 total time=  17.4s\n",
      "[CV 1/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1400, subsample=0.8;, score=-500.820 total time=  15.4s\n",
      "[CV 2/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1400, subsample=0.8;, score=-508.393 total time=  15.8s\n",
      "[CV 4/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1400, subsample=0.8;, score=-552.285 total time=  15.1s\n",
      "[CV 3/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1400, subsample=0.8;, score=-513.050 total time=  15.7s\n",
      "[CV 5/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1400, subsample=0.8;, score=-506.364 total time=  15.3s\n",
      "[CV 1/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1400, subsample=0.85;, score=-499.944 total time=  15.4s\n",
      "[CV 2/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1400, subsample=0.85;, score=-507.649 total time=  15.4s\n",
      "[CV 3/5] END colsample_bytree=1, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1000, subsample=0.85;, score=-511.575 total time=   4.8s\n",
      "[CV 2/5] END colsample_bytree=1, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1000, subsample=0.85;, score=-505.717 total time=   4.8s\n",
      "[CV 1/5] END colsample_bytree=1, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1000, subsample=0.85;, score=-502.694 total time=   5.6s\n",
      "[CV 4/5] END colsample_bytree=1, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1000, subsample=0.85;, score=-552.677 total time=   4.9s\n",
      "[CV 5/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1400, subsample=0.85;, score=-507.163 total time=  15.8s\n",
      "[CV 5/5] END colsample_bytree=1, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1000, subsample=0.85;, score=-507.372 total time=   5.0s\n",
      "[CV 1/5] END colsample_bytree=1, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1200, subsample=0.8;, score=-503.727 total time=   5.9s\n",
      "[CV 3/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1400, subsample=0.85;, score=-514.815 total time=  18.0s\n",
      "[CV 4/5] END colsample_bytree=1, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1200, subsample=0.8;, score=-550.939 total time=   5.6s\n",
      "[CV 2/5] END colsample_bytree=1, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1200, subsample=0.8;, score=-503.069 total time=   6.3s\n",
      "[CV 3/5] END colsample_bytree=1, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1200, subsample=0.8;, score=-510.346 total time=   6.2s\n",
      "[CV 1/5] END colsample_bytree=1, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1200, subsample=0.85;, score=-502.315 total time=   5.8s\n",
      "[CV 5/5] END colsample_bytree=1, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1200, subsample=0.8;, score=-506.437 total time=   6.2s\n",
      "[CV 4/5] END colsample_bytree=0.95, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1400, subsample=0.85;, score=-553.591 total time=  18.4s\n",
      "[CV 2/5] END colsample_bytree=1, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1200, subsample=0.85;, score=-504.998 total time=   6.0s\n",
      "[CV 4/5] END colsample_bytree=1, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1200, subsample=0.85;, score=-551.085 total time=   5.8s\n",
      "[CV 3/5] END colsample_bytree=1, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1200, subsample=0.85;, score=-510.235 total time=   6.0s\n",
      "[CV 5/5] END colsample_bytree=1, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1200, subsample=0.85;, score=-506.484 total time=   5.9s\n",
      "[CV 3/5] END colsample_bytree=1, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1400, subsample=0.8;, score=-510.136 total time=   6.3s\n",
      "[CV 1/5] END colsample_bytree=1, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1400, subsample=0.8;, score=-503.821 total time=   6.7s\n",
      "[CV 4/5] END colsample_bytree=1, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1400, subsample=0.8;, score=-550.110 total time=   6.4s\n",
      "[CV 2/5] END colsample_bytree=1, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1400, subsample=0.8;, score=-502.811 total time=   6.9s\n",
      "[CV 1/5] END colsample_bytree=1, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1400, subsample=0.85;, score=-502.771 total time=   6.7s\n",
      "[CV 5/5] END colsample_bytree=1, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1400, subsample=0.8;, score=-506.105 total time=   7.3s\n",
      "[CV 3/5] END colsample_bytree=1, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1400, subsample=0.85;, score=-510.154 total time=   6.5s\n",
      "[CV 2/5] END colsample_bytree=1, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1400, subsample=0.85;, score=-504.459 total time=   6.7s\n",
      "[CV 4/5] END colsample_bytree=1, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1400, subsample=0.85;, score=-549.852 total time=   6.6s\n",
      "[CV 5/5] END colsample_bytree=1, learning_rate=0.01, max_depth=6, missing=inf, n_estimators=1400, subsample=0.85;, score=-505.531 total time=   7.3s[CV 1/5] END colsample_bytree=1, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1000, subsample=0.8;, score=-499.580 total time=   7.3s\n",
      "\n",
      "[CV 4/5] END colsample_bytree=1, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1000, subsample=0.8;, score=-550.268 total time=   7.4s\n",
      "[CV 2/5] END colsample_bytree=1, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1000, subsample=0.8;, score=-504.284 total time=   8.5s\n",
      "[CV 3/5] END colsample_bytree=1, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1000, subsample=0.8;, score=-508.298 total time=   8.6s\n",
      "[CV 3/5] END colsample_bytree=1, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1000, subsample=0.85;, score=-508.584 total time=   7.3s\n",
      "[CV 2/5] END colsample_bytree=1, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1000, subsample=0.85;, score=-505.749 total time=   7.5s\n",
      "[CV 4/5] END colsample_bytree=1, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1000, subsample=0.85;, score=-550.918 total time=   7.4s\n",
      "[CV 5/5] END colsample_bytree=1, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1000, subsample=0.8;, score=-505.998 total time=   8.5s\n",
      "[CV 5/5] END colsample_bytree=1, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1000, subsample=0.85;, score=-506.674 total time=   7.6s\n",
      "[CV 1/5] END colsample_bytree=1, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1000, subsample=0.85;, score=-499.154 total time=   8.7s\n",
      "[CV 1/5] END colsample_bytree=1, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1200, subsample=0.8;, score=-499.874 total time=   8.7s\n",
      "[CV 3/5] END colsample_bytree=1, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1200, subsample=0.8;, score=-507.864 total time=   8.8s\n",
      "[CV 2/5] END colsample_bytree=1, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1200, subsample=0.8;, score=-504.163 total time=   9.0s\n",
      "[CV 4/5] END colsample_bytree=1, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1200, subsample=0.8;, score=-549.285 total time=   8.8s\n",
      "[CV 5/5] END colsample_bytree=1, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1200, subsample=0.8;, score=-505.792 total time=   8.7s\n",
      "[CV 2/5] END colsample_bytree=1, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1200, subsample=0.85;, score=-505.199 total time=   8.8s\n",
      "[CV 3/5] END colsample_bytree=1, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1200, subsample=0.85;, score=-508.032 total time=   8.7s\n",
      "[CV 1/5] END colsample_bytree=1, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1200, subsample=0.85;, score=-499.264 total time=  10.4s\n",
      "[CV 4/5] END colsample_bytree=1, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1200, subsample=0.85;, score=-550.387 total time=   8.8s\n",
      "[CV 5/5] END colsample_bytree=1, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1200, subsample=0.85;, score=-506.161 total time=   9.0s\n",
      "[CV 2/5] END colsample_bytree=1, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1400, subsample=0.8;, score=-504.395 total time=  10.5s\n",
      "[CV 1/5] END colsample_bytree=1, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1400, subsample=0.8;, score=-500.787 total time=  12.1s\n",
      "[CV 5/5] END colsample_bytree=1, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1400, subsample=0.8;, score=-505.582 total time=  10.5s\n",
      "[CV 1/5] END colsample_bytree=1, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1400, subsample=0.85;, score=-499.888 total time=  10.4s\n",
      "[CV 3/5] END colsample_bytree=1, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1400, subsample=0.85;, score=-508.130 total time=  10.5s\n",
      "[CV 4/5] END colsample_bytree=1, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1400, subsample=0.85;, score=-550.524 total time=  10.9s\n",
      "[CV 3/5] END colsample_bytree=1, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1400, subsample=0.8;, score=-507.996 total time=  12.0s\n",
      "[CV 4/5] END colsample_bytree=1, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1400, subsample=0.8;, score=-548.940 total time=  12.1s\n",
      "[CV 2/5] END colsample_bytree=1, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1400, subsample=0.85;, score=-504.888 total time=  12.1s\n",
      "[CV 5/5] END colsample_bytree=1, learning_rate=0.01, max_depth=7, missing=inf, n_estimators=1400, subsample=0.85;, score=-505.669 total time=  12.6s\n",
      "[CV 1/5] END colsample_bytree=1, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1000, subsample=0.8;, score=-500.524 total time=  11.8s\n",
      "[CV 2/5] END colsample_bytree=1, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1000, subsample=0.8;, score=-507.685 total time=  11.7s\n",
      "[CV 3/5] END colsample_bytree=1, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1000, subsample=0.8;, score=-511.607 total time=  11.9s\n",
      "[CV 5/5] END colsample_bytree=1, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1000, subsample=0.8;, score=-506.077 total time=  11.6s\n",
      "[CV 4/5] END colsample_bytree=1, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1000, subsample=0.8;, score=-552.167 total time=  12.2s\n",
      "[CV 1/5] END colsample_bytree=1, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1000, subsample=0.85;, score=-501.709 total time=  12.1s\n",
      "[CV 2/5] END colsample_bytree=1, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1000, subsample=0.85;, score=-507.867 total time=  12.2s\n",
      "[CV 4/5] END colsample_bytree=1, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1000, subsample=0.85;, score=-554.035 total time=  12.5s\n",
      "[CV 5/5] END colsample_bytree=1, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1000, subsample=0.85;, score=-507.272 total time=  11.8s\n",
      "[CV 3/5] END colsample_bytree=1, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1000, subsample=0.85;, score=-511.410 total time=  13.9s\n",
      "[CV 1/5] END colsample_bytree=1, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1200, subsample=0.8;, score=-500.693 total time=  13.9s\n",
      "[CV 4/5] END colsample_bytree=1, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1200, subsample=0.8;, score=-551.531 total time=  14.3s\n",
      "[CV 3/5] END colsample_bytree=1, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1200, subsample=0.8;, score=-511.485 total time=  14.7s\n",
      "[CV 5/5] END colsample_bytree=1, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1200, subsample=0.8;, score=-505.658 total time=  14.5s\n",
      "[CV 1/5] END colsample_bytree=1, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1200, subsample=0.85;, score=-502.187 total time=  14.8s\n",
      "[CV 2/5] END colsample_bytree=1, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1200, subsample=0.8;, score=-507.604 total time=  17.8s\n",
      "[CV 1/5] END colsample_bytree=1, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1000, subsample=0.8;, score=-502.836 total time=   5.2s\n",
      "[CV 3/5] END colsample_bytree=1, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1200, subsample=0.85;, score=-511.442 total time=  17.2s\n",
      "[CV 2/5] END colsample_bytree=1, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1200, subsample=0.85;, score=-507.929 total time=  17.6s\n",
      "[CV 4/5] END colsample_bytree=1, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1200, subsample=0.85;, score=-553.675 total time=  17.5s\n",
      "[CV 2/5] END colsample_bytree=1, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1000, subsample=0.8;, score=-504.602 total time=   4.9s\n",
      "[CV 4/5] END colsample_bytree=1, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1000, subsample=0.8;, score=-552.260 total time=   5.0s\n",
      "[CV 1/5] END colsample_bytree=1, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1400, subsample=0.8;, score=-501.389 total time=  15.6s\n",
      "[CV 3/5] END colsample_bytree=1, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1000, subsample=0.8;, score=-511.387 total time=   5.1s\n",
      "[CV 5/5] END colsample_bytree=1, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1200, subsample=0.85;, score=-506.898 total time=  17.2s\n",
      "[CV 2/5] END colsample_bytree=1, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1400, subsample=0.8;, score=-507.549 total time=  15.6s\n",
      "[CV 4/5] END colsample_bytree=1, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1400, subsample=0.8;, score=-551.485 total time=  15.4s\n",
      "[CV 5/5] END colsample_bytree=1, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1000, subsample=0.8;, score=-505.644 total time=   5.0s\n",
      "[CV 3/5] END colsample_bytree=1, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1400, subsample=0.8;, score=-511.781 total time=  16.2s\n",
      "[CV 5/5] END colsample_bytree=1, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1400, subsample=0.8;, score=-505.507 total time=  16.0s\n",
      "[CV 1/5] END colsample_bytree=1, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1400, subsample=0.85;, score=-502.534 total time=  15.9s\n",
      "[CV 2/5] END colsample_bytree=1, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1400, subsample=0.85;, score=-508.203 total time=  16.0s\n",
      "[CV 2/5] END colsample_bytree=1, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1000, subsample=0.85;, score=-504.963 total time=   4.7s\n",
      "[CV 1/5] END colsample_bytree=1, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1000, subsample=0.85;, score=-502.820 total time=   5.2s\n",
      "[CV 3/5] END colsample_bytree=1, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1400, subsample=0.85;, score=-511.425 total time=  15.3s\n",
      "[CV 4/5] END colsample_bytree=1, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1400, subsample=0.85;, score=-553.671 total time=  15.1s\n",
      "[CV 3/5] END colsample_bytree=1, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1000, subsample=0.85;, score=-509.797 total time=   5.3s\n",
      "[CV 4/5] END colsample_bytree=1, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1000, subsample=0.85;, score=-549.746 total time=   5.3s\n",
      "[CV 5/5] END colsample_bytree=1, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1000, subsample=0.85;, score=-506.544 total time=   5.3s\n",
      "[CV 1/5] END colsample_bytree=1, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1200, subsample=0.8;, score=-503.438 total time=   5.3s\n",
      "[CV 3/5] END colsample_bytree=1, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1200, subsample=0.8;, score=-511.203 total time=   5.1s\n",
      "[CV 2/5] END colsample_bytree=1, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1200, subsample=0.8;, score=-504.015 total time=   5.4s\n",
      "[CV 4/5] END colsample_bytree=1, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1200, subsample=0.8;, score=-551.154 total time=   5.5s\n",
      "[CV 1/5] END colsample_bytree=1, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1200, subsample=0.85;, score=-503.100 total time=   5.5s\n",
      "[CV 2/5] END colsample_bytree=1, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1200, subsample=0.85;, score=-504.264 total time=   5.4s\n",
      "[CV 3/5] END colsample_bytree=1, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1200, subsample=0.85;, score=-508.578 total time=   5.5s\n",
      "[CV 5/5] END colsample_bytree=1, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1200, subsample=0.8;, score=-505.354 total time=   6.3s\n",
      "[CV 4/5] END colsample_bytree=1, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1200, subsample=0.85;, score=-548.982 total time=   5.4s\n",
      "[CV 5/5] END colsample_bytree=1, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1200, subsample=0.85;, score=-506.165 total time=   5.4s\n",
      "[CV 5/5] END colsample_bytree=1, learning_rate=0.01, max_depth=8, missing=inf, n_estimators=1400, subsample=0.85;, score=-506.415 total time=  18.3s\n",
      "[CV 1/5] END colsample_bytree=1, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1400, subsample=0.8;, score=-503.940 total time=   6.4s\n",
      "[CV 3/5] END colsample_bytree=1, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1400, subsample=0.8;, score=-511.002 total time=   6.3s\n",
      "[CV 2/5] END colsample_bytree=1, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1400, subsample=0.8;, score=-503.442 total time=   6.9s\n",
      "[CV 5/5] END colsample_bytree=1, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1400, subsample=0.8;, score=-505.177 total time=   6.3s\n",
      "[CV 1/5] END colsample_bytree=1, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1400, subsample=0.85;, score=-504.010 total time=   6.2s\n",
      "[CV 4/5] END colsample_bytree=1, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1400, subsample=0.8;, score=-550.807 total time=   7.2s\n",
      "[CV 5/5] END colsample_bytree=1, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1400, subsample=0.85;, score=-505.812 total time=   6.4s\n",
      "[CV 2/5] END colsample_bytree=1, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1400, subsample=0.85;, score=-503.714 total time=   7.5s\n",
      "[CV 3/5] END colsample_bytree=1, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1400, subsample=0.85;, score=-508.635 total time=   7.4s\n",
      "[CV 4/5] END colsample_bytree=1, learning_rate=0.012, max_depth=6, missing=inf, n_estimators=1400, subsample=0.85;, score=-548.408 total time=   7.2s\n",
      "[CV 1/5] END colsample_bytree=1, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1000, subsample=0.8;, score=-499.915 total time=   6.9s\n",
      "[CV 2/5] END colsample_bytree=1, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1000, subsample=0.8;, score=-504.577 total time=   7.0s\n",
      "[CV 4/5] END colsample_bytree=1, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1000, subsample=0.8;, score=-548.608 total time=   6.8s\n",
      "[CV 3/5] END colsample_bytree=1, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1000, subsample=0.8;, score=-509.148 total time=   7.0s\n",
      "[CV 5/5] END colsample_bytree=1, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1000, subsample=0.8;, score=-504.777 total time=   7.0s\n",
      "[CV 3/5] END colsample_bytree=1, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1000, subsample=0.85;, score=-509.654 total time=   6.8s\n",
      "[CV 1/5] END colsample_bytree=1, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1000, subsample=0.85;, score=-499.385 total time=   7.1s\n",
      "[CV 4/5] END colsample_bytree=1, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1000, subsample=0.85;, score=-550.557 total time=   7.3s\n",
      "[CV 2/5] END colsample_bytree=1, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1000, subsample=0.85;, score=-505.314 total time=   8.6s\n",
      "[CV 5/5] END colsample_bytree=1, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1000, subsample=0.85;, score=-505.801 total time=   8.7s\n",
      "[CV 1/5] END colsample_bytree=1, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1200, subsample=0.8;, score=-500.318 total time=   8.5s\n",
      "[CV 2/5] END colsample_bytree=1, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1200, subsample=0.8;, score=-504.472 total time=   8.4s\n",
      "[CV 3/5] END colsample_bytree=1, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1200, subsample=0.8;, score=-509.067 total time=   8.4s\n",
      "[CV 5/5] END colsample_bytree=1, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1200, subsample=0.8;, score=-504.721 total time=   8.1s\n",
      "[CV 4/5] END colsample_bytree=1, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1200, subsample=0.8;, score=-547.962 total time=   8.7s\n",
      "[CV 2/5] END colsample_bytree=1, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1200, subsample=0.85;, score=-505.152 total time=   8.4s\n",
      "[CV 1/5] END colsample_bytree=1, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1200, subsample=0.85;, score=-500.294 total time=  10.0s\n",
      "[CV 3/5] END colsample_bytree=1, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1200, subsample=0.85;, score=-509.320 total time=   9.9s\n",
      "[CV 1/5] END colsample_bytree=1, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1400, subsample=0.8;, score=-501.152 total time=   9.5s\n",
      "[CV 4/5] END colsample_bytree=1, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1200, subsample=0.85;, score=-550.298 total time=  10.1s\n",
      "[CV 3/5] END colsample_bytree=1, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1400, subsample=0.8;, score=-509.405 total time=   9.5s\n",
      "[CV 5/5] END colsample_bytree=1, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1200, subsample=0.85;, score=-505.457 total time=  10.1s\n",
      "[CV 2/5] END colsample_bytree=1, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1400, subsample=0.8;, score=-504.647 total time=  10.0s\n",
      "[CV 4/5] END colsample_bytree=1, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1400, subsample=0.8;, score=-547.890 total time=  10.0s\n",
      "[CV 1/5] END colsample_bytree=1, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1400, subsample=0.85;, score=-501.237 total time=   9.7s\n",
      "[CV 5/5] END colsample_bytree=1, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1400, subsample=0.8;, score=-504.813 total time=   9.9s\n",
      "[CV 2/5] END colsample_bytree=1, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1400, subsample=0.85;, score=-505.284 total time=  10.2s\n",
      "[CV 3/5] END colsample_bytree=1, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1400, subsample=0.85;, score=-509.428 total time=  10.3s\n",
      "[CV 4/5] END colsample_bytree=1, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1400, subsample=0.85;, score=-550.679 total time=  12.3s\n",
      "[CV 1/5] END colsample_bytree=1, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1000, subsample=0.8;, score=-501.822 total time=  11.5s\n",
      "[CV 5/5] END colsample_bytree=1, learning_rate=0.012, max_depth=7, missing=inf, n_estimators=1400, subsample=0.85;, score=-505.221 total time=  12.3s\n",
      "[CV 2/5] END colsample_bytree=1, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1000, subsample=0.8;, score=-507.261 total time=  11.6s\n",
      "[CV 3/5] END colsample_bytree=1, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1000, subsample=0.8;, score=-510.008 total time=  11.6s\n",
      "[CV 4/5] END colsample_bytree=1, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1000, subsample=0.8;, score=-554.135 total time=  11.5s\n",
      "[CV 5/5] END colsample_bytree=1, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1000, subsample=0.8;, score=-506.561 total time=  11.7s\n",
      "[CV 1/5] END colsample_bytree=1, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1000, subsample=0.85;, score=-500.290 total time=  11.7s\n",
      "[CV 4/5] END colsample_bytree=1, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1000, subsample=0.85;, score=-552.537 total time=  11.5s\n",
      "[CV 2/5] END colsample_bytree=1, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1000, subsample=0.85;, score=-508.518 total time=  14.7s\n",
      "[CV 3/5] END colsample_bytree=1, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1000, subsample=0.85;, score=-514.294 total time=  14.1s\n",
      "[CV 1/5] END colsample_bytree=1, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1200, subsample=0.8;, score=-502.322 total time=  13.6s\n",
      "[CV 3/5] END colsample_bytree=1, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1200, subsample=0.8;, score=-510.096 total time=  14.0s\n",
      "[CV 5/5] END colsample_bytree=1, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1000, subsample=0.85;, score=-508.424 total time=  14.9s[CV 4/5] END colsample_bytree=1, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1200, subsample=0.8;, score=-553.786 total time=  14.1s\n",
      "\n",
      "[CV 5/5] END colsample_bytree=1, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1200, subsample=0.8;, score=-506.356 total time=  13.9s\n",
      "[CV 1/5] END colsample_bytree=1, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1200, subsample=0.85;, score=-500.917 total time=  14.1s\n",
      "[CV 2/5] END colsample_bytree=1, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1200, subsample=0.85;, score=-508.652 total time=  13.4s\n",
      "[CV 3/5] END colsample_bytree=1, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1200, subsample=0.85;, score=-514.616 total time=  13.9s\n",
      "[CV 2/5] END colsample_bytree=1, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1200, subsample=0.8;, score=-507.720 total time=  16.5s\n",
      "[CV 1/5] END colsample_bytree=1, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1000, subsample=0.8;, score=-504.404 total time=   5.1s\n",
      "[CV 2/5] END colsample_bytree=1, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1000, subsample=0.8;, score=-504.216 total time=   5.1s\n",
      "[CV 3/5] END colsample_bytree=1, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1000, subsample=0.8;, score=-510.321 total time=   5.2s\n",
      "[CV 4/5] END colsample_bytree=1, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1000, subsample=0.8;, score=-548.770 total time=   5.1s\n",
      "[CV 5/5] END colsample_bytree=1, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1000, subsample=0.8;, score=-506.986 total time=   5.0s\n",
      "[CV 1/5] END colsample_bytree=1, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1000, subsample=0.85;, score=-502.313 total time=   5.1s\n",
      "[CV 5/5] END colsample_bytree=1, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1200, subsample=0.85;, score=-508.161 total time=  14.0s\n",
      "[CV 2/5] END colsample_bytree=1, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1000, subsample=0.85;, score=-505.400 total time=   5.2s\n",
      "[CV 4/5] END colsample_bytree=1, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1200, subsample=0.85;, score=-552.325 total time=  16.7s\n",
      "[CV 3/5] END colsample_bytree=1, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1000, subsample=0.85;, score=-509.636 total time=   5.7s\n",
      "[CV 2/5] END colsample_bytree=1, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1400, subsample=0.8;, score=-508.173 total time=  15.5s\n",
      "[CV 3/5] END colsample_bytree=1, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1400, subsample=0.8;, score=-510.640 total time=  15.4s\n",
      "[CV 4/5] END colsample_bytree=1, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1400, subsample=0.8;, score=-553.822 total time=  15.6s\n",
      "[CV 5/5] END colsample_bytree=1, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1400, subsample=0.8;, score=-506.173 total time=  15.7s\n",
      "[CV 4/5] END colsample_bytree=1, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1000, subsample=0.85;, score=-547.899 total time=   4.8s\n",
      "[CV 5/5] END colsample_bytree=1, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1000, subsample=0.85;, score=-506.155 total time=   4.7s\n",
      "[CV 1/5] END colsample_bytree=1, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1400, subsample=0.8;, score=-502.965 total time=  18.5s\n",
      "[CV 1/5] END colsample_bytree=1, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1200, subsample=0.8;, score=-504.957 total time=   5.6s\n",
      "[CV 2/5] END colsample_bytree=1, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1200, subsample=0.8;, score=-503.601 total time=   5.5s\n",
      "[CV 3/5] END colsample_bytree=1, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1200, subsample=0.8;, score=-510.929 total time=   5.6s\n",
      "[CV 4/5] END colsample_bytree=1, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1200, subsample=0.8;, score=-547.908 total time=   5.7s\n",
      "[CV 2/5] END colsample_bytree=1, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1400, subsample=0.85;, score=-508.816 total time=  14.9s\n",
      "[CV 5/5] END colsample_bytree=1, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1200, subsample=0.8;, score=-506.998 total time=   5.6s\n",
      "[CV 1/5] END colsample_bytree=1, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1400, subsample=0.85;, score=-501.525 total time=  17.6s\n",
      "[CV 1/5] END colsample_bytree=1, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1200, subsample=0.85;, score=-503.024 total time=   5.5s\n",
      "[CV 5/5] END colsample_bytree=1, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1400, subsample=0.85;, score=-507.730 total time=  14.4s\n",
      "[CV 2/5] END colsample_bytree=1, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1200, subsample=0.85;, score=-505.079 total time=   6.0s\n",
      "[CV 4/5] END colsample_bytree=1, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1200, subsample=0.85;, score=-547.971 total time=   5.3s\n",
      "[CV 5/5] END colsample_bytree=1, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1200, subsample=0.85;, score=-505.589 total time=   5.3s\n",
      "[CV 3/5] END colsample_bytree=1, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1200, subsample=0.85;, score=-509.968 total time=   6.4s\n",
      "[CV 1/5] END colsample_bytree=1, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1400, subsample=0.8;, score=-505.562 total time=   6.2s\n",
      "[CV 2/5] END colsample_bytree=1, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1400, subsample=0.8;, score=-503.551 total time=   6.3s\n",
      "[CV 3/5] END colsample_bytree=1, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1400, subsample=0.8;, score=-511.333 total time=   6.2s\n",
      "[CV 3/5] END colsample_bytree=1, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1400, subsample=0.85;, score=-514.865 total time=  17.6s\n",
      "[CV 4/5] END colsample_bytree=1, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1400, subsample=0.8;, score=-547.479 total time=   6.4s\n",
      "[CV 4/5] END colsample_bytree=1, learning_rate=0.012, max_depth=8, missing=inf, n_estimators=1400, subsample=0.85;, score=-552.520 total time=  17.6s\n",
      "[CV 1/5] END colsample_bytree=1, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1400, subsample=0.85;, score=-503.612 total time=   6.4s\n",
      "[CV 2/5] END colsample_bytree=1, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1400, subsample=0.85;, score=-505.285 total time=   6.4s\n",
      "[CV 3/5] END colsample_bytree=1, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1400, subsample=0.85;, score=-509.890 total time=   6.3s\n",
      "[CV 4/5] END colsample_bytree=1, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1400, subsample=0.85;, score=-547.783 total time=   6.4s\n",
      "[CV 5/5] END colsample_bytree=1, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1400, subsample=0.85;, score=-505.345 total time=   6.6s\n",
      "[CV 5/5] END colsample_bytree=1, learning_rate=0.014, max_depth=6, missing=inf, n_estimators=1400, subsample=0.8;, score=-506.703 total time=   7.5s\n",
      "[CV 1/5] END colsample_bytree=1, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1000, subsample=0.8;, score=-501.577 total time=   6.9s\n",
      "[CV 3/5] END colsample_bytree=1, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1000, subsample=0.8;, score=-508.543 total time=   7.0s\n",
      "[CV 4/5] END colsample_bytree=1, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1000, subsample=0.8;, score=-550.899 total time=   6.9s\n",
      "[CV 1/5] END colsample_bytree=1, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1000, subsample=0.85;, score=-499.987 total time=   7.1s\n",
      "[CV 2/5] END colsample_bytree=1, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1000, subsample=0.8;, score=-506.691 total time=   8.2s\n",
      "[CV 2/5] END colsample_bytree=1, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1000, subsample=0.85;, score=-505.198 total time=   7.2s\n",
      "[CV 5/5] END colsample_bytree=1, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1000, subsample=0.8;, score=-506.375 total time=   8.0s\n",
      "[CV 4/5] END colsample_bytree=1, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1000, subsample=0.85;, score=-549.045 total time=   6.7s\n",
      "[CV 3/5] END colsample_bytree=1, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1000, subsample=0.85;, score=-508.993 total time=   8.1s\n",
      "[CV 5/5] END colsample_bytree=1, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1000, subsample=0.85;, score=-505.496 total time=   7.3s\n",
      "[CV 1/5] END colsample_bytree=1, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1200, subsample=0.8;, score=-502.448 total time=   8.5s\n",
      "[CV 3/5] END colsample_bytree=1, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1200, subsample=0.8;, score=-508.925 total time=   8.7s\n",
      "[CV 5/5] END colsample_bytree=1, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1200, subsample=0.8;, score=-505.879 total time=   8.3s\n",
      "[CV 1/5] END colsample_bytree=1, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1200, subsample=0.85;, score=-501.099 total time=   8.6s\n",
      "[CV 3/5] END colsample_bytree=1, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1200, subsample=0.85;, score=-509.206 total time=   8.2s\n",
      "[CV 4/5] END colsample_bytree=1, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1200, subsample=0.85;, score=-549.168 total time=   8.1s\n",
      "[CV 2/5] END colsample_bytree=1, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1200, subsample=0.85;, score=-505.170 total time=   8.6s\n",
      "[CV 2/5] END colsample_bytree=1, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1200, subsample=0.8;, score=-506.823 total time=  10.0s\n",
      "[CV 4/5] END colsample_bytree=1, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1200, subsample=0.8;, score=-550.768 total time=   9.8s\n",
      "[CV 5/5] END colsample_bytree=1, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1200, subsample=0.85;, score=-504.662 total time=  10.3s\n",
      "[CV 1/5] END colsample_bytree=1, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1400, subsample=0.8;, score=-503.355 total time=  10.0s\n",
      "[CV 3/5] END colsample_bytree=1, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1400, subsample=0.8;, score=-509.040 total time=   9.7s\n",
      "[CV 2/5] END colsample_bytree=1, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1400, subsample=0.8;, score=-507.069 total time=  10.4s\n",
      "[CV 4/5] END colsample_bytree=1, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1400, subsample=0.8;, score=-551.129 total time=  10.0s\n",
      "[CV 1/5] END colsample_bytree=1, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1400, subsample=0.85;, score=-501.530 total time=  10.3s\n",
      "[CV 3/5] END colsample_bytree=1, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1400, subsample=0.85;, score=-508.960 total time=   9.9s\n",
      "[CV 5/5] END colsample_bytree=1, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1400, subsample=0.8;, score=-505.892 total time=  11.9s\n",
      "[CV 5/5] END colsample_bytree=1, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1400, subsample=0.85;, score=-504.647 total time=  10.3s\n",
      "[CV 2/5] END colsample_bytree=1, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1400, subsample=0.85;, score=-505.883 total time=  12.2s\n",
      "[CV 4/5] END colsample_bytree=1, learning_rate=0.014, max_depth=7, missing=inf, n_estimators=1400, subsample=0.85;, score=-549.015 total time=  12.0s\n",
      "[CV 1/5] END colsample_bytree=1, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1000, subsample=0.8;, score=-501.601 total time=  11.5s\n",
      "[CV 2/5] END colsample_bytree=1, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1000, subsample=0.8;, score=-508.012 total time=  11.5s\n",
      "[CV 3/5] END colsample_bytree=1, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1000, subsample=0.8;, score=-512.562 total time=  11.0s\n",
      "[CV 5/5] END colsample_bytree=1, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1000, subsample=0.8;, score=-507.304 total time=  11.1s\n",
      "[CV 1/5] END colsample_bytree=1, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1000, subsample=0.85;, score=-502.893 total time=  11.2s\n",
      "[CV 4/5] END colsample_bytree=1, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1000, subsample=0.8;, score=-551.698 total time=  11.6s\n",
      "[CV 2/5] END colsample_bytree=1, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1000, subsample=0.85;, score=-505.059 total time=  11.7s\n",
      "[CV 4/5] END colsample_bytree=1, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1000, subsample=0.85;, score=-552.034 total time=  14.1s\n",
      "[CV 3/5] END colsample_bytree=1, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1000, subsample=0.85;, score=-512.676 total time=  14.5s\n",
      "[CV 1/5] END colsample_bytree=1, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1200, subsample=0.8;, score=-502.228 total time=  13.9s\n",
      "[CV 5/5] END colsample_bytree=1, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1000, subsample=0.85;, score=-507.981 total time=  14.2s\n",
      "[CV 2/5] END colsample_bytree=1, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1200, subsample=0.8;, score=-508.476 total time=  13.7s\n",
      "[CV 3/5] END colsample_bytree=1, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1200, subsample=0.8;, score=-512.935 total time=  13.7s\n",
      "[CV 4/5] END colsample_bytree=1, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1200, subsample=0.8;, score=-551.486 total time=  13.4s\n",
      "[CV 1/5] END colsample_bytree=1, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1200, subsample=0.85;, score=-503.613 total time=  13.2s\n",
      "[CV 5/5] END colsample_bytree=1, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1200, subsample=0.8;, score=-507.135 total time=  13.6s\n",
      "[CV 3/5] END colsample_bytree=1, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1200, subsample=0.85;, score=-512.673 total time=  13.1s\n",
      "[CV 2/5] END colsample_bytree=1, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1200, subsample=0.85;, score=-505.433 total time=  14.6s\n",
      "[CV 4/5] END colsample_bytree=1, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1200, subsample=0.85;, score=-551.943 total time=  14.3s\n",
      "[CV 5/5] END colsample_bytree=1, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1200, subsample=0.85;, score=-507.696 total time=  14.0s\n",
      "[CV 3/5] END colsample_bytree=1, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1400, subsample=0.8;, score=-513.445 total time=  12.7s\n",
      "[CV 1/5] END colsample_bytree=1, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1400, subsample=0.8;, score=-502.742 total time=  13.3s\n",
      "[CV 2/5] END colsample_bytree=1, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1400, subsample=0.8;, score=-508.878 total time=  12.9s\n",
      "[CV 5/5] END colsample_bytree=1, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1400, subsample=0.8;, score=-507.407 total time=  12.4s\n",
      "[CV 4/5] END colsample_bytree=1, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1400, subsample=0.8;, score=-551.847 total time=  12.6s\n",
      "[CV 1/5] END colsample_bytree=1, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1400, subsample=0.85;, score=-504.201 total time=  12.4s\n",
      "[CV 2/5] END colsample_bytree=1, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1400, subsample=0.85;, score=-505.632 total time=  12.2s\n",
      "[CV 3/5] END colsample_bytree=1, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1400, subsample=0.85;, score=-513.421 total time=  10.4s\n",
      "[CV 4/5] END colsample_bytree=1, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1400, subsample=0.85;, score=-552.396 total time=  10.2s\n",
      "[CV 5/5] END colsample_bytree=1, learning_rate=0.014, max_depth=8, missing=inf, n_estimators=1400, subsample=0.85;, score=-507.656 total time=   8.8s\n",
      "Best parameters found:  {'colsample_bytree': 0.95, 'learning_rate': 0.012, 'max_depth': 7, 'missing': inf, 'n_estimators': 1200, 'subsample': 0.8}\n",
      "Best score found:  -512.9028418919763\n",
      "Test score with best parameters:  0.9933299694965709\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid_art, cv=5, n_jobs=-1, verbose=3, scoring='neg_root_mean_squared_error')\n",
    "\n",
    "# Fit the GridSearchCV instance to the training data\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Get the best parameters and the best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Best parameters found: \", best_params)\n",
    "print(\"Best score found: \", best_score)\n",
    "\n",
    "# Evaluate the model on the test set using the best parameters\n",
    "best_model = grid_search.best_estimator_\n",
    "test_score = best_model.score(X_test, y_test)\n",
    "print(\"Test score with best parameters: \", test_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "ax = plot_importance(best_model, max_num_features=len(X.columns)+1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rf_regressor = RandomForestRegressor()\\n\\n# Define the parameter grid to search through\\nparam_grid_rf = {\\n    \\'n_estimators\\': [300, 400, 500],\\n    \\'max_depth\\': [None],\\n    \\'min_samples_split\\': [2, 3],\\n    \\'min_samples_leaf\\': [2,3]\\n}\\n\\n# Initialize GridSearchCV\\ngrid_search = GridSearchCV(estimator=rf_regressor, param_grid=param_grid_rf, cv=3, n_jobs=-1, verbose=2, scoring=\\'neg_root_mean_squared_error\\')\\nX_nan = np.nan_to_num(X.astype(np.float32))\\n# Fit the GridSearchCV instance to the training data\\ngrid_search.fit(X_nan, y)\\n\\n# Get the best parameters and the best score\\nbest_params = grid_search.best_params_\\nbest_score = grid_search.best_score_\\nbest_model = grid_search.best_estimator_\\n\\nprint(\"Best parameters found: \", best_params)\\nprint(\"Best score found: \", best_score)'"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''rf_regressor = RandomForestRegressor()\n",
    "\n",
    "# Define the parameter grid to search through\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [300, 400, 500],\n",
    "    'max_depth': [None],\n",
    "    'min_samples_split': [2, 3],\n",
    "    'min_samples_leaf': [2,3]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=rf_regressor, param_grid=param_grid_rf, cv=3, n_jobs=-1, verbose=2, scoring='neg_root_mean_squared_error')\n",
    "X_nan = np.nan_to_num(X.astype(np.float32))\n",
    "# Fit the GridSearchCV instance to the training data\n",
    "grid_search.fit(X_nan, y)\n",
    "\n",
    "# Get the best parameters and the best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "print(\"Best parameters found: \", best_params)\n",
    "print(\"Best score found: \", best_score)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'param_grid_lr = {\\n    \\'penalty\\': [\\'l1\\', \\'l2\\'],\\n    \\'C\\': [ 0.1, 1, 10,]\\n}\\nlogistic = LogisticRegression()\\ngrid_search = GridSearchCV(estimator=logistic, param_grid=param_grid_lr, cv=3, n_jobs=None, verbose=2, scoring=\\'neg_root_mean_squared_error\\')\\n\\n# Fit the GridSearchCV instance to the training data\\ngrid_search.fit(X_nan, y)\\n\\n# Get the best parameters and the best score\\nbest_params = grid_search.best_params_\\nbest_score = grid_search.best_score_\\n\\nprint(\"Best parameters found: \", best_params)\\nprint(\"Best score found: \", best_score)\\n'"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''param_grid_lr = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [ 0.1, 1, 10,]\n",
    "}\n",
    "logistic = LogisticRegression()\n",
    "grid_search = GridSearchCV(estimator=logistic, param_grid=param_grid_lr, cv=3, n_jobs=None, verbose=2, scoring='neg_root_mean_squared_error')\n",
    "\n",
    "# Fit the GridSearchCV instance to the training data\n",
    "grid_search.fit(X_nan, y)\n",
    "\n",
    "# Get the best parameters and the best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Best parameters found: \", best_params)\n",
    "print(\"Best score found: \", best_score)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.79</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>F</td>\n",
       "      <td>SI1</td>\n",
       "      <td>62.7</td>\n",
       "      <td>60.0</td>\n",
       "      <td>5.82</td>\n",
       "      <td>5.89</td>\n",
       "      <td>3.67</td>\n",
       "      <td>Amsterdam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.20</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>J</td>\n",
       "      <td>VS1</td>\n",
       "      <td>61.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>6.81</td>\n",
       "      <td>6.89</td>\n",
       "      <td>4.18</td>\n",
       "      <td>Surat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.57</td>\n",
       "      <td>Premium</td>\n",
       "      <td>H</td>\n",
       "      <td>SI1</td>\n",
       "      <td>62.2</td>\n",
       "      <td>61.0</td>\n",
       "      <td>7.38</td>\n",
       "      <td>7.32</td>\n",
       "      <td>4.57</td>\n",
       "      <td>Kimberly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.90</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>F</td>\n",
       "      <td>SI1</td>\n",
       "      <td>63.8</td>\n",
       "      <td>54.0</td>\n",
       "      <td>6.09</td>\n",
       "      <td>6.13</td>\n",
       "      <td>3.90</td>\n",
       "      <td>Kimberly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.50</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>F</td>\n",
       "      <td>VS1</td>\n",
       "      <td>62.9</td>\n",
       "      <td>58.0</td>\n",
       "      <td>5.05</td>\n",
       "      <td>5.09</td>\n",
       "      <td>3.19</td>\n",
       "      <td>Amsterdam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13480</th>\n",
       "      <td>0.57</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>E</td>\n",
       "      <td>SI1</td>\n",
       "      <td>61.9</td>\n",
       "      <td>56.0</td>\n",
       "      <td>5.35</td>\n",
       "      <td>5.32</td>\n",
       "      <td>3.30</td>\n",
       "      <td>Amsterdam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13481</th>\n",
       "      <td>0.71</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>I</td>\n",
       "      <td>VS2</td>\n",
       "      <td>62.2</td>\n",
       "      <td>55.0</td>\n",
       "      <td>5.71</td>\n",
       "      <td>5.73</td>\n",
       "      <td>3.56</td>\n",
       "      <td>New York City</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13482</th>\n",
       "      <td>0.70</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>F</td>\n",
       "      <td>VS1</td>\n",
       "      <td>61.6</td>\n",
       "      <td>55.0</td>\n",
       "      <td>5.75</td>\n",
       "      <td>5.71</td>\n",
       "      <td>3.53</td>\n",
       "      <td>Tel Aviv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13483</th>\n",
       "      <td>0.70</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>F</td>\n",
       "      <td>SI2</td>\n",
       "      <td>58.8</td>\n",
       "      <td>57.0</td>\n",
       "      <td>5.85</td>\n",
       "      <td>5.89</td>\n",
       "      <td>3.45</td>\n",
       "      <td>Surat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13484</th>\n",
       "      <td>0.40</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>I</td>\n",
       "      <td>VVS2</td>\n",
       "      <td>62.4</td>\n",
       "      <td>55.0</td>\n",
       "      <td>4.70</td>\n",
       "      <td>4.73</td>\n",
       "      <td>2.94</td>\n",
       "      <td>New York City</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13485 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       carat        cut color clarity  depth  table     x     y     z  \\\n",
       "0       0.79  Very Good     F     SI1   62.7   60.0  5.82  5.89  3.67   \n",
       "1       1.20      Ideal     J     VS1   61.0   57.0  6.81  6.89  4.18   \n",
       "2       1.57    Premium     H     SI1   62.2   61.0  7.38  7.32  4.57   \n",
       "3       0.90  Very Good     F     SI1   63.8   54.0  6.09  6.13  3.90   \n",
       "4       0.50  Very Good     F     VS1   62.9   58.0  5.05  5.09  3.19   \n",
       "...      ...        ...   ...     ...    ...    ...   ...   ...   ...   \n",
       "13480   0.57      Ideal     E     SI1   61.9   56.0  5.35  5.32  3.30   \n",
       "13481   0.71      Ideal     I     VS2   62.2   55.0  5.71  5.73  3.56   \n",
       "13482   0.70      Ideal     F     VS1   61.6   55.0  5.75  5.71  3.53   \n",
       "13483   0.70  Very Good     F     SI2   58.8   57.0  5.85  5.89  3.45   \n",
       "13484   0.40      Ideal     I    VVS2   62.4   55.0  4.70  4.73  2.94   \n",
       "\n",
       "                city  \n",
       "0          Amsterdam  \n",
       "1              Surat  \n",
       "2           Kimberly  \n",
       "3           Kimberly  \n",
       "4          Amsterdam  \n",
       "...              ...  \n",
       "13480      Amsterdam  \n",
       "13481  New York City  \n",
       "13482       Tel Aviv  \n",
       "13483          Surat  \n",
       "13484  New York City  \n",
       "\n",
       "[13485 rows x 10 columns]"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('./data/src/diamonds_test.csv')\n",
    "test['price'] = 0\n",
    "submit_df = test[['id', 'price']]\n",
    "test = test.drop(['id','price'], axis = 1)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>13485.000000</td>\n",
       "      <td>13485.000000</td>\n",
       "      <td>13485.000000</td>\n",
       "      <td>13485.000000</td>\n",
       "      <td>13485.000000</td>\n",
       "      <td>13485.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.798642</td>\n",
       "      <td>61.739095</td>\n",
       "      <td>57.490337</td>\n",
       "      <td>5.736454</td>\n",
       "      <td>5.739648</td>\n",
       "      <td>3.543474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.469399</td>\n",
       "      <td>1.435310</td>\n",
       "      <td>2.237109</td>\n",
       "      <td>1.113671</td>\n",
       "      <td>1.128507</td>\n",
       "      <td>0.731005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>50.800000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>4.730000</td>\n",
       "      <td>4.730000</td>\n",
       "      <td>2.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.700000</td>\n",
       "      <td>61.900000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>5.700000</td>\n",
       "      <td>5.720000</td>\n",
       "      <td>3.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.040000</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>6.530000</td>\n",
       "      <td>6.530000</td>\n",
       "      <td>4.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.010000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>10.740000</td>\n",
       "      <td>31.800000</td>\n",
       "      <td>31.800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              carat         depth         table             x             y  \\\n",
       "count  13485.000000  13485.000000  13485.000000  13485.000000  13485.000000   \n",
       "mean       0.798642     61.739095     57.490337      5.736454      5.739648   \n",
       "std        0.469399      1.435310      2.237109      1.113671      1.128507   \n",
       "min        0.200000     50.800000     51.000000      0.000000      0.000000   \n",
       "25%        0.400000     61.000000     56.000000      4.730000      4.730000   \n",
       "50%        0.700000     61.900000     57.000000      5.700000      5.720000   \n",
       "75%        1.040000     62.500000     59.000000      6.530000      6.530000   \n",
       "max        5.010000     79.000000     73.000000     10.740000     31.800000   \n",
       "\n",
       "                  z  \n",
       "count  13485.000000  \n",
       "mean       3.543474  \n",
       "std        0.731005  \n",
       "min        0.000000  \n",
       "25%        2.920000  \n",
       "50%        3.530000  \n",
       "75%        4.040000  \n",
       "max       31.800000  "
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>city</th>\n",
       "      <th>x/y</th>\n",
       "      <th>td</th>\n",
       "      <th>ad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "      <td>40455.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.796861</td>\n",
       "      <td>61.749873</td>\n",
       "      <td>57.439239</td>\n",
       "      <td>5.729562</td>\n",
       "      <td>5.731737</td>\n",
       "      <td>3.537474</td>\n",
       "      <td>3.904783</td>\n",
       "      <td>4.400766</td>\n",
       "      <td>4.049388</td>\n",
       "      <td>1.036433</td>\n",
       "      <td>0.999362</td>\n",
       "      <td>0.930796</td>\n",
       "      <td>0.006131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.471897</td>\n",
       "      <td>1.381551</td>\n",
       "      <td>2.195948</td>\n",
       "      <td>1.123308</td>\n",
       "      <td>1.114987</td>\n",
       "      <td>0.694681</td>\n",
       "      <td>1.117876</td>\n",
       "      <td>1.701260</td>\n",
       "      <td>1.648181</td>\n",
       "      <td>0.019357</td>\n",
       "      <td>0.008881</td>\n",
       "      <td>0.046552</td>\n",
       "      <td>0.000308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>56.855746</td>\n",
       "      <td>49.711387</td>\n",
       "      <td>1.796384</td>\n",
       "      <td>1.827576</td>\n",
       "      <td>1.103360</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.960889</td>\n",
       "      <td>0.766397</td>\n",
       "      <td>0.002847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>4.710000</td>\n",
       "      <td>4.720000</td>\n",
       "      <td>2.910000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.031075</td>\n",
       "      <td>0.992606</td>\n",
       "      <td>0.898876</td>\n",
       "      <td>0.006047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.700000</td>\n",
       "      <td>61.800000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>5.690000</td>\n",
       "      <td>5.710000</td>\n",
       "      <td>3.520000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.038843</td>\n",
       "      <td>0.995736</td>\n",
       "      <td>0.923825</td>\n",
       "      <td>0.006116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.040000</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>6.540000</td>\n",
       "      <td>6.540000</td>\n",
       "      <td>4.035000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.048770</td>\n",
       "      <td>1.006928</td>\n",
       "      <td>0.955519</td>\n",
       "      <td>0.006189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.454322</td>\n",
       "      <td>66.646117</td>\n",
       "      <td>65.173617</td>\n",
       "      <td>9.662704</td>\n",
       "      <td>9.635886</td>\n",
       "      <td>5.971430</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.069918</td>\n",
       "      <td>1.037898</td>\n",
       "      <td>1.095425</td>\n",
       "      <td>0.019147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              carat         depth         table             x             y  \\\n",
       "count  40455.000000  40455.000000  40455.000000  40455.000000  40455.000000   \n",
       "mean       0.796861     61.749873     57.439239      5.729562      5.731737   \n",
       "std        0.471897      1.381551      2.195948      1.123308      1.114987   \n",
       "min        0.200000     56.855746     49.711387      1.796384      1.827576   \n",
       "25%        0.400000     61.000000     56.000000      4.710000      4.720000   \n",
       "50%        0.700000     61.800000     57.000000      5.690000      5.710000   \n",
       "75%        1.040000     62.500000     59.000000      6.540000      6.540000   \n",
       "max        2.454322     66.646117     65.173617      9.662704      9.635886   \n",
       "\n",
       "                  z           cut         color       clarity          city  \\\n",
       "count  40455.000000  40455.000000  40455.000000  40455.000000  40455.000000   \n",
       "mean       3.537474      3.904783      4.400766      4.049388      1.036433   \n",
       "std        0.694681      1.117876      1.701260      1.648181      0.019357   \n",
       "min        1.103360      1.000000      1.000000      1.000000      1.000000   \n",
       "25%        2.910000      3.000000      3.000000      3.000000      1.031075   \n",
       "50%        3.520000      4.000000      4.000000      4.000000      1.038843   \n",
       "75%        4.035000      5.000000      6.000000      5.000000      1.048770   \n",
       "max        5.971430      5.000000      7.000000      8.000000      1.069918   \n",
       "\n",
       "                x/y            td            ad  \n",
       "count  40455.000000  40455.000000  40455.000000  \n",
       "mean       0.999362      0.930796      0.006131  \n",
       "std        0.008881      0.046552      0.000308  \n",
       "min        0.960889      0.766397      0.002847  \n",
       "25%        0.992606      0.898876      0.006047  \n",
       "50%        0.995736      0.923825      0.006116  \n",
       "75%        1.006928      0.955519      0.006189  \n",
       "max        1.037898      1.095425      0.019147  "
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labeling_test(test, ['cut', 'color', 'clarity','city'],dic2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[outlier_columns] = sub_outlyers(test[outlier_columns],3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['x/y'] = test.apply(lambda x : feature_xy(x['x'], x['y']), axis=1)\n",
    "test['td'] = test.apply(lambda x : feature_xy(x['table'], x['depth']), axis=1)\n",
    "test['ad'] = test.apply(lambda x : feature_ad(x['carat'],x['x'], x['y'], x['z']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = pd.concat([test, pd.get_dummies(test['city'], drop_first=True)], axis = 1).drop('city',axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['x/y','td', 'ad']] = sub_outlyers(test[['x/y','td', 'ad']],3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def filter_transform2(df,selected_features,target):\\n    data_df = df[selected_features]\\n    to_dummy = [feature for feature in selected_features if len(df[feature].unique()) < 15]\\n    for feature in to_dummy:\\n        dummies_df = pd.get_dummies(data_df[feature], prefix=feature, drop_first=True)\\n        data_df = pd.concat([data_df, dummies_df],axis=1).drop(feature, axis=1)   \\n    data_df = pd.concat([data_df.select_dtypes(exclude=['object']), df[target]], axis=1)\\n    return data_df\""
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def filter_transform2(df,selected_features,target):\n",
    "    data_df = df[selected_features]\n",
    "    to_dummy = [feature for feature in selected_features if len(df[feature].unique()) < 15]\n",
    "    for feature in to_dummy:\n",
    "        dummies_df = pd.get_dummies(data_df[feature], prefix=feature, drop_first=True)\n",
    "        data_df = pd.concat([data_df, dummies_df],axis=1).drop(feature, axis=1)   \n",
    "    data_df = pd.concat([data_df.select_dtypes(exclude=['object']), df[target]], axis=1)\n",
    "    return data_df'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test[X.columns]\n",
    "test.to_csv('./data/clean/testdl123fexydatdso3x2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_submit = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_list = best_model.predict(X_submit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13480</th>\n",
       "      <td>13480</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13481</th>\n",
       "      <td>13481</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13482</th>\n",
       "      <td>13482</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13483</th>\n",
       "      <td>13483</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13484</th>\n",
       "      <td>13484</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13485 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  price\n",
       "0          0      0\n",
       "1          1      0\n",
       "2          2      0\n",
       "3          3      0\n",
       "4          4      0\n",
       "...      ...    ...\n",
       "13480  13480      0\n",
       "13481  13481      0\n",
       "13482  13482      0\n",
       "13483  13483      0\n",
       "13484  13484      0\n",
       "\n",
       "[13485 rows x 2 columns]"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit_df['price'] = 0\n",
    "submit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2889.993164\n",
       "1        5639.766602\n",
       "2        9761.402344\n",
       "3        4022.315186\n",
       "4        1613.657593\n",
       "            ...     \n",
       "13480    1708.315430\n",
       "13481    2495.214111\n",
       "13482    3077.759766\n",
       "13483    2101.745850\n",
       "13484     802.257263\n",
       "Name: price, Length: 13485, dtype: float32"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit_df['price'] = price_list\n",
    "submit_df['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_df.set_index('id')['price'].to_csv('./submissions/xgartl123gridtdso3.5x2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(best_model, open('./models/xgartdl123dlgridtdso3.5x2.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open('./models/dl123dlgridtdso3.sav', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streamlit_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
